{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b17924-0d9e-402c-b5e8-8f5279cbb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input flags\n",
    "iloc = int(input('I-location is '))#location of the section along i axis\n",
    "jloc = 14#int(input('j-location is '))#location of the secion along j axis\n",
    "\n",
    "variabd = 0#int(input('Enter DV flag: ')) #flag for if in variable discharge regime\n",
    "adjustment_time = 65#int(input('Enter bed adjustment time here, be wary of timestep conversion: ')) #number of timesteps, in hours taken for bed to equilibrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2904c-b40a-4186-8168-0cd6aab2f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import math \n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.colors as mcol\n",
    "import matplotlib.cm as cm\n",
    "import time\n",
    "import random\n",
    "import statistics as stat\n",
    "import collections\n",
    "import copy \n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import PIL\n",
    "from xml.dom import minidom\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy import signal\n",
    "from scipy import interpolate\n",
    "from scipy.stats import variation\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from IPython.display import Image\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from celluloid import Camera\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "\n",
    "#plt.style.use('seaborn-white')\n",
    "\n",
    "from matplotlib.collections import LineCollection, PatchCollection\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm, LinearSegmentedColormap\n",
    "\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "#%matplotlib qt\n",
    "#%matplotlib notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e687aad-1fad-4184-949d-318bb9b80a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Setting up all the aesthetics:\n",
    "font = {'family' : 'Helvetica',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 10}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "kwargs = dict(edgecolor = 'k', fc = 'xkcd:greyish', alpha=0.5, bins=20)\n",
    "\n",
    "#—————————————————————— Set up model parameters\n",
    "Q = str(100)# discharge in m3/s\n",
    "slope = 0.00137#gradient\n",
    "d50 = 0.31e-3 #d50 grain size in m\n",
    "\n",
    "thot_SS = 0 # 802800\n",
    "#time from which we restarted the runs to add a flood\n",
    "fldstart_s = 802800 ###start time of flood in seconds\n",
    "\n",
    "fldlength = 0\n",
    "idf = '0hfld' #identifier/classifier about the flood setting\n",
    "intstring = '2hour' #time interval of output\n",
    "nickname = 'agubh2-10km' #model nickname, agubh2, gentle_wide etc\n",
    "floodname ='_0hrflood'\n",
    "\n",
    "datnam = f'{idf}-datamaster-{intstring}.npy' #name of the data file to upload\n",
    "dataloc = f'data-{nickname}{floodname}' #where csv files are\n",
    "arrayfolder = f'c-{idf}-{nickname}' #where raw centroid data array stored\n",
    "mainsurfto = f'ms-{idf}-{nickname}' #where bounding surfaces array will go\n",
    "iricoutputt =600 #output time from the model, s\n",
    "\n",
    "datamaster = np.load(f'/Volumes/SAF_Data/NAYS2DH_files/Data/ConvertedArrays/{dataloc}/{datnam}', allow_pickle = True)\n",
    "\n",
    "### More model domain set up\n",
    "\n",
    "cellW = 4\n",
    "cellL = 10\n",
    "xloc = iloc*cellL\n",
    "xsloc = iloc*cellL\n",
    "spacing = 1 #spacing of cross stream x locations, m\n",
    "\n",
    "ps = 2650 # bulk density of quartz kg/m3\n",
    "p = 1000 # density of water in kg/m3\n",
    "nu = 1.787*10e-6 #kinematic viscosity of water in m2/s\n",
    "nu=1.0533e-6\n",
    "g = 9.81 # acceleration due to gravity, m/s2\n",
    "\n",
    "savefilesto = '/Volumes/SAF_Data/NAYS2DH_files/Plots/'\n",
    "modelrun = f'{nickname}-{idf}-{iloc}'\n",
    "\n",
    "gridx = 1001\n",
    "gridy = 26\n",
    "\n",
    "datacond = 1\n",
    "if datacond == 1:\n",
    "    cells = gridy\n",
    "else:\n",
    "    cells = gridx\n",
    "length = 1001 #length of the domain in the x direction\n",
    "erostart = 5\n",
    "erostop = 5\n",
    "\n",
    "num_timesteps = datamaster.shape[2] ##### or 168 for 2 weeks# len(os.listdir(filepathcore))-1 ###when u want to stop plotting\n",
    "\n",
    "datamaster = datamaster[:, :, :num_timesteps]\n",
    "position = np.arange(0, length, dtype = float)\n",
    "coevelev = np.empty([num_timesteps])\n",
    "interval_to_plot = 120/60 #we want to plot every ___  HOURS \n",
    "end_t = num_timesteps #len(np.arange(1, num_timesteps, skipstep)) #number of timesteps in data master array\n",
    "fldstart = ((thot_SS+fldstart_s)/3600)/interval_to_plot ###flood starttime, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05874fa-f9e3-4086-bdb6-ad34dd394ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "##—————————————————————— Create dataframe to store data for this section\n",
    "bardata = pd.DataFrame(columns=['BarName', \n",
    "                                'iloc', \n",
    "                                'Facies', \n",
    "                                'Barcode',\n",
    "                                'StartTime', \n",
    "                                'EndTime', \n",
    "                                'ElapsedTime', \n",
    "                                'LeftEdge', \n",
    "                                'RightEdge', \n",
    "                                'BarWidth', \n",
    "                                'BarHeight', \n",
    "                                'BarAspect', \n",
    "                                'BarArea', \n",
    "                                'Section Area', \n",
    "                                'ChannelProp', \n",
    "                                'MinClinoHt','MaxClinoHt', 'ModeClinoHt', 'MeanClinoHt', 'SDClinoHt', 'MedClinoHt',\n",
    "                                'MinClinoWt','MaxClinoWt', 'ModeClinoWt', 'MeanClinoWt', 'SDClinoWt', 'MedClinoWt',\n",
    "                                'MinClinoICD','MaxClinoICD', 'ModeClinoICD', 'MeanClinoICD', 'SDClinoICD', 'MedClinoICD',\n",
    "                                'MinFlowDepth', 'MaxFlowDepth', 'ModeFlowDepth', 'MeanFlowDepth', 'SDFlowDepth', 'MedFlowDepth',\n",
    "                                'MinShear', 'MaxShear', 'ModeShear', 'MeanShear', 'SDShear', 'MedShear',\n",
    "                                'MinVelocity', 'MaxVelocity', 'ModeVelocity', 'MeanVelocity', 'SDVelocity', 'MedVelocity',\n",
    "                                'notes'])\n",
    "\n",
    "## define things\n",
    "\n",
    "def ls_regression(xarr, yarr):\n",
    "    ''' do a least squares regression, compute the variance between the regression and the data'''\n",
    "    A = np.ones([len(xarr), 2])\n",
    "    A[:, 1] = copy.deepcopy(xarr)\n",
    "    y = copy.deepcopy(yarr)\n",
    "    AT = np.transpose(A)\n",
    "    ata = np.matmul(AT, A)\n",
    "    aty = np.matmul(AT, y)\n",
    "    xls1 = np.matmul(np.linalg.inv(ata), aty)\n",
    "    \n",
    "    x = np.linspace(A[:, 1].min(), A[:, 1].max(), len(A))\n",
    "    y = xls1[1]*x + xls1[0]\n",
    "    \n",
    "    rms = (np.sum((y-yarr)**2)/len(yarr))**.5\n",
    "    return rms, x, y\n",
    "\n",
    "def movinggrad(stratarray, posarray):\n",
    "    ##### find the moving gradient of the surfaces at 1m intervals\n",
    "    ###stratarray = array with final stratigraphy, posarray = array with xposition in m\n",
    "    #create gradmatrix\n",
    "    \n",
    "    gradmatrix = np.zeros_like(stratarray)\n",
    "    angles = np.zeros_like(gradmatrix)\n",
    "\n",
    "    for t in range (0, stratarray.shape[0]): #for each timestep in the matrix\n",
    "        for x in range (0, stratarray.shape[1]-1): #for each x position, have to stop before 1 to work with array dims\n",
    "           \n",
    "            delta_z = (stratarray[t, x+1]-stratarray[t, x])\n",
    "            delta_x = (posarray[x+1]-posarray[x])\n",
    "            gradmatrix[t, x] = delta_z/delta_x\n",
    "            angles[t, x] = np.degrees(np.arctan(gradmatrix[t,x]))\n",
    "            #print(gradmatrix[t, x], angles[t,x])\n",
    "    return gradmatrix, angles\n",
    "\n",
    "## using cos rule to calculate bearing angles instead\n",
    "## impose a limit on the y axis to create a y axis vector\n",
    "def dist_between(x1, y1, x2, y2):\n",
    "    d = np.sqrt((x1-x2)**2+(y1-y2)**2)\n",
    "    return d\n",
    "\n",
    "def angle_between(ax, ay, bx, by, ox = 0, oy = 0):\n",
    "    \n",
    "    '''find the angle between two vectors in centroid space, i.e. the trajectory'''\n",
    "\n",
    "    ##find the lengths of all the sides of the triangle\n",
    "    oa = dist_between(ox, oy, ax, ay)\n",
    "    ob = dist_between(ox, oy, bx, by)\n",
    "    ab = dist_between(ax, ay, bx, by)\n",
    "        \n",
    "    #use cosing rule to get the bearing\n",
    "    cos_theta = (oa**2+ob**2-ab**2)/(2*oa*ob)\n",
    "    bear = np.rad2deg(np.arccos(cos_theta))\n",
    "    #if bx < ax:\n",
    "    #    bear = 360-bear\n",
    "    return bear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f9a53-f5b1-44be-82d7-02a65271a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###—————————————————————— DATA WRANGLING\n",
    "\n",
    "remove_ilocs = np.where(datamaster[:, 0, :] != iloc)\n",
    "data = np.delete(datamaster, remove_ilocs[0], axis=0)\n",
    "\n",
    "for i in range (0, num_timesteps):\n",
    "    data[:, :, i] = data[:, :, i][np.argsort(data[:, :, i][:, 3])]\n",
    "\n",
    "stratigraphy = np.empty([end_t, cells]) ###will hold data for topography accounting for changes due to erosion\n",
    "stratigraphy_idx = stratigraphy.copy()\n",
    "shearstresseroded = stratigraphy.copy() #will hold data for shear stress accounting for changes due to erosion\n",
    "stratflowdepth =  stratigraphy.copy() #will hold data for flow depth accounting for changes due to erosion\n",
    "scaleflowdepth =  stratigraphy.copy() #will hold data for local flow depth scaled to max accounting for changes due to erosion\n",
    "froudedata =  stratigraphy.copy() #will hold data for local flow depth scaled to max accounting for changes due to erosion\n",
    "velocity =  stratigraphy.copy()\n",
    "runtime = len(stratigraphy)\n",
    "xposition =  stratigraphy.copy()\n",
    "\n",
    "in_section = np.where(datamaster[:, 0, -1]==iloc)\n",
    "in_section = in_section[0]\n",
    "bankpos = datamaster[:, 3, :][in_section]\n",
    "\n",
    "rightbank = bankpos.min() ###negativee\n",
    "leftbank = bankpos.max() #positive\n",
    "\n",
    "xy_strat = np.empty([end_t, int(3+np.round((leftbank-rightbank)/spacing, 0))]) # will put stratigraphies here, in proper x pos\n",
    "xy_strat[:] = np.nan\n",
    "ages = np.empty_like(xy_strat)\n",
    "#—————————————————————— Import the data\n",
    "for time in range (0, data.shape[2]): #TIME\n",
    "    #print(stratigraphy[time, :].shape)\n",
    "    stratigraphy[time, :] = data[:, 7, time] #elevation change, elevation in 5\n",
    "    shearstresseroded[time, :] = data[:, 6, time] \n",
    "    stratflowdepth[time, :] = data[:, 4, time]\n",
    "    froudedata[time, :] = data[:, 9, time]\n",
    "    velocity[time, :] = data[:, 10, time]\n",
    "    \n",
    "    ypos = data[:, 3, time]-rightbank #coreect supid centreline indexing\n",
    "    \n",
    "    xposition[time, :] = ypos\n",
    "\n",
    "stratigraphy_idx = stratigraphy.copy()\n",
    "shear = xy_strat.copy()\n",
    "froude = xy_strat.copy()\n",
    "scaleflow = xy_strat.copy() #flow depth scaled to max per time\n",
    "trueflow = xy_strat.copy() #unscaled flow depth\n",
    "flowvel = xy_strat.copy() #flow velocity\n",
    "\n",
    "for t in range (0, end_t):\n",
    "    for idx, x in zip(np.arange(0, stratigraphy.shape[1]), xposition[t, :]):\n",
    "        x = int(np.floor(x)) #rounding down positions, making integers so can use as index\n",
    "        xy_strat[t, x] = stratigraphy[t, idx]\n",
    "        shear[t, x] = shearstresseroded[t, idx]\n",
    "        froude[t, x] = froudedata[t, idx]\n",
    "        trueflow[t, x] = stratflowdepth[t, idx]\n",
    "        scaleflow[t, x] = scaleflowdepth[t, idx]\n",
    "        flowvel[t, x] = velocity[t, idx]\n",
    "xy_topo = np.empty_like(xy_strat)\n",
    "xy_topo[:] = np.nan\n",
    "for t in range (0, end_t):\n",
    "    length = int(np.floor(xposition[t, -1]-xposition[t, 0])) #length of the section at time, t\n",
    "    pos = np.linspace(0, length, length) #create a metre scale array with each x pos = location\n",
    "    dataint = np.arange(xposition[t, 0], np.round(xposition[t, -1], 0), dtype=int)\n",
    "\n",
    "    stratnotnan = xy_strat[t, :][~np.isnan(xy_strat[t, :])] #pull out real values of strat\n",
    "    shearnotnan = shear[t, :][~np.isnan(shear[t, :])] #pull out real values of shear\n",
    "    froudenotnan = froude[t, :][~np.isnan(froude[t, :])] #pull out real values of froude\n",
    "    truefnotnan = trueflow[t, :][~np.isnan(trueflow[t, :])] #pull out real values of true flow depth\n",
    "    velnotnan = flowvel[t, :][~np.isnan(flowvel[t, :])] #pull out real values of strat\n",
    "    \n",
    "    fx = interpolate.interp1d(xposition[t, :], stratnotnan[:], kind = 'cubic', fill_value = 'extrapolate') #stratigraphy interpolation\n",
    "    fsh = interpolate.interp1d(xposition[t, :], shearnotnan[:], kind = 'cubic', fill_value = 'extrapolate') #shear stress interpolation\n",
    "    ffr = interpolate.interp1d(xposition[t, :], froudenotnan[:], kind = 'cubic', fill_value = 'extrapolate') #froude number interpolation\n",
    "    ftf = interpolate.interp1d(xposition[t, :], truefnotnan[:], kind = 'cubic', fill_value = 'extrapolate') #true flow depth interpolation\n",
    "    ffv = interpolate.interp1d(xposition[t, :], velnotnan[:], kind = 'cubic', fill_value = 'extrapolate') #flow veloity interpolation\n",
    "    \n",
    "    xy_topo[t, dataint] = fx(dataint) #reassign strat\n",
    "    shear[t, dataint] = fsh(dataint) #reassign shear\n",
    "    froude[t, dataint] = ffr(dataint) #reassign froude\n",
    "    trueflow[t, dataint] = ftf(dataint) #reassign true fd\n",
    "    flowvel[t, dataint] = ffv(dataint) #reassign flow vel\n",
    "\n",
    "stratcondition = np.zeros_like(xy_topo)\n",
    "stratcondition[:] = np.nan\n",
    "erosurf = np.empty([end_t, xy_topo.shape[1]])\n",
    "erosurf[:] = np.nan\n",
    "strat = copy.deepcopy(xy_topo)\n",
    "halfwidth=25\n",
    "\n",
    "for time in range (0, end_t):    \n",
    "    for space in range (0, xy_topo.shape[1]):\n",
    "        preexisting_strata = xy_topo[:time, :] #this is our search array, where we will erode\n",
    "        willerode = np.where(preexisting_strata[:, space] > xy_topo[time, space])\n",
    "        xy_topo[willerode, space] = xy_topo[time, space]\n",
    "        ages[willerode, space] = time\n",
    "        \n",
    "for i in range (end_t-2, -1, -1):\n",
    "    fillinx = np.where(np.isnan(xy_topo[i, :]))\n",
    "    xy_topo[i, fillinx] = xy_topo[i+1, fillinx]\n",
    "    stratcondition[i, fillinx] = 1\n",
    "    fillinsh = np.where(np.isnan(shear[i, :]))\n",
    "    shear[i, fillinsh] = shear[i+1, fillinsh]\n",
    "    fillinfr = np.where(np.isnan(froude[i, :]))\n",
    "    froude[i, fillinfr] = froude[i+1, fillinfr]\n",
    "    fillintf = np.where(np.isnan(trueflow[i, :]))\n",
    "    trueflow[i, fillintf] = trueflow[i+1, fillintf]\n",
    "    fillinsf = np.where(np.isnan(scaleflow[i, :]))\n",
    "    scaleflow[i, fillinsf] = scaleflow[i+1, fillinsf]\n",
    "    fillinfv = np.where(np.isnan(flowvel[i, :]))\n",
    "    flowvel[i, fillinfv] = flowvel[i+1, fillinfv]\n",
    "    fillinstrat = np.where(np.isnan(strat[i, :]))\n",
    "\n",
    "###—————————————————————— Find area of cross section\n",
    "fig = plt.figure()\n",
    "xs_area = plt.fill_between(np.arange(0, xy_topo.shape[1]), xy_topo[0, :], xy_topo[-1, :])\n",
    "plt.close(fig)\n",
    "xs_verts = xs_area.get_paths()[0].vertices\n",
    "xs_polygon = Polygon(xs_verts) \n",
    "xs_bds = xs_polygon.bounds\n",
    "xs_area = xs_polygon.area\n",
    "xsleft = xs_bds[0]\n",
    "xsbottom = xs_bds[1]\n",
    "xsright = xs_bds[2]\n",
    "xstop = xs_bds[3]\n",
    "xs_thick = abs(np.nanmin(xy_topo[-1])-np.nanmax(xy_topo))\n",
    "xs_width = xsright-xsleft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf14b19-45b0-4edf-b1a1-dc4becd21d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "###—————————————————————— Get surfaces and do the rest of data wrangling for the final section to be interpreted\n",
    "fig, ax = plt.subplots(figsize = (12, 3), tight_layout=True)\n",
    "for i in range(0, end_t):\n",
    "    ax.plot(xy_topo[i, :], 'k', alpha = 0.2, lw = 0.5)\n",
    "ax.set_aspect('10')\n",
    "ax.set_xlim(xposition[-1, :].min(), xposition[-1, :].max());\n",
    "\n",
    "maxflow = np.reshape(np.nanmax(trueflow, axis=1), [end_t, 1]) ## max flow across the section at each timestep\n",
    "scaleflow = trueflow/maxflow\n",
    "#—————————————————————— time and x location\n",
    "start_time = 0 #would be start of model run\n",
    "end_time = end_t #would be end of model run\n",
    "tim = range(start_time,end_time) #range of time\n",
    "position = np.arange(0, xy_topo.shape[1], dtype = float)\n",
    "\n",
    "cm1 = 'hsv'\n",
    "cnorm = mcol.Normalize(vmin=min(tim),vmax=max(tim))\n",
    "cpick = cm.ScalarMappable(norm=cnorm,cmap='hsv') \n",
    "cpick.set_array([])\n",
    "\n",
    "cmap_vvfac = ListedColormap(['xkcd:mud brown', 'xkcd:shit', 'xkcd:dark tan', 'xkcd:sand', 'xkcd:stone'])\n",
    "##——————————————————————facies\n",
    "temp = 20.0 #temperature in degrees celcius\n",
    "kv = (1.14-0.031*(temp-15)+0.00068*((temp-15)**2))*10**-6\n",
    "D_star = np.power((1.65*9.81)/(1e3*(kv**2)), 0.333)*3.1e-4\n",
    "d50um = 310\n",
    "chezy_rough = 18*(np.log10(4*trueflow/d50)) \n",
    "mmp =(p*(flowvel**2))/(1650*(chezy_rough**2)*d50)##### modified mobility parameter\n",
    "mmprange = [0.01, 0.025, 0.17, 0.35, 1.5, 10] ##### LP, R, D, D-UP, UP\n",
    "norm_vvfac = BoundaryNorm(mmprange, cmap_vvfac.N)\n",
    "\n",
    "ages_ero = np.empty_like(xy_topo)\n",
    "posnew = np.arange(0, xy_topo.shape[1])\n",
    "\n",
    "##### SCENARIO 1: Hiatus = any elevation change that is less than x cm\n",
    "##### SCENARIO 2: Hiatus = any elevation change that is less than x % of the average elevation change\n",
    "##### SCENARIO 3: Hiatus = any elevation change that is less than the xth percentile of change at the timestep\n",
    "hiatal_scenario = 2\n",
    "threshold_thick = 0.005  #absolute value in metres \n",
    "nth = 25 #what percentile distribution to use to calculate hiatal surfaces for each timestep\n",
    "perc = 0.1 #fraction of percentile\n",
    "\n",
    "for time in range (1, end_t):\n",
    "    ages_ero[time, :] = time \n",
    "    lessthan = np.where(strat[time, :] < xy_topo[time-1, :]) ###where it erodes\n",
    "    ele_change = strat[time, :]-xy_topo[time-1, :] #find the bed elevation change across the domain it has to be before you reassign topography else you will get zeros where replaced\n",
    "    ages_ero[time, lessthan] = ages_ero[time-1, lessthan]   \n",
    "    if hiatal_scenario == 1:\n",
    "        threshold_thick = perc*(np.nanmean(ele_change)) #find the fraction of the average elevation change between the two latest timesteps\n",
    "    if hiatal_scenario == 2:\n",
    "        threshold_thick = perc*np.nanpercentile(ele_change, nth)\n",
    "    if time != 0:\n",
    "        hiatus_idx = np.where(abs(ele_change) < abs(threshold_thick))\n",
    "        ages_ero[time, hiatus_idx] = ages_ero[time-1, hiatus_idx]\n",
    "ages_ero[-1, :] = end_t\n",
    "deposurf = erosurf.copy()\n",
    "hiatalsurf = erosurf.copy()\n",
    "erohiatalsurf = erosurf.copy()\n",
    "time_of_ero = erosurf.copy()\n",
    "thickness = np.zeros_like(xy_topo)\n",
    "\n",
    "# fig, ax = plt.subplots(2, 3, figsize = (19.80, 10.8), tight_layout = True)#, sharey = True, sharex = True)#, sharex = True, sharey = True)\n",
    "for time in range (1, end_t):\n",
    "    thickness[time, :] = strat[time, :]-strat[time-1, :]\n",
    "    thickness[time, :][np.isnan(thickness[time, :])] = 0\n",
    "    if hiatal_scenario == 1:  #using a percentage of the average thickness to constrain hiatal surfaces\n",
    "        threshold_thick = perc*(np.mean(thickness[time, :]))  #x% of the average sediment thickness at x time. may be negative.\n",
    "    elif hiatal_scenario == 2:  #using nth percentile of the thicknesses at each timestep\n",
    "        threshold_thick = perc*np.percentile(abs(thickness[time, :]), nth) #find the nth percentile of depositon at x time. positive number.\n",
    "\n",
    "    hiatalwhere = np.where(abs(thickness[time, :]) < threshold_thick) #where elev change is less than 10% avg\n",
    "    depositingwhere = np.where(thickness[time, :] > threshold_thick)\n",
    "    erodingwhere = np.where(thickness[time, :] < -abs(threshold_thick))\n",
    "    stratcondition[time, depositingwhere] = 0\n",
    "    stratcondition[time, erodingwhere] = 1\n",
    "    stratcondition[time, hiatalwhere] = 2\n",
    "    erodereset = np.where(stratcondition[time, :]==1)\n",
    "    erosurf[time, erodereset] = xy_topo[time, erodereset]\n",
    "    deposurf[time, depositingwhere] = xy_topo[time, depositingwhere]\n",
    "    hiatalsurf[time, hiatalwhere] = xy_topo[time, hiatalwhere]\n",
    "    erohiatalsurf[time, erodereset] = xy_topo[time, erodereset]\n",
    "    erohiatalsurf[time, hiatalwhere] = xy_topo[time, hiatalwhere]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb249faa-883a-4aef-801a-8c29dca5a564",
   "metadata": {},
   "source": [
    "## Actually Map Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81e958-ef33-4647-baa0-b2202db68de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc97f4-d434-4889-8ccb-f4b8724420eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "goflag = 'Y'\n",
    "\n",
    "labelbars = plt.figure(figsize = (20, 5), tight_layout = True, dpi = 200)\n",
    "fnlfig = plt.gca()\n",
    "\n",
    "while goflag=='Y':\n",
    "    #import matplotlib\n",
    "    mpl.use('TkAgg')\n",
    "    #%matplotlib tk\n",
    "\n",
    "    fig = plt.figure(tight_layout=True, figsize = (20, 8))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.set_aspect('20')\n",
    "    ax.set_facecolor('k')\n",
    "    \n",
    "    for strata in range (0, end_t):\n",
    "\n",
    "        elevint = xy_topo[strata]\n",
    "        vvfacies = mmp[strata] #van den berg and van gelder\n",
    "\n",
    "        pointsint = np.array([posnew, elevint]).T.reshape(-1, 1, 2)\n",
    "        segmentsint = np.concatenate([pointsint[:-1], pointsint[1:]], axis=1)\n",
    "\n",
    "        #-------- MOBILITY WITH MMP\n",
    "        vvfac = LineCollection(segmentsint, cmap = cmap_vvfac, norm = norm_vvfac)\n",
    "        vvfac.set_array(vvfacies)\n",
    "        vvfac.set_linewidth(1.5)\n",
    "        vvfac_line = ax.add_collection(vvfac)\n",
    "\n",
    "    for i in range (0, end_t):\n",
    "        plt.plot(erohiatalsurf[i], color = cpick.to_rgba(i), lw = 1)\n",
    "    ticks = np.arange(0, end_t, 25)\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "    # ax.colorbar(cpick,label=\"Time\", shrink=0.3, ticks=ticks, aspect = 20, pad = 0.1, orientation = 'horizontal')\n",
    "\n",
    "    start_time = 0 #would be start of model run\n",
    "    end_time_int = num_timesteps-1 #would be end of model run\n",
    "\n",
    "    tim_int = range(start_time,end_time) #range of time\n",
    "    xdat_int = posnew #position\n",
    "    ydat_int = [xy_topo[t] for t in tim_int] #stratigraphy/elevation @ position\n",
    "\n",
    "\n",
    "\n",
    "    hmm = np.array(plt.ginput(6)) #extract 4 coordinates from the plot\n",
    "    print(hmm)\n",
    "    # plt.close(fig)\n",
    "\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    fakecrop = copy.deepcopy(xy_topo[:, int(hmm[0, 0]):int(hmm[1, 0])])\n",
    "    # for i in range (0, num_timesteps):\n",
    "        # plt.plot(fakecrop[i, :])\n",
    "\n",
    "    if hmm.shape != (6, 2):\n",
    "        print('check coordinates!!')\n",
    "        break\n",
    "\n",
    "    # st_time = np.where(np.logical_and(np.isclose(xy_topo, hmm[2, 1], atol=0.005), (np.isclose(strat, hmm[2, 1], atol=0.005))))[0][0] ##get the row index of the start time picked from the plot\n",
    "    searchst = np.arange(int(hmm[2, 0])-5, int(hmm[2, 0])+5) #(2, 2)\n",
    "    searchen = np.arange(int(hmm[3, 0])-5, int(hmm[3, 0])+5) #(3, 3)\n",
    "    \n",
    "#     basearchst = np.arange(int(hmm[4, 0])-5, int(hmm[4, 0])+5) #(2, 2)\n",
    "#     basearchen = np.arange(int(hmm[5, 0])-5, int(hmm[5, 0])+5) #(3, 3)\n",
    "    \n",
    "    st_time = np.where(np.logical_and((np.isclose(erohiatalsurf[:, searchst], hmm[2, 1], atol=0.05)),\n",
    "                                      (np.isclose(xy_topo[:, searchst], hmm[2, 1], atol=0.05))))[0][-1]\n",
    "    \n",
    "    # bas_st_time = np.where(np.logical_and((np.isclose(erohiatalsurf[:, basearchst], hmm[4, 1], atol=0.05)),\n",
    "    #                                   (np.isclose(xy_topo[:, searchst], hmm[, 1], atol=0.05))))[0][-1]\n",
    "                                      \n",
    "    # np.where(np.isclose(erohiatalsurf[:, searchst], hmm[2, 1], atol=0.05)))\n",
    "    # np.isclose(erohiatalsurf[:, searchen], hmm[3, 1], atol=0.05))[0][0]\n",
    "    en_time = np.where(np.logical_and((np.isclose(erohiatalsurf[:, searchen], hmm[3, 1], atol=0.05)),\n",
    "                                      (np.isclose(xy_topo[:, searchen], hmm[3, 1], atol=0.05))))[0][-1]\n",
    "    bar_time = en_time-st_time\n",
    "    #en_time = np.where(np.logical_and(np.isclose(xy_topo, hmm[3, 1], atol=0.001), (np.isclose(strat, hmm[3, 1], atol=0.001))))[0][0] ##get the row index of the start time picked from the plot\n",
    "    #print(st_time)\n",
    "    print(st_time, en_time) #start and end times of bar deposition\n",
    "\n",
    "    barleft = int(hmm[0, 0])\n",
    "    barright = int(hmm[1, 0])\n",
    "    bas_surf = copy.deepcopy(xy_topo[st_time, int(hmm[4, 0]):int(hmm[5, 0])]) ## basal erosion surface explicitly\n",
    "    bas_length_lr = hmm[5, 0] - hmm[4, 0] ## straight line length of basal surface \n",
    "    \n",
    "    \n",
    "    fakecrop = fakecrop[st_time:en_time, :]\n",
    "    # for i in range (0, len(fakecrop)):\n",
    "        # plt.plot(fakecrop[i, :])\n",
    "\n",
    "    pkg_type = input('Enter bar facies type: ')\n",
    "    bargrade = input('Enter bar goodness: ')\n",
    "    coherence = 'co'+ input('intnernal coherence value: ')\n",
    "    aggmeasure = 'va'+ input('intnernal vertical agg value: ')\n",
    "    basalrelief = 'br'+ input('basal relief value: ')\n",
    "    barcode = f'{coherence}-{aggmeasure}-{basalrelief}'\n",
    "    barID = f'{iloc}-{bargrade}-{pkg_type}'\n",
    "    cropped_topo = copy.deepcopy(xy_topo[st_time:en_time+1, barleft:barright+1])\n",
    "\n",
    "    #lt.colorbar(cpick,label=\"Time\")\n",
    "\n",
    "    ####Pull flow depth data\n",
    "\n",
    "    maxdep = np.nanmax(trueflow[st_time:en_time+1, :])\n",
    "    depCV = stats.variation(trueflow[st_time:en_time+1, :][~np.isnan(trueflow[st_time:en_time+1, :])].ravel())\n",
    "\n",
    "\n",
    "    ## Crop EVERYTHING\n",
    "\n",
    "    ## crop the strat matrix too\n",
    "    xy_topoc = copy.deepcopy(cropped_topo)\n",
    "    positionc = position[barleft:barright+1]\n",
    "    stratc = xy_strat[st_time:en_time+1, barleft:barright+1]\n",
    "    #stratconditionc = stratcondition[st_time:en_time+1, barleft:barright+1]\n",
    "\n",
    "    end_tc = len(xy_topoc)\n",
    "    num_timestepsc = end_tc\n",
    "\n",
    "    shearc = shear[st_time:en_time+1, barleft:barright+1]\n",
    "    scaleflowc = scaleflow[st_time:en_time+1, barleft:barright+1]\n",
    "    trueflowc = trueflow[st_time:en_time+1, barleft:barright+1]\n",
    "    flowvelc = flowvel[st_time:en_time+1, barleft:barright+1]\n",
    "    ehsurfc = erohiatalsurf[st_time:en_time+1, barleft:barright+1]\n",
    "\n",
    "    #=============MAKE MASTER ARRAY FOR BAR PKG, EXPORT============================\n",
    "    if not os.path.exists(f'/Volumes/SAF_Data/NAYS2DH_files/Data/nparrays/barpkg-arrays/Summer-bpkg-redo/{iloc}'):\n",
    "        os.makedirs(f'/Volumes/SAF_Data/NAYS2DH_files/Data/nparrays/barpkg-arrays/Summer-bpkg-redo/{iloc}')\n",
    "\n",
    "    masterarr = np.concatenate((np.expand_dims(xy_topoc, 2), \n",
    "                                np.expand_dims(ehsurfc, 2), \n",
    "                                np.expand_dims(shearc, 2),\n",
    "                                np.expand_dims(trueflowc, 2),\n",
    "                                np.expand_dims(flowvelc, 2)), axis=2)\n",
    "    np.save(f'/Volumes/SAF_Data/NAYS2DH_files/Data/nparrays/barpkg-arrays/Summer-bpkg-redo/{iloc}/{barID}-{counter}', masterarr, allow_pickle=True)\n",
    "\n",
    "    ####————————————————— EXTRACT FLOW DEPTH STATISTICS—————————————————————————————\n",
    "    max_depth= np.nanmax(trueflowc)\n",
    "    min_depth = np.nanmin(trueflowc)\n",
    "    mean_depth = np.nanmean(trueflowc)\n",
    "    stddev_depth = np.nanstd(trueflowc)\n",
    "    mod_depth = float(stats.mode(trueflowc, nan_policy = 'omit')[0][0][0])\n",
    "    med_depth = np.nanmedian(trueflowc)\n",
    "\n",
    "    ####————————————————— EXTRACT SHEAR STRESS STATISTICS—————————————————————————————\n",
    "    max_shear= np.nanmax(shearc)\n",
    "    min_shear = np.nanmin(shearc)\n",
    "    mean_shear = np.nanmean(shearc)\n",
    "    stddev_shear = np.nanstd(shearc)\n",
    "    mod_shear = float(stats.mode(shearc, nan_policy = 'omit')[0][0][0])\n",
    "    med_shear = np.nanmedian(shearc)\n",
    "\n",
    "    ####————————————————— EXTRACT FLOW VELOCITY STATISTICS—————————————————————————————\n",
    "    max_vel= np.nanmax(flowvelc)\n",
    "    min_vel = np.nanmin(flowvelc)\n",
    "    mean_vel = np.nanmean(flowvelc)\n",
    "    stddev_vel = np.nanstd(flowvelc)\n",
    "    mod_vel = float(stats.mode(flowvelc, nan_policy = 'omit')[0][0][0])\n",
    "    med_vel = np.nanmedian(flowvelc)\n",
    "\n",
    "    ####————————————————— EXTRACT SLOPE for COHERENCE STATISTICS—————————————————————————————\n",
    "    slopes, coher_angles = movinggrad(xy_topoc, positionc)\n",
    "    ##————————————––––––––––––––––––––––––––––––GET BAR PACKAGE DIMENSIONS————————————––––––––––––––––––––––––\n",
    "    fig=plt.figure()\n",
    "    fullbar = plt.fill_between(posnew[barleft:barright+1], xy_topoc[0], xy_topoc[-1])\n",
    "    plt.close(fig)\n",
    "\n",
    "    barverts = fullbar.get_paths()[0].vertices\n",
    "    bar_polygon = Polygon(barverts) \n",
    "    bds = bar_polygon.bounds\n",
    "    bar_area = bar_polygon.area\n",
    "    left = bds[0]\n",
    "    bottom = bds[1]\n",
    "    right = bds[2]\n",
    "    top = bds[3]\n",
    "    bar_height = abs(np.nanmin(xy_topoc[-1])-np.nanmax(xy_topoc[-1]))\n",
    "    bar_width = right-left\n",
    "    bar_w_to_h = bar_width/bar_height\n",
    "    prop_area = bar_area/xs_area\n",
    "\n",
    "    bas_surf_relief = bas_surf.max()-bas_surf.min()\n",
    "    br_scale = bas_surf_relief/bar_height\n",
    "    \n",
    "    ###==================================GET CENTROIDS AND INTERNAL STATS==================================\n",
    "\n",
    "    nan_border = np.empty([end_tc, 1])\n",
    "    nan_border.shape\n",
    "    nan_border[:] = np.nan\n",
    "    #topointerp_nb = np.concatenate((nan_border, strat, nan_border), axis = 1) #nb = nan border\n",
    "    stratinterpcub_nb = np.concatenate((nan_border, xy_topoc, nan_border), axis = 1)\n",
    "\n",
    "    posnewc = np.arange(0, xy_topoc.shape[1], dtype = float)\n",
    "    posnewc_nb = np.insert(posnewc, [0], [np.nan])\n",
    "    posnewc_nb = np.insert(posnewc_nb, -1, np.nan)\n",
    "\n",
    "    SIC_startcrop = copy.deepcopy(xy_topoc)\n",
    "    SIC_startcrop = np.concatenate((nan_border, SIC_startcrop, nan_border), axis = 1)\n",
    "\n",
    "    SIC_startcrop = np.delete(SIC_startcrop, 0, axis = 0)\n",
    "    SIC_startcrop.shape\n",
    "\n",
    "    SIC_endcrop = copy.deepcopy(xy_topoc)\n",
    "    SIC_endcrop = np.concatenate((nan_border, SIC_endcrop, nan_border), axis = 1)\n",
    "\n",
    "    SIC_endcrop = np.delete(SIC_endcrop, -1, axis=0)\n",
    "    SIC_endcrop.shape\n",
    "\n",
    "    delta = SIC_startcrop-SIC_endcrop\n",
    "\n",
    "    delta[np.where(delta[:]==0)] = np.nan\n",
    "\n",
    "    #print(len(posnewc_nb))\n",
    "    #l_edges = np.empty([1,]) #left edge distance measure of the wheeler fill\n",
    "    l_idx = np.empty([1,]) #index value of left edge\n",
    "    #r_edges = np.empty([1,]) #right edge of the wheeler fill\n",
    "    r_idx = np.empty([1,]) #index value of right edge\n",
    "    surf_age = np.empty([1,]) #age of each deposit for the wheeler diagram\n",
    "    for i in range (0, len(delta)):\n",
    "        #if ~np.isnan(deposurf[i, 0]):\n",
    "        #   rectangles[i, 0] = ages[i, 0] #if the left edge of the stratigraphy is a depositional surface\n",
    "\n",
    "        for xpos in range (0, len(posnewc_nb)-1):\n",
    "            l_edge = np.all((np.isnan(delta[i, xpos]) and ~np.isnan(delta[i, xpos+1])))\n",
    "            r_edge = np.all((~np.isnan(delta[i, xpos]) and np.isnan(delta[i, xpos+1])))\n",
    "            #print(xpos, 'L', l_edge)\n",
    "            #print(xpos, 'R', r_edge)\n",
    "            if l_edge == True:\n",
    "                #l_edges = np.append(l_edges, [posnewc_nb[xpos+1]], axis = 0)\n",
    "                l_idx = np.append(l_idx, [xpos], axis = 0)\n",
    "                #print(posnewc_nb[xpos+1], 'potato')\n",
    "                surf_age = np.append(surf_age, [i], axis = 0)\n",
    "            if r_edge == True:\n",
    "                #print(xpos, 'tomato')\n",
    "                #r_edges = np.append(r_edges, [posnewc_nb[xpos-1]], axis = 0)\n",
    "                r_idx = np.append(r_idx, [xpos], axis = 0)\n",
    "\n",
    "\n",
    "    # Store all those vertices in an array that houses the time of each polygon and the left and right edges of the poly\n",
    "\n",
    "    l_idx = np.reshape(l_idx, [len(l_idx), 1])\n",
    "    #l_index = np.delete(l_index, 1)\n",
    "\n",
    "    l_idx = l_idx.astype(int)\n",
    "    r_idx = np.reshape(r_idx, [len(r_idx), 1])\n",
    "    #r_index = np.delete(r_index, 1)\n",
    "    r_idx = r_idx.astype(int)\n",
    "\n",
    "    #print(l_idx[:, 0], r_idx[:, 0])\n",
    "    surf_age = np.reshape(surf_age, [len(surf_age), 1])\n",
    "\n",
    "    # print(l_idx.shape, r_idx.shape, surf_age.shape)\n",
    "\n",
    "    vertices_b = surf_age\n",
    "    vertices_b = np.append(vertices_b, l_idx, axis = 1)\n",
    "    vertices_b = np.append(vertices_b, r_idx, axis = 1)\n",
    "\n",
    "    #print(vertices)#, vertices.shape)\n",
    "\n",
    "\n",
    "    cent_array = np.empty([len(vertices_b), 8])\n",
    "    cent_array[:] = np.nan\n",
    "    cent_mass = np.empty([len(vertices_b), 2]) ##store the centre of mass of the polygons\n",
    "    # print(cent_mass.shape)\n",
    "\n",
    "    ## Plot the filled stratigraphy, create a polygon for each, find the centroid, store the centroid and its age\n",
    "    testfig, ax1 = plt.subplots(1, 1, tight_layout=True, squeeze=True, figsize = (10,6))\n",
    "    poly_data = {}\n",
    "    for i in range (1, len(vertices_b)):\n",
    "        time = int(vertices_b[i, 0])\n",
    "        left = int(vertices_b[i, 1])\n",
    "        right = int(vertices_b[i, 2])\n",
    "\n",
    "        poly = ax1.fill_between(posnewc[left:right], xy_topoc[time, left:right], xy_topoc[time+1, left:right], color=cpick.to_rgba(i))\n",
    "        pverts = poly.get_paths()[0].vertices\n",
    "\n",
    "        polygon = Polygon(pverts) #create a shapely polygon\n",
    "        #print(polygon)\n",
    "        poly_data[i] = polygon\n",
    "        area = polygon.area\n",
    "        bounds = polygon.bounds\n",
    "        #print(type(bounds[0]))\n",
    "        cent_array[i, 4] = bounds[0]\n",
    "        cent_array[i, 5] = bounds[1]\n",
    "        cent_array[i, 6] = bounds[2]\n",
    "        cent_array[i, 7] = bounds[3]\n",
    "        cent_array[i, 3] = area\n",
    "        ctroid = polygon.centroid\n",
    "        cent_array[i, 0] = time\n",
    "        cent_array[i, 1] = ctroid.x\n",
    "        cent_array[i, 2] = ctroid.y\n",
    "\n",
    "    ptile10 = 1 ## setting an actual threshold bc some of these are still too small\n",
    "\n",
    "\n",
    "    cent_wnan = copy.deepcopy(cent_array)\n",
    "    deletewhere = np.where(cent_array[:, 3] < ptile10)\n",
    "    cent_wnan[deletewhere] = np.nan\n",
    "\n",
    "    cent_nonan = cent_wnan[~np.isnan(cent_wnan).any(axis = 1)] #delete all rows in cent_wnan matrix with nan values\n",
    "\n",
    "    poly_widths = cent_nonan[:, 6]-cent_nonan[:, 4]\n",
    "    poly_widths = np.reshape(poly_widths, [len(poly_widths), 1])\n",
    "\n",
    "    poly_heights = cent_nonan[:, 7]-cent_nonan[:, 5]\n",
    "    poly_heights = np.reshape(poly_heights, [len(poly_heights), 1])\n",
    "\n",
    "    cent_nonan = np.concatenate((cent_nonan, poly_widths, poly_heights), axis = 1)\n",
    "\n",
    "    range_x = position.max()-position.min()\n",
    "    range_y = np.nanmax(xy_topoc)-np.nanmin(xy_topoc)\n",
    "    # print(range_x, range_y)\n",
    "\n",
    "    dists = np.zeros([len(cent_nonan[:, 3]), 1])\n",
    "    reltime = np.zeros([len(cent_nonan[:, 3]), 1]) #will store the elapsed time between two consecutive centroids\n",
    "    delx = np.zeros([len(cent_nonan[:, 3]), 1])\n",
    "    dely = np.zeros([len(cent_nonan[:, 3]), 1])\n",
    "\n",
    "    for i in range (0, len(cent_nonan)-1):\n",
    "        dists[i, 0] = ((cent_nonan[i+1, 1]-cent_nonan[i, 1])**2+(cent_nonan[i+1, 2]-cent_nonan[i, 2])**2)**0.5\n",
    "        reltime[i,0] = cent_nonan[i+1, 0]-cent_nonan[i, 0]\n",
    "        delx[i, 0] = abs(cent_nonan[i+1, 1]-cent_nonan[i, 1])/range_x\n",
    "        dely[i, 0] = abs(cent_nonan[i+1, 2]-cent_nonan[i, 2])/range_y\n",
    "    cent_nonan = np.concatenate((cent_nonan, dists, reltime, delx, dely), axis = 1)\n",
    "\n",
    "\n",
    "    delx = np.delete(delx, -1, axis=0)\n",
    "    dely = np.delete(dely, -1, axis=0)\n",
    "\n",
    "    # slopes, coher_angles = movinggrad(xy_topoc, positionc)\n",
    "    skewness = stats.skew(coher_angles.ravel())\n",
    "    ve_10x = ((cent_nonan[1:, 2]-cent_nonan[:-1, 2])*10)/(cent_nonan[1:, 1]-cent_nonan[:-1, 1])\n",
    "    mean_ve_10x = np.average(ve_10x)\n",
    "    med_ve_10x = np.median(ve_10x)\n",
    "    ####————————————————— EXTRACT CLINO STATISTICS—————————————————————————————\n",
    "    mod_ht =float(stats.mode(poly_heights)[0])\n",
    "    med_ht = np.median(poly_heights)\n",
    "    mean_ht = np.mean(poly_heights)\n",
    "    max_ht = np.max(poly_heights)\n",
    "    min_ht = np.min(poly_heights)\n",
    "    stddev_ht = np.std(poly_heights)\n",
    "\n",
    "    ## Widths\n",
    "    mod_wt = float(stats.mode(poly_widths)[0])\n",
    "    med_wt = np.median(poly_widths)\n",
    "    mean_wt = np.mean(poly_widths)\n",
    "    max_wt = np.max(poly_widths)\n",
    "    min_wt = np.min(poly_widths)\n",
    "    stddev_wt = np.std(poly_widths)\n",
    "\n",
    "    ## ICDs\n",
    "    mod_icd =float(stats.mode(dists)[0])\n",
    "    med_icd = np.median(dists)\n",
    "    mean_icd = np.mean(dists)\n",
    "    max_icd = np.max(dists)\n",
    "    min_icd = np.min(dists)\n",
    "    stddev_icd = np.std(dists)\n",
    "\n",
    "    ###--------------------Relating clinos to full bar--------------------------------\n",
    "\n",
    "    packages_rel_to_bar_height = cent_nonan[:, 9]/bar_height\n",
    "    prop_of_xs_is_bar = bar_area/xs_polygon.area \n",
    "\n",
    "    note = input('any notes? ')\n",
    "    plt.close()\n",
    "\n",
    "    pkgdata = pd.DataFrame([(barID, iloc, pkg_type, barcode, st_time, en_time, bar_time, barleft, barright, bar_width, bar_height, bar_w_to_h, bar_area, xs_area, prop_area,\n",
    "                            skewness, bas_surf_relief, mean_ve_10x, med_ve_10x,\n",
    "                            min_ht, max_ht, mod_ht, mean_ht, stddev_ht, med_ht,\n",
    "                            min_wt, max_wt, mod_wt, mean_wt, stddev_wt, med_wt,\n",
    "                            min_icd, max_icd, mod_icd, mean_icd, stddev_icd, med_icd,\n",
    "                            min_depth, max_depth, mod_depth, mean_depth, stddev_depth, med_depth, \n",
    "                            min_shear, max_shear, mod_shear, mean_shear, stddev_shear, med_shear, \n",
    "                            min_vel, max_vel, mod_vel, mean_vel, stddev_vel, med_vel, note)],\n",
    "\n",
    "                           columns=['BarName', \n",
    "                                'iloc', \n",
    "                                'Facies', \n",
    "                                'Barcode',\n",
    "                                'StartTime', \n",
    "                                'EndTime', \n",
    "                                'ElapsedTime', \n",
    "                                'LeftEdge', \n",
    "                                'RightEdge', \n",
    "                                'BarWidth', \n",
    "                                'BarHeight', \n",
    "                                'BarAspect', \n",
    "                                'BarArea', \n",
    "                                'Section Area', \n",
    "                                'ChannelProp', \n",
    "                                'Angle Skewness', 'Basal Surf Relief', 'Mean dzdx', 'Median dzdx'\n",
    "                                'MinClinoHt','MaxClinoHt', 'ModeClinoHt', 'MeanClinoHt', 'SDClinoHt', 'MedClinoHt',\n",
    "                                'MinClinoWt','MaxClinoWt', 'ModeClinoWt', 'MeanClinoWt', 'SDClinoWt', 'MedClinoWt',\n",
    "                                'MinClinoICD','MaxClinoICD', 'ModeClinoICD', 'MeanClinoICD', 'SDClinoICD', 'MedClinoICD',\n",
    "                                'MinFlowDepth', 'MaxFlowDepth', 'ModeFlowDepth', 'MeanFlowDepth', 'SDFlowDepth', 'MedFlowDepth',\n",
    "                                'MinShear', 'MaxShear', 'ModeShear', 'MeanShear', 'SDShear', 'MedShear',\n",
    "                                'MinVelocity', 'MaxVelocity', 'ModeVelocity', 'MeanVelocity', 'SDVelocity', 'MedVelocity',\n",
    "                                'notes'])\n",
    "    bardata = pd.concat([bardata, pkgdata], axis = 0, ignore_index = True)\n",
    "    ## final bar plot\n",
    "    plt.plot(bas_surf)\n",
    "    counter = counter + 1 \n",
    "    goflag = input('Go flag: ')\n",
    "    \n",
    "    x, y = bar_polygon.exterior.xy\n",
    "    fnlfig.plot(x, y, label = barID)\n",
    "#fnlfig.legend()\n",
    "    \n",
    "bardata.to_csv(f'/Volumes/SAF_Data/NAYS2DH_files/Data/barCSVs/Sp2022/Summer-redo/{iloc}.csv')\n",
    "\n",
    "5# In[59]:\n",
    "\n",
    "\n",
    "bardata\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ac3f7-ce47-408a-be73-8de7a1e2b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bas_sinuousity = np.sum(bas_length_sin)/bas_length_lr\n",
    "print(bas_sinuousity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b9da28-c038-4adb-a93b-34eb9c27687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bas_surf_relief = bas_surf.max()-bas_surf.min()\n",
    "br_scale = bas_surf_relief/bar_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53a079-b030-44c8-a79a-284106ca646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "br_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8beab-7695-43b9-8d9b-1d37993a547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### time series of surface variance\n",
    "variancess = np.var(xy_topoc, where = ~np.isnan(xy_topoc), axis = 1)\n",
    "\n",
    "plt.plot(variancess)\n",
    "plt.ylabel('Variance')\n",
    "plt.xlabel('Timestep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a13bdc3-ea70-492f-8caa-d1adae667fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (50, 4))\n",
    "plt.plot(bas_surf)\n",
    "# plt.plot(xy_topoc[-8, :])\n",
    "# plt.plot(xy_topoc[-6, :])\n",
    "# plt.plot(xy_topoc[-4, :])\n",
    "plt.axhline(np.nanmean(bas_surf))\n",
    "cvbase =stats.variation(bas_surf, nan_policy = 'omit')\n",
    "plt.title(f'Variance = {np.var(bas_surf)}, CV = {cvbase}')\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b88d3-ef70-4e64-a287-56cb7d2aaec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(xy_topoc)):\n",
    "    plt.plot(xy_topoc[i])\n",
    "plt.plot(cent_array[:, 1], cent_array[:, 2], lw = 0, marker = '.')\n",
    "plt.plot(ls_regression(cent_nonan[:, 1], cent_nonan[:, 2])[1], ls_regression(cent_nonan[:, 1], cent_nonan[:, 2])[2])\n",
    "plt.title(f'RMS of centroid regression = {ls_regression(cent_nonan[:, 1], cent_nonan[:, 2])[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc4006-259f-4a2e-b8c7-125e8d210ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92a4e2-473b-427a-a76a-a59af4f6f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_growth = cent_nonan[1:, 2]-cent_nonan[:-1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a180197-7def-48af-8143-d04250ac4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs_thick = abs(np.nanmin(xy_topo[-1])-np.nanmax(xy_topo))\n",
    "plt.plot(vert_growth/xs_thick)\n",
    "plt.axhline(np.mean(vert_growth))\n",
    "plt.title('thickness change between centroids/section thickness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b82d96-c36d-445e-a925-fa1c5ec1e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, coher_angles = movinggrad(xy_topoc, positionc)\n",
    "# coh_offset = coher_angles-np.mean(coher_angles) ##distance from the mean angle i cant remember the statistical name for this\n",
    "\n",
    "# sns.histplot(coher_angles.ravel(), kde=True, bins = 9, bin_range = (0, 90))\n",
    "#sns.histplot(x=coher_angles.ravel(), y=coh_offset.ravel())\n",
    "# sns.histplot(coh_offset.ravel(), bins = 10)\n",
    "sns.histplot(coher_angles.ravel(), bins = 10, element = 'step', fill=False, legend=False, kde = False)\n",
    "plt.title('Distribution of surface slope/dip angles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd18883-492b-41fc-bbd3-4f364725ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, tight_layout = True, figsize = (10, 6))\n",
    "cmap_dip = ListedColormap(['xkcd:red', 'xkcd:orange', 'xkcd:baby blue', 'xkcd:taupe', 'xkcd:taupe', 'xkcd:baby blue', 'xkcd:orange', 'xkcd:red'])\n",
    "norm_dip = BoundaryNorm([-30, -20, -5, -2, 0, 2, 5, 20, 30], cmap_dip.N)\n",
    "# cmap_dip = ListedColormap(['xkcd:red', 'xkcd:orange', 'xkcd:baby blue', 'xkcd:taupe'])\n",
    "# norm_dip = BoundaryNorm([-30, -20, -5, -2, 0], cmap_dip.N)\n",
    "\n",
    "for t in range (0, len(xy_topoc)):\n",
    "    #slope = strat_gradient[t]\n",
    "    dip = coher_angles[t]\n",
    "    zdata = xy_topoc[t]\n",
    "    \n",
    "    points = np.array([positionc, zdata]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    \n",
    "    # slopenorm = plt.Normalize(np.nanmin(strat_gradient), np.nanmax(strat_gradient))\n",
    "    dipnorm = plt.Normalize(np.nanmin(coher_angles), 0)\n",
    "    \n",
    "    slopelc = LineCollection(segments, cmap = cmap_dip, norm = norm_dip)\n",
    "    slopelc.set_array(dip)\n",
    "    slopelc.set_linewidth(2)\n",
    "    slopeline = ax.add_collection(slopelc)\n",
    "\n",
    "ax.set_facecolor('xkcd:midnight blue')    \n",
    "ax.set_xlim(positionc.min(), positionc.max());\n",
    "ax.set_ylim(np.nanmin(xy_topoc), np.nanmax(xy_topoc));\n",
    "ax.set_aspect(20)\n",
    "cbar = fig.colorbar(slopeline, label='dip angles', ax=ax, shrink = 0.6, pad = 0.01, aspect = 10)\n",
    "cbar.ax.tick_params(labelsize=6) \n",
    "\n",
    "meanang = np.mean(coher_angles)\n",
    "sd_ang = np.std(coher_angles)\n",
    "cv_ang = stats.variation(coher_angles.ravel())\n",
    "print(sd_ang, cv_ang, meanang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe16e57-4c5e-4978-a81f-33e6c193ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(coher_angles.ravel(), bins = [-30, -20, -5, -2, 0, 2, 5, 20, 30], element = 'step', fill=False, legend=False, kde = False)\n",
    "plt.title('Distribution of surface slope/dip angles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcabe4e-01f1-42bc-881a-dcfeec124f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(coher_angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f833a-a7f1-4555-be39-a7e841d025fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "nbins = np.deg2rad(np.arange(0, 361, 1)) #10° bins\n",
    "rad_bearings = np.deg2rad(coher_angles) #these are fine, exactly the same as the degree version, duh\n",
    "bearing_radii, bearing_edges = np.histogram(rad_bearings, bins = nbins, density=False)\n",
    "bearing_centres = bearing_edges[:-1]+0.5*(bearing_edges[1]-bearing_edges[0])\n",
    "# print(bearing_radii)\n",
    "wedge_width = ((2*np.pi)/len(nbins))*np.ones(len(bearing_centres)) #10°, this is also fine. \n",
    "\n",
    "\n",
    "# rad_ang_between = np.deg2rad(ang_between)\n",
    "# ab_radii, ab_edges = np.histogram(rad_ang_between, bins = nbins, range= (0, 2*np.pi))\n",
    "# ab_centres = ab_edges[:-1]+0.5*(ab_edges[1]-ab_edges[0])\n",
    "\n",
    "# mean_bearing = np.average(coher_angles)\n",
    "# variance = abs(coher_angles-mean_bearing)\n",
    "\n",
    "# rad_variance = np.deg2rad(variance)\n",
    "# var_radii, var_edges = np.histogram(rad_variance, bins = nbins, range= (0, 2*np.pi))\n",
    "# var_centres = var_edges[:-1]+0.5*(var_edges[1]-var_edges[0])\n",
    "# print(var_radii)\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(projection='polar')\n",
    "\n",
    "# ax1 = fig.add_subplot(projection='polar')\n",
    "\n",
    "\n",
    "ax.set_theta_zero_location('N')\n",
    "\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "ax.bar(bearing_centres, bearing_radii, width = wedge_width, bottom=0)\n",
    "# ax2.bar(var_centres, var_radii, width = wedge_width, bottom=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4ce0b-bc50-409e-a81b-b53cc3918e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot((delx/dely).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f5122-8f01-47f4-9910-9a4cc68d9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 4))\n",
    "plt.plot(positionc, coher_angles[50, :], lw = 0, marker = 'o')\n",
    "plt.plot(positionc, xy_topoc[50, :], marker = '+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a326da-f24b-4eae-89af-2b364c5ca6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.histplot(dely/med_depth)\n",
    "sns.histplot(dely/max_depth, element = 'step', fill=False)\n",
    "plt.title('Vert agg metric? dely/med_depth, ins max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab05cd8-3f8c-4b67-bd12-5f3fba704722",
   "metadata": {},
   "outputs": [],
   "source": [
    "va_ratio = dely/med_depth\n",
    "sns.histplot(dely/med_depth, kde = True, label = 'VA_ratio')\n",
    "ptiles = np.quantile(va_ratio, [0.5, .75, .9])\n",
    "plt.axvline(ptiles[0], label = '50th', color = 'r')\n",
    "plt.axvline(ptiles[1], label = '75th', color = 'g')\n",
    "plt.axvline(ptiles[2], label = '90th', color = 'b')\n",
    "plt.legend()\n",
    "plt.title('VA Ratio and some percentiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ece60-cee8-404e-9242-4e976a12e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanang = np.mean(coher_angles)\n",
    "sd_ang = np.std(coher_angles)\n",
    "cv_ang = stats.variation(coher_angles.ravel())\n",
    "print(sd_ang, cv_ang, meanang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75461b5-ab5c-4b8d-87ad-13064eec9837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40128b13-1095-485a-bdac-18ad223cd027",
   "metadata": {},
   "outputs": [],
   "source": [
    "ang_between = np.empty(len(cent_nonan)-1)\n",
    "for i in range(0, len(cent_nonan)-2):\n",
    "    #print('a',cent_nonan[i, 1:3])\n",
    "    #print('b', cent_nonan[i+1, 1:3])\n",
    "    ax, ay, bx, by = cent_nonan[i:i+2, 1:3].ravel()\n",
    "    #print(ax, ay, bx, by)\n",
    "    ang_between[i] = angle_between(ax, ay, bx, by)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22dfc1b-4fac-4662-8e05-37de3ba22a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(ang_between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9935881e-616b-45dc-a89e-638dc930198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Skewness: ', stats.skew(coher_angles.ravel()))\n",
    "print('Basal relief: ', bas_surf_relief)\n",
    "sns.histplot(ve_10x.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da431909-f04a-4108-b47a-300343b82068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
