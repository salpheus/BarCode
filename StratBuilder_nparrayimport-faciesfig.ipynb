{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR FACIES PLOT ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so first things first, every other stratbuilder code is wrong, lol, yikes--and I blame NAYS and NAYS only. So apparently, heads up, if you are exporting shit from the graphing window in NAYS the indexing will be wrong because it makes everything 0, so you only cover the expansion of one of the banks, not both. \n",
    "\n",
    "This code actually uses the distance from the FINAL channel centreline to recover the actual distance/cross stream position and scales all earlier distances to that. For example, if the initial width, t0, is 10, (centreline at 0, Left Bank at 5, Right Bank at -5) and the final width is 20 (CL = 0m, LB = 10m, RB= -10m), the actual distance measure you'd use for plotting t0 is *actually* 5-15m, and tfin would be from 0-20? U know wat i mean jellybean?\n",
    "\n",
    "ANYWAY, to be able to get the right indexing you need to actually download the full resuly csvs from NAYS, ~NOT~ the freakin' csvs from the graph window. The cool part about this though is that you can now plot any section u want, not just whatever exprots you predetermine, and well you can also pull out other data as well, so there is more flexibility here, which is actually a really big deal because that manual exportation is SUCH a timesink. \n",
    "\n",
    "But, like Sridhar says, 'there's no such thing as a free lunch', and the caveat here is that the full result export takes longer and requires waaaaaaayyyyy more storage than the graph window csvs. But that's why we got hella harddrives babyguuurrrllllll\n",
    "\n",
    "now go forth, plot shit & b great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOVVsban4uU-"
   },
   "source": [
    "## Must start naming files @ 0!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dxfnM2L4uVD"
   },
   "source": [
    "Hello all, and welcome to the shitshow--\n",
    "\n",
    "This is the most recent version of the Strat Builder code, that includes:\n",
    "1. plotting stratigraphy\n",
    "2. delineating surfaces and ages of erosion, deposition and hiatus\n",
    "3. the shear stress/velocity/flow depth/gradient facies models\n",
    "4. heat maps showing the locus of the channel at every timestep\n",
    "5. Wheeler diagram of the stratigraphy\n",
    "6. Code to core stratigraphy, and pull out mean and max set thicknesses in time and space \n",
    "7. Centroids of each depositional package, grouped by vector of motion between timesteps (clunky)\n",
    "8. Code to create 'field scale' stratigraphy using the percentile/% reworking method (described in a later ramble below)\n",
    "9. Eventually-- code to do 5-8 for the new, field scale stratigraphy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "id": "UpkwI8oJ4uVE"
   },
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T13:26:40.814782Z",
     "start_time": "2021-04-21T13:26:39.428591Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30854,
     "status": "ok",
     "timestamp": 1616012075897,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "hide_input": false,
    "id": "UfJ8KbOl4uVE",
    "outputId": "b09c468a-be2a-423e-a950-95008f74745d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import math \n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.colors as mcol\n",
    "import matplotlib.cm as cm\n",
    "import time\n",
    "import random\n",
    "import statistics as stat\n",
    "\n",
    "from xml.dom import minidom\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "#from descartes import PolygonPatch\n",
    "from PIL import Image\n",
    "from scipy.spatial import distance\n",
    "from scipy import signal\n",
    "from scipy import interpolate\n",
    "from scipy.stats import variation\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from IPython.display import Image\n",
    "#from celluloid import Camera\n",
    "\n",
    "#plt.style.use('seaborn-white')\n",
    "\n",
    "from matplotlib.collections import LineCollection, PatchCollection\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "%matplotlib inline\n",
    "#%matplotlib qt\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABjUenJS4uVF"
   },
   "source": [
    "## Setting up all the aesthetics:\n",
    "plot fonts, colourmaps (for timestep, shears tress facies and gradient) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T13:26:40.819677Z",
     "start_time": "2021-04-21T13:26:40.816985Z"
    },
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1616012087286,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "hide_input": true,
    "id": "i5syel9j4uVG"
   },
   "outputs": [],
   "source": [
    "font = {'family' : 'Helvetica',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 10}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "kwargs = dict(edgecolor = 'k', fc = 'xkcd:greyish', alpha=0.5, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T13:26:41.548612Z",
     "start_time": "2021-04-21T13:26:41.389288Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2644,
     "status": "ok",
     "timestamp": 1616012090978,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "hide_input": false,
    "id": "_al9IuVW4uVG",
    "outputId": "bafdf1f6-e3d3-4ba0-c113-698a054892a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted successfully!\n",
      "converted successfully!\n",
      "converted successfully!\n",
      "converted successfully!\n",
      "converted successfully!\n"
     ]
    }
   ],
   "source": [
    " ## source of this function: http://schubert.atmos.colostate.edu/~cslocum/custom_cmap.html#code\n",
    "def make_cmap(colors, position=None, bit=False):\n",
    "\n",
    "    if len(position) != len(colors):\n",
    "        sys.exit('position length must be the same as colors')\n",
    "    elif position[0] != 0 or position[-1] != 1:\n",
    "        sys.exit('position must start with 0 and end with 1')\n",
    "    \n",
    "    cdict = {'red':[], 'green':[], 'blue':[]}\n",
    "    for pos, color in zip(position, colors):\n",
    "\n",
    "        cdict['red'].append((pos, color[0], color[0]))\n",
    "        cdict['green'].append((pos, color[1], color[1]))\n",
    "        cdict['blue'].append((pos, color[2], color[2]))\n",
    "\n",
    "        cmap = mpl.colors.LinearSegmentedColormap('my_colormap',cdict,256)\n",
    "\n",
    "    return cmap\n",
    "\n",
    "## load source xml file\n",
    "xmldoc = minidom.parse('/Volumes/SAF_MSCWORK/Python/colourmaps/mellow-rainbow.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('Point')\n",
    "data_vals=[]\n",
    "color_vals=[]\n",
    "\n",
    "for s in itemlist:\n",
    "    \n",
    "    data_vals.append(float(s.attributes['x'].value))\n",
    "    color_vals.append((float(s.attributes['r'].value),\n",
    "    float(s.attributes['g'].value),\n",
    "    float(s.attributes['b'].value)))\n",
    "\n",
    "## construct the colormap\n",
    "\n",
    "mycmap = make_cmap(color_vals,data_vals) ##this is the main rainbow colourmap to be used in the code\n",
    "print('converted successfully!')\n",
    "\n",
    "#mycmap_r = ListedColormap(mycmap.colors[::-1])\n",
    "\n",
    "# mycmap is matplotlib compatible object. to query color value out of it:\n",
    "\n",
    "#print('example rgba value for data value 0 is: ' + str(mycmap(0.0)))\n",
    "\n",
    "## load source xml file\n",
    "xmldoc = minidom.parse('/Volumes/SAF_MSCWORK/Python/colourmaps/brown-2.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('Point')\n",
    "data_vals=[]\n",
    "color_vals=[]\n",
    "\n",
    "for s in itemlist:\n",
    "    \n",
    "    data_vals.append(float(s.attributes['x'].value))\n",
    "    color_vals.append((float(s.attributes['r'].value),\n",
    "    float(s.attributes['g'].value),\n",
    "    float(s.attributes['b'].value)))\n",
    "\n",
    "## construct the colormap\n",
    "\n",
    "ss_facies = make_cmap(color_vals,data_vals) ##this is the main rainbow colourmap to be used in the code\n",
    "print('converted successfully!')\n",
    "\n",
    "## load source xml file\n",
    "xmldoc = minidom.parse('/Volumes/SAF_MSCWORK/Python/colourmaps/green-brown-div.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('Point')\n",
    "data_vals=[]\n",
    "color_vals=[]\n",
    "\n",
    "for s in itemlist:\n",
    "    \n",
    "    data_vals.append(float(s.attributes['x'].value))\n",
    "    color_vals.append((float(s.attributes['r'].value),\n",
    "    float(s.attributes['g'].value),\n",
    "    float(s.attributes['b'].value)))\n",
    "\n",
    "## construct the colormap\n",
    "\n",
    "grads = make_cmap(color_vals,data_vals) ##this is the main rainbow colourmap to be used in the code\n",
    "print('converted successfully!')\n",
    "\n",
    "xmldoc = minidom.parse('/Volumes/SAF_MSCWORK/Python/colourmaps/blue-3.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('Point')\n",
    "data_vals=[]\n",
    "color_vals=[]\n",
    "\n",
    "for s in itemlist:\n",
    "    \n",
    "    data_vals.append(float(s.attributes['x'].value))\n",
    "    color_vals.append((float(s.attributes['r'].value),\n",
    "    float(s.attributes['g'].value),\n",
    "    float(s.attributes['b'].value)))\n",
    "\n",
    "## construct the colormap\n",
    "\n",
    "depths = make_cmap(color_vals,data_vals) ##this is the main rainbow colourmap to be used in the code\n",
    "print('converted successfully!')\n",
    "\n",
    "##for velocities\n",
    "\n",
    "xmldoc = minidom.parse('/Volumes/SAF_MSCWORK/Python/colourmaps/orange-6.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('Point')\n",
    "data_vals=[]\n",
    "color_vals=[]\n",
    "\n",
    "for s in itemlist:\n",
    "    \n",
    "    data_vals.append(float(s.attributes['x'].value))\n",
    "    color_vals.append((float(s.attributes['r'].value),\n",
    "    float(s.attributes['g'].value),\n",
    "    float(s.attributes['b'].value)))\n",
    "\n",
    "## construct the colormap\n",
    "\n",
    "vels = make_cmap(color_vals,data_vals) ##this is the main rainbow colourmap to be used in the code\n",
    "print('converted successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T13:26:42.019016Z",
     "start_time": "2021-04-21T13:26:42.014474Z"
    },
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1616012096398,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "hide_input": true,
    "id": "q8rGrmVD4uVH"
   },
   "outputs": [],
   "source": [
    "def reverse_colourmap(cmap, name = 'my_cmap_r'):\n",
    "    reverse = []\n",
    "    k = []   \n",
    "\n",
    "    for key in cmap._segmentdata:    \n",
    "        k.append(key)\n",
    "        channel = cmap._segmentdata[key]\n",
    "        data = []\n",
    "\n",
    "        for t in channel:                    \n",
    "            data.append((1-t[0],t[2],t[1]))            \n",
    "        reverse.append(sorted(data))    \n",
    "\n",
    "    LinearL = dict(zip(k,reverse))\n",
    "    my_cmap_r = mpl.colors.LinearSegmentedColormap(name, LinearL) \n",
    "    return my_cmap_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T13:26:42.452319Z",
     "start_time": "2021-04-21T13:26:42.449769Z"
    },
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1616012097503,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "hide_input": true,
    "id": "5ZvyK8_d4uVH"
   },
   "outputs": [],
   "source": [
    "ss_facies_r = reverse_colourmap(ss_facies)\n",
    "timesteps_r = reverse_colourmap(mycmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RGObugk4uVI"
   },
   "source": [
    "Important model parameters that might be used in calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T13:26:43.311059Z",
     "start_time": "2021-04-21T13:26:43.307868Z"
    },
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1616012099576,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "hide_input": true,
    "id": "Xk47jzkH4uVI"
   },
   "outputs": [],
   "source": [
    "def display_gif(fn):\n",
    "    from IPython import display\n",
    "    return display.HTML('<img src=\"{}\">'.format(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:15:17.148916Z",
     "start_time": "2021-04-07T21:15:17.143629Z"
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1616012100018,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "La49XvqT4uVI"
   },
   "outputs": [],
   "source": [
    "Q = str(100)# discharge in m3/s\n",
    "slope = 0.00137#gradient\n",
    "d50 = 0.31e-3 #d50 grain size in m\n",
    "iloc = 175 #location of the section along i axis\n",
    "jloc = 14#location of the secion along j axis\n",
    "\n",
    "datnam = 'bh2-datamaster-facies.npy' #name of the data file to upload\n",
    "dataloc = 'data-agubh2_0hrflood' #where csv files are\n",
    "floodnam=''\n",
    "arrayfolder = 'c-agubh2' #where raw centroid data array stored\n",
    "mainsurfto = 'ms-agubh2' #where bounding surfaces array will go\n",
    "iricoutputt =600 #output time from the model, s\n",
    "cellW = 4\n",
    "cellL = 10\n",
    "xloc = iloc*cellW\n",
    "xsloc = iloc*cellL\n",
    "#print(xsloc)\n",
    "spacing = 1 #spacing of cross stream x locations, m\n",
    "\n",
    "ps = 2650 # bulk density of quartz kg/m3\n",
    "p = 1000 # density of water in kg/m3\n",
    "nu = 1.787*10e-6 #kinematic viscosity of water in m2/s\n",
    "nu=1.0533e-6\n",
    "g = 9.81 # acceleration due to gravity, m/s2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S7JkgH74uVI"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:15:37.805590Z",
     "start_time": "2021-04-07T21:15:18.401534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/SAF_MSCWORK/Python/Stratigraphy/Data/ConvertedArrays/data-agubh2_0hrflood/bh2-datamaster-facies.npy\n"
     ]
    }
   ],
   "source": [
    "print(f'/Volumes/SAF_MSCWORK/Python/Stratigraphy/Data/ConvertedArrays/{dataloc}/{datnam}')\n",
    "datamaster = np.load(f'/Volumes/SAF_MSCWORK/Python/Stratigraphy/Data/ConvertedArrays/{dataloc}/{datnam}', allow_pickle = True)\n",
    "\n",
    "fldlength = 48\n",
    "savefilesto = '/Volumes/SAF_MSCWORK/Python/Stratigraphy/Plots/'\n",
    "#modelrun = f'agubh2-hr-{fldlength}hrfld-{iloc}'\n",
    "modelrun = f'agubh2-{floodnam}-{iloc}'\n",
    "\n",
    "##define grid dimensions\n",
    "gridx = 501\n",
    "gridy = 26\n",
    "\n",
    "datacond = 1\n",
    "if datacond == 1:\n",
    "    cells = gridy\n",
    "else:\n",
    "    cells = gridx\n",
    "length = 501 #length of the domain in the x direction\n",
    "erostart = 5\n",
    "erostop = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:15:37.817708Z",
     "start_time": "2021-04-07T21:15:37.809899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2046\n",
      "2046\n",
      "(13026, 11, 2046)\n"
     ]
    }
   ],
   "source": [
    "num_timesteps = datamaster.shape[2]#len(os.listdir(filepathcore))-1\n",
    "print(num_timesteps)\n",
    "\n",
    "position = np.arange(0, length, dtype = float)\n",
    "coevelev = np.empty([num_timesteps])\n",
    "len(coevelev)\n",
    "#print(coevelev[40])\n",
    "\n",
    "interval_to_plot = 120/60 #we want to plot every ___  HOURS \n",
    "end_t = num_timesteps#len(np.arange(1, num_timesteps, skipstep)) #number of timesteps in data master array\n",
    "print(end_t)\n",
    "\n",
    "fldstart = ((820800+3600)/3600)/interval_to_plot ##flood starttime, s\n",
    "\n",
    "\n",
    "print(datamaster.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP--Ok I LITERALLY DONT KNOW WHY THIS IS HAPPENING BUT YOU CANT PLOT THE WIDTH (REACH WIDTH) AND GET THE INDEXING RIGHT FOR THE ACTUAL STRATIGRAPHY.\n",
    "SO UNTIL YOU FIX THAT YOU'L HAVE TO ACTIVATE/REACTIVATE THIS CELL AND RESTART THE KERNEL AND RELOAD THE DATA TO GET PROPERLY INDEXED START/ IDFK, AND \n",
    "## Plot up some reach stuff to get oriented\n",
    "\n",
    "(1) Width stats over the whole reach, through time\n",
    "(2) Pic of the final planform (or planform at whatever t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reach_data = np.load(f'/Users/safiya/Documents/Python/Stratigraphy/Data/ConvertedArrays/{dataloc}/{datnam}', allow_pickle = True)\n",
    " ##will have all the data for the reach width change plot\n",
    "for i in range (0, end_t):\n",
    "    reach_data[:, :, i] = reach_data[:, :, i][np.argsort(reach_data[:, :, i][:, 0])] #sort big file by I loc (i.e. XS order)\n",
    "\n",
    "chw_reach = np.empty([gridx-1, 3, end_t])\n",
    "chw_reach[:] = np.nan #array has 3 columns: 0 = min, 1 = max, 2 = mean\n",
    "for t in range (0, end_t): #have to find widths at each timestep\n",
    "    #print(t)\n",
    "    for iloc in range (1, gridx): #have to scan each section\n",
    "        sectionIdx = np.where(reach_data[:, 0, t]==iloc) #find all j positions at each iloc\n",
    "        section = reach_data[sectionIdx, 3, t] #pull the y position, m of each j position in the matrix, to calculate width\n",
    "        \n",
    "        #populate the chw_reach array with the right bank (min, -ve) and left bank (max, +ve)\n",
    "        chw_reach[iloc-1, 0, t] = section.min()\n",
    "        chw_reach[iloc-1, 1, t] = section.max()\n",
    "\n",
    "    chw_reach[:, 2, t] = chw_reach[:, 1, t]-chw_reach[:, 0, t] #calculate the channel width at that XS location \n",
    "to_delete = np.arange(-erostop, erostart)     \n",
    "chw_reach = np.delete(chw_reach, to_delete, axis=0) #deleting parts of the domain that there is no erosion\n",
    "t_range  = np.arange(0, end_t*interval_to_plot, step = interval_to_plot)\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.plot(t_range, chw_reach[:, 2, :].min(axis=0), 'v', c = 'k', ms = '5', mec = 'r', label = 'Minimum CW Along Reach')\n",
    "plt.plot(t_range, chw_reach[:, 2, :].max(axis=0), '^', c = 'k', ms = '5', mec = 'r', label = 'Maximum CW Along Reach')\n",
    "plt.plot(t_range, np.mean(chw_reach[:, 2, :], axis=0), '+', c = 'k', ms = '5', mew = 3, label = 'Mean CW Along Reach')\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "#plt.ylim(-500, 400)\n",
    "#plt.xlim(0, num_dim2)\n",
    "plt.xlabel(f'Timestep, hours')\n",
    "plt.ylabel('Channel Width, m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T16:26:15.189888Z",
     "start_time": "2021-03-23T16:26:14.224771Z"
    }
   },
   "source": [
    "#create 2D grid\n",
    "#extract elevation\n",
    "\n",
    "xdomain = np.reshape(datamaster[:, 2, -1], (gridy, gridx))\n",
    "ydomain = np.reshape(datamaster[ :, 3, -1] , (gridy, gridx))\n",
    "elev = np.reshape(datamaster[:, 4, -1], (gridy, gridx)) ##using flow depth to plot\n",
    "\n",
    "\n",
    "llim = datamaster[:, 4, :].min()\n",
    "ulim = datamaster[:, 4, :].max()\n",
    "\n",
    "#find the max and min of the variable you're plotting to scale the colourmap\n",
    "var_llim = datamaster[:, 4, :].min() \n",
    "var_ulim = datamaster[:, 4, :].max()\n",
    "print(var_llim, var_ulim)\n",
    "\n",
    "class MidpointNormalize(mcol.Normalize):\n",
    "    def __init__(self, vmin=None, vmax=None, vcenter=None, clip=False):\n",
    "        self.vcenter = vcenter\n",
    "        mcol.Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        # I'm ignoring masked values and all kinds of edge cases to make a\n",
    "        # simple example...\n",
    "        x, y = [self.vmin, self.vcenter, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "#normalise = plt.Normalize(-1, 2)\n",
    "#midnorm = MidpointNormalize(vmin=-np.quantile(datamaster[:, 4, :], 0.1), vcenter=0.5, vmax=np.quantile(datamaster[:, 4, :], 0.99))\n",
    "midnorm = MidpointNormalize(vmin=-1.5, vcenter=0, vmax=2) \n",
    "\n",
    "plt.figure(figsize = (10, 2), tight_layout = True, dpi = 200)\n",
    "plt.ylabel('Distance rel \\n to centreline, m')\n",
    "plt.xlabel('Streamwise distance, m')\n",
    "\n",
    "plt.axvline(xsloc, c = 'r', ls = '--')\n",
    "dat = plt.pcolormesh(xdomain, ydomain, elev, cmap = 'tab20b_r', norm = midnorm, shading = 'gouraud', alpha = 1)\n",
    "plt.colorbar(dat, label = 'elev, m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Crop and Sort the data so its in section order and not streamwise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:15:46.086162Z",
     "start_time": "2021-04-07T21:15:45.217501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 11, 2046)\n"
     ]
    }
   ],
   "source": [
    "#crop data\n",
    "\n",
    "remove_ilocs = np.where(datamaster[:, 0, :] != iloc)\n",
    "data = np.delete(datamaster, remove_ilocs[0], axis=0)\n",
    "print(data.shape)\n",
    "\n",
    "#sort in cross stream direction from rightbank to left bank\n",
    "#test = np.empty_like(datamaster)    \n",
    "for i in range (0, datamaster.shape[2]):\n",
    "    data[:, :, i] = data[:, :, i][np.argsort(data[:, :, i][:, 3])]\n",
    "    #print(i, data[:, 3, i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now fill in arrays for elevation, position, shear etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:15:46.912922Z",
     "start_time": "2021-04-07T21:15:46.905819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2046, 26)\n"
     ]
    }
   ],
   "source": [
    "##Make the arrays to store data\n",
    "stratigraphy = np.empty([end_t, cells]) ##will hold data for topography accounting for changes due to erosion\n",
    "#print(elevation.shape) #elevation = np.empty([n, end_t])\n",
    "\n",
    "print(stratigraphy.shape)\n",
    "\n",
    "#this matrix is to record unmodified stratigraphy in the same shape as the eroded strat\n",
    "stratigraphy_idx = stratigraphy.copy()\n",
    "#stratigraphy_idx[:] =  np.nan\n",
    "#print(stratigraphy)\n",
    "\n",
    "shearstresseroded = stratigraphy.copy() #will hold data for shear stress accounting for changes due to erosion\n",
    "#shearstresseroded[:] = np.nan\n",
    "#print('!!!', shearstresseroded.shape)\n",
    "\n",
    "stratflowdepth =  stratigraphy.copy() #will hold data for flow depth accounting for changes due to erosion\n",
    "#stratflowdepth[:] = np.nan\n",
    "\n",
    "scaleflowdepth =  stratigraphy.copy() #will hold data for local flow depth scaled to max accounting for changes due to erosion\n",
    "#scaleflowdepth[:] = np.nan\n",
    "\n",
    "froudedata =  stratigraphy.copy() #will hold data for local flow depth scaled to max accounting for changes due to erosion\n",
    "#froudedata[:] = np.nan\n",
    "\n",
    "velocity =  stratigraphy.copy()\n",
    "runtime = len(stratigraphy)\n",
    "#print(runtime)\n",
    "xposition =  stratigraphy.copy()\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:18:42.937289Z",
     "start_time": "2021-04-07T21:18:42.871225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -50.          -51.11371773  -51.63738684 ... -118.08872167\n",
      "  -118.09702251 -118.10208086]\n",
      " [ -46.          -47.02462031  -47.5063959  ... -108.83539539\n",
      "  -108.84302194 -108.84751296]\n",
      " [ -42.          -42.9355229   -43.37540495 ...  -99.5820691\n",
      "   -99.58902137  -99.59294507]\n",
      " ...\n",
      " [  42.           42.9355229    43.37540495 ...   94.73778288\n",
      "    94.74499059   94.75298075]\n",
      " [  46.           47.02462031   47.5063959  ...  103.99110917\n",
      "   103.99899115  104.00754865]\n",
      " [  50.           51.11371773   51.63738684 ...  113.24443545\n",
      "   113.25299173  113.26211655]]\n",
      "113.262116546 -118.102080857\n",
      "(2046, 234)\n"
     ]
    }
   ],
   "source": [
    "in_section = np.where(datamaster[:, 0, -1]==iloc)\n",
    "in_section = in_section[0]\n",
    "bankpos = datamaster[:, 3, :][in_section]\n",
    "print(bankpos)\n",
    "rightbank = bankpos.min() ##negativee\n",
    "leftbank = bankpos.max() #positive\n",
    "print(leftbank, rightbank)\n",
    "\n",
    "xy_strat = np.empty([end_t, int(3+np.round((leftbank-rightbank)/spacing, 0))]) # will put stratigraphies here, in proper x pos\n",
    "print(xy_strat.shape)\n",
    "xy_strat[:] = np.nan\n",
    "ages = np.empty_like(xy_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:18:55.449519Z",
     "start_time": "2021-04-07T21:18:55.424176Z"
    }
   },
   "outputs": [],
   "source": [
    "## Import the data\n",
    "for time in range (0, data.shape[2]): #TIME\n",
    "    #print(stratigraphy[time, :].shape)\n",
    "    stratigraphy[time, :] = data[:, 7, time] #elevation change, elevation in 5\n",
    "    shearstresseroded[time, :] = data[:, 6, time] \n",
    "    stratflowdepth[time, :] = data[:, 4, time]\n",
    "    froudedata[time, :] = data[:, 9, time]\n",
    "    velocity[time, :] = data[:, 10, time]\n",
    "    \n",
    "    ypos = data[:, 3, time]-rightbank #coreect supid centreline indexing\n",
    "    #print(ypos)\n",
    "    xposition[time, :] = ypos\n",
    "\n",
    "stratigraphy_idx = stratigraphy.copy()\n",
    "\n",
    "##you need to change the y positions from centreline position to actual positional data\n",
    "#print(xposition.max(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:18:57.795130Z",
     "start_time": "2021-04-07T21:18:57.769581Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43567,
     "status": "ok",
     "timestamp": 1616014269921,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "g_6iKJ3s4uVL",
    "outputId": "fb5f5ccb-67b1-4dcf-ef5f-e4ad7f520265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2046, 234) (2046, 234) (2046, 234) (2046, 234) (2046, 234)\n"
     ]
    }
   ],
   "source": [
    "## these arrays will house interpolated data\n",
    "shear = xy_strat.copy()\n",
    "froude = xy_strat.copy()\n",
    "scaleflow = xy_strat.copy() #flow depth scaled to max per time\n",
    "trueflow = xy_strat.copy() #unscaled flow depth\n",
    "flowvel = xy_strat.copy() #flow velocity\n",
    "\n",
    "print(shear.shape, froude.shape, scaleflow.shape, trueflow.shape, flowvel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:19:00.935993Z",
     "start_time": "2021-04-07T21:19:00.933020Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43235,
     "status": "ok",
     "timestamp": 1616014269921,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "UAdHnNqM4uVM",
    "outputId": "563089d9-ef48-4ab7-f3c6-ce5d9ec04e70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "print(stratigraphy.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:19:06.647840Z",
     "start_time": "2021-04-07T21:19:06.424810Z"
    },
    "executionInfo": {
     "elapsed": 42262,
     "status": "ok",
     "timestamp": 1616014269922,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "KFYjB5X94uVM"
   },
   "outputs": [],
   "source": [
    "#put all data values in their correct x/index position\n",
    "for t in range (0, end_t):\n",
    "    for idx, x in zip(np.arange(0, stratigraphy.shape[1]), xposition[t, :]):\n",
    "        #print(x)\n",
    "        x = int(np.floor(x)) #rounding down positions, making integers so can use as index\n",
    "        #print(x, idx)\n",
    "        xy_strat[t, x] = stratigraphy[t, idx]\n",
    "        shear[t, x] = shearstresseroded[t, idx]\n",
    "        froude[t, x] = froudedata[t, idx]\n",
    "        trueflow[t, x] = stratflowdepth[t, idx]\n",
    "        scaleflow[t, x] = scaleflowdepth[t, idx]\n",
    "        flowvel[t, x] = velocity[t, idx]\n",
    "    #plt.plot(xy_strat[t, :], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:19:10.943991Z",
     "start_time": "2021-04-07T21:19:08.713385Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xy_topo = np.empty_like(xy_strat)\n",
    "xy_topo[:] = np.nan\n",
    "for t in range (0, end_t):\n",
    "    #print(t)\n",
    "    length = int(np.floor(xposition[t, -1]-xposition[t, 0])) #length of the section at time, t\n",
    "    pos = np.linspace(0, length, length) #create a metre scale array with each x pos = location\n",
    "    #dataint = np.linspace(int(np.round(xposition[t, 0], 0)), int(np.round(xposition[t, -1], 0)), length, dtype=int) #range of locations to interpolate over?\n",
    "    dataint = np.arange(xposition[t, 0], np.round(xposition[t, -1], 0), dtype=int)\n",
    "    #print(t, pos.max(), length)\n",
    "    #print(len(dataint))\n",
    "    #print(pos)\n",
    "    \n",
    "    stratnotnan = xy_strat[t, :][~np.isnan(xy_strat[t, :])] #pull out real values of strat\n",
    "    shearnotnan = shear[t, :][~np.isnan(shear[t, :])] #pull out real values of shear\n",
    "    froudenotnan = froude[t, :][~np.isnan(froude[t, :])] #pull out real values of froude\n",
    "    truefnotnan = trueflow[t, :][~np.isnan(trueflow[t, :])] #pull out real values of true flow depth\n",
    "    #scalefnotnan = scaleflow[t, :][~np.isnan(scaleflow[t, :])] #pull out real values of scaled flow depth\n",
    "    velnotnan = flowvel[t, :][~np.isnan(flowvel[t, :])] #pull out real values of strat\n",
    "    #print(shear[t, :][~np.isnan(shear[t, :])])\n",
    "    #print(shearnotnan[:].shape, froudenotnan[:].shape, truefnotnan[:].shape, velnotnan[:].shape)\n",
    "    #print(xposition[t, :])\n",
    "    #print(dataint)\n",
    "    fx = interpolate.interp1d(xposition[t, :], stratnotnan[:], kind = 'cubic', fill_value = 'extrapolate') #stratigraphy interpolation\n",
    "    #print(fx)\n",
    "    fsh = interpolate.interp1d(xposition[t, :], shearnotnan[:], kind = 'cubic', fill_value = 'extrapolate') #shear stress interpolation\n",
    "    ffr = interpolate.interp1d(xposition[t, :], froudenotnan[:], kind = 'cubic', fill_value = 'extrapolate') #froude number interpolation\n",
    "    ftf = interpolate.interp1d(xposition[t, :], truefnotnan[:], kind = 'cubic', fill_value = 'extrapolate') #true flow depth interpolation\n",
    "    #fsf = interpolate.interp1d(xposition[t, :], scalefnotnan[:], kind = 'cubic') #scaled flow depth interpolation\n",
    "    ffv = interpolate.interp1d(xposition[t, :], velnotnan[:], kind = 'cubic', fill_value = 'extrapolate') #flow veloity interpolation\n",
    "    \n",
    "    #print(fx(dataint))\n",
    "    \n",
    "    xy_topo[t, dataint] = fx(dataint) #reassign strat\n",
    "    #print(xy_topo[t, :])\n",
    "    shear[t, dataint] = fsh(dataint) #reassign shear\n",
    "    froude[t, dataint] = ffr(dataint) #reassign froude\n",
    "    trueflow[t, dataint] = ftf(dataint) #reassign true fd\n",
    "    #scaleflow[t, 0:length] = fsf(pos) #reassign scaled fd\n",
    "    flowvel[t, dataint] = ffv(dataint) #reassign flow vel\n",
    "    \n",
    "    #plt.plot(xy_topo[t, :])\n",
    "#plt.ylim(-3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tF74__Am4uVM"
   },
   "source": [
    "Now I have an array with one column of cross-stream distance data and n columns of elevation data...now we plot. \n",
    "Because we are eroding to the position it was before then the shear stress must also be what it was before, therefore we can do thge same thing with stratgiraphy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:19:17.256151Z",
     "start_time": "2021-04-07T21:19:17.244156Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40898,
     "status": "ok",
     "timestamp": 1616014270077,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "953kXuke4uVN",
    "outputId": "4a0f1c18-0f1c-45f3-b981-dd14a9f2eab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2046, 234)\n",
      "(2046, 234)\n"
     ]
    }
   ],
   "source": [
    "#each of these conditional arrays have a border of nans to helo pick out the 'packages'\n",
    "stratcondition = np.zeros_like(xy_topo)\n",
    "stratcondition[:] = np.nan\n",
    "print(stratcondition.shape)\n",
    "\n",
    "\n",
    "erosurf = np.empty([end_t, xy_topo.shape[1]])\n",
    "print(erosurf.shape)\n",
    "erosurf[:] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:02.723385Z",
     "start_time": "2021-04-07T21:19:57.143127Z"
    },
    "executionInfo": {
     "elapsed": 41044,
     "status": "ok",
     "timestamp": 1616014270735,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "TilC6Tq14uVN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strat = xy_topo.copy()\n",
    "for time in range (0, end_t):\n",
    "    #plt.plot(stratigraphy[time]) #this is just a check to make sure youre importing the data correctly\n",
    "    \n",
    "    #now we have stratigraphy at each timestep with time on the j axis and position on the i\n",
    "    #we want to check each location at each timestep for areas that have erodible elevation (i.e. higher than current timestep)\n",
    "    \n",
    "    for space in range (0, xy_topo.shape[1]):\n",
    "        #print(space)\n",
    "        preexisting_strata = xy_topo[:time, :] #this is our search array, where we will erode\n",
    "        #preexisting_strata_idx = stratigraphy_idx[:time, :]\n",
    "        \n",
    "        #print(preexisting_strata.shape)\n",
    "        #print('xy', xy_strat[time, :].shape)\n",
    "        willerode = np.where(preexisting_strata[:, space] > xy_topo[time, space])\n",
    "        #print('willerode', willerode)\n",
    "        \n",
    "        #print(time, column, willerode)\n",
    "        \n",
    "        xy_topo[willerode, space] = xy_topo[time, space]\n",
    "        ages[willerode, space] = time\n",
    "        #stratigraphy_idx[willerode_idx, column] == np.nan\n",
    "    #print(stratigraphy_idx[time])\n",
    "    \n",
    "\n",
    "#plt.ylim(0.1, 4)\n",
    "    #print('st', stratigraphy)\n",
    "#print('sh', shearstresseroded)\n",
    "#print(ages[:5])\n",
    "## fill an areas with data 'above it' in the section\n",
    "\n",
    "for i in range (end_t-2, -1, -1):\n",
    "    \n",
    "    fillinx = np.where(np.isnan(xy_topo[i, :]))\n",
    "    xy_topo[i, fillinx] = xy_topo[i+1, fillinx]\n",
    "    \n",
    "    ## fill in all filled in areas in the strat condition matrix with the flag for erosion\n",
    "    stratcondition[i, fillinx] = 1\n",
    "    #print(stratcondition[i, fillinx])\n",
    "    \n",
    "    fillinsh = np.where(np.isnan(shear[i, :]))\n",
    "    shear[i, fillinsh] = shear[i+1, fillinsh]\n",
    "    \n",
    "    fillinfr = np.where(np.isnan(froude[i, :]))\n",
    "    froude[i, fillinfr] = froude[i+1, fillinfr]\n",
    "    \n",
    "    fillintf = np.where(np.isnan(trueflow[i, :]))\n",
    "    trueflow[i, fillintf] = trueflow[i+1, fillintf]\n",
    "    \n",
    "    fillinsf = np.where(np.isnan(scaleflow[i, :]))\n",
    "    scaleflow[i, fillinsf] = scaleflow[i+1, fillinsf]\n",
    "    \n",
    "    fillinfv = np.where(np.isnan(flowvel[i, :]))\n",
    "    flowvel[i, fillinfv] = flowvel[i+1, fillinfv]\n",
    "    \n",
    "    fillinstrat = np.where(np.isnan(strat[i, :]))\n",
    "    strat[i, fillinstrat] = strat[i+1, fillinstrat]\n",
    "    \n",
    "    #print('no fill', xy_topo[i, fillin])\n",
    "    \n",
    "    #print('fill', xy_topo[i, fillin])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T15:57:32.609382Z",
     "start_time": "2021-03-23T15:57:29.867085Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 41237,
     "status": "ok",
     "timestamp": 1616014271452,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "pWi8QQXv4uVN",
    "outputId": "fe84810b-1581-448d-976e-a92eb85487fc"
   },
   "source": [
    "plt.figure(figsize = (10, 4))\n",
    "for i in range (0, end_t):\n",
    "    plt.plot(strat[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T15:57:35.108830Z",
     "start_time": "2021-03-23T15:57:32.611524Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 40066,
     "status": "ok",
     "timestamp": 1616014271841,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "9TYJqm-A4uVO",
    "outputId": "91f3d444-eb91-42ce-fdc4-c56f95ad0b61"
   },
   "source": [
    "plt.figure(figsize = (10, 4))\n",
    "for i in range(0, end_t):\n",
    "    plt.plot(xy_topo[i, :], 'k', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T13:11:12.373515Z",
     "start_time": "2021-03-23T13:11:11.658492Z"
    }
   },
   "source": [
    "fig, ax = plt.subplots(3, figsize = (12, 12), tight_layout = True)\n",
    "preflood_t = int(fldstart/interval_to_plot)\n",
    "print(preflood_t)\n",
    "endflood_t = int((fldstart+fldlength)/interval_to_plot)\n",
    "print(endflood_t)\n",
    "for i in range(0, preflood_t):\n",
    "    ax[0].plot(xy_topo[i, :], 'k', alpha = 0.5)\n",
    "    \n",
    "for i in range(0, endflood_t): \n",
    "    ax[1].plot(xy_topo[i, :], 'k', alpha = 0.5)\n",
    "\n",
    "for i in range(0, end_t):\n",
    "    ax[2].plot(xy_topo[i, :], 'k', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:12.033318Z",
     "start_time": "2021-04-07T21:20:12.026977Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37969,
     "status": "ok",
     "timestamp": 1616014271842,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "hjLK-Wor4uVO",
    "outputId": "508a969d-7af9-4116-fefb-773c7a0bfbde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.where(stratcondition==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:20.617962Z",
     "start_time": "2021-04-07T21:20:20.614468Z"
    },
    "executionInfo": {
     "elapsed": 36419,
     "status": "ok",
     "timestamp": 1616014272037,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "OX_o99mx4uVO"
   },
   "outputs": [],
   "source": [
    "def rounddown(num_timesteps, lineint):\n",
    "    return num_timesteps - (num_timesteps%lineint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOTSTe364uVO"
   },
   "source": [
    "## Declare maximum flow depth for scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:22.938329Z",
     "start_time": "2021-04-07T21:20:22.041711Z"
    },
    "executionInfo": {
     "elapsed": 35226,
     "status": "ok",
     "timestamp": 1616014272038,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "3ghxct7e4uVP"
   },
   "outputs": [],
   "source": [
    "maxflow = np.empty([end_t, 1])\n",
    "maxflow[:] = np.nan\n",
    "for i in range (0, end_t):\n",
    "    maxflow[i] = np.amax(trueflow[i]) #find the max flowdepth at each time step for flow depth scaling\n",
    "#maxflow = stratflowdepth.max()\n",
    "#print(maxflow[:5])\n",
    "\n",
    "#scale flow depth values across the matrix to the max flow depth per timestep\n",
    "for row in range (0, end_t):\n",
    "    for column in range(0, scaleflow.shape[1]):\n",
    "        scaleflow[row, column] = trueflow[row, column]/maxflow[row]\n",
    "#print(stratflowdepth[0, :5])\n",
    "#print(scaleflowdepth[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:23.881888Z",
     "start_time": "2021-04-07T21:20:23.693157Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 34727,
     "status": "ok",
     "timestamp": 1616014272038,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "9z4BzHs84uVP",
    "outputId": "970515b4-af44-4c41-b030-6e9ba794b53a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(maxflow.max())\n",
    "plt.hist(stratflowdepth.ravel())\n",
    "flowdep = np.mean(stratflowdepth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPIUlbb64uVP"
   },
   "source": [
    "###### Set up the grids for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:26.281601Z",
     "start_time": "2021-04-07T21:20:26.277366Z"
    },
    "executionInfo": {
     "elapsed": 33689,
     "status": "ok",
     "timestamp": 1616014272039,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "6Nge8uwA4uVP"
   },
   "outputs": [],
   "source": [
    "start_time = 0 #would be start of model run\n",
    "end_time = end_t #would be end of model run\n",
    "\n",
    "tim = range(start_time,end_time) #range of time\n",
    "\n",
    "position = np.arange(0, xy_topo.shape[1], dtype = float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjQiavSI4uVP"
   },
   "source": [
    "## Define formulae:\n",
    "\n",
    "<b>Calulating moving gravient along the stratigraphy </b> this shit dont work, take t from an old code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1616016509799,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "V1j0l9_M4uVP"
   },
   "source": [
    "gradientdata = np.empty([end_t, n])\n",
    "def movinggrad(yax, xax, ydata, xdata): #want a function to create a matrix of the gradient between two grid cells across domain\n",
    "    \n",
    "    gradmatrix = np.empty([yax, xax]) #matrix will look like plot axes so yax = rows = strat data, xax = cols\n",
    "    print(gradmatrix.shape)\n",
    "    for row in range (0, ydata.shape[0]):\n",
    "      print(row)\n",
    "      for k in range (0, xax):\n",
    "        print(k)\n",
    "        if ydata.shape[0]<=1:\n",
    "          gradmatrix[k] = (ydata[k+1]-ydata[k])/(xdata[k+1]-xdata[k])\n",
    "        else:  \n",
    "          gradmatrix[row, k] = (ydata[row, k+1]-ydata[row, k])/(xdata[k+1]-xdata[k])\n",
    "    return gradmatrix   \n",
    "    #print(gradmatrix)\n",
    "    #gradientdata[:] = gradmatrix[:]\n",
    "    #print('gradients', gradientdata)\n",
    "    #gradientdata[:] = np.degrees(np.arctan(gradientdata[:]))\n",
    "    #print('angles', gradientdata)\n",
    "    #print(gradmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoHy_hxA4uVP"
   },
   "source": [
    "<b> Calculating the shear stress boundaries from the model data </b>\n",
    "\n",
    "Calculating thresholds to predict bedforms from shear stress a la Baas et al., 2015\n",
    "\n",
    "Assuming a quartz mineralogy\n",
    "\n",
    "D50 (Limaye) = 0.42mm\n",
    "D50 (North Loup) = 0.31mm\n",
    "\n",
    "For medium sand: \n",
    "    psi = 2 – 1\t\n",
    "    D50 = 0.25 – 0.5\t\n",
    "    Dimensionless Shield's Parameter = 0.048 – 0.033\t\n",
    "    Critical bed shear stress (N/m2) = 0.194 – 0.27\n",
    "\n",
    "https://pubs.usgs.gov/sir/2008/5093/table7.html\n",
    "\n",
    "\n",
    "From Baas et al, T = excess bed shear stress parameter:\n",
    "\n",
    "T = (τb - τb, cr)/τb, cr\n",
    "<br> where: τb = bed shear stress based on skin friction instead of form drag and τb, cr = critical bed shear stress for sediment movement\n",
    "    \n",
    "<br>D* = [g((ρs/ρ)-1)/(ν^2)]^1/3 * D50\n",
    "<br>where g = gravity, ρs = sediment density, ρ = fluid density, ν = kinematic viscosity of water (1.787*10-6 m2/s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:28.730697Z",
     "start_time": "2021-04-07T21:20:28.724408Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31664,
     "status": "ok",
     "timestamp": 1616014272224,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "hrWrFfBf4uVQ",
    "outputId": "cd0fbb48-0d3a-4095-a0a3-b430f01a4204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.574918331981756 2.2821055520670046 0.8520415984542138\n"
     ]
    }
   ],
   "source": [
    "#Tdunes = 17\n",
    "#Tanti = 25 #assuming transition is maximum threshold for dune vs higher order bedform for uniform \n",
    "#Tmripples = 12 # T for megaripples & dunes\n",
    "#Trippleslow = 3 #T for miniripples\n",
    "if d50 == 0.31e-3:\n",
    "    Tarray = [0, 0.07, 0.3, 0.6, 1, 10]\n",
    "    #Tanti = 1\n",
    "    #Tdunes = 0.3\n",
    "    #Twashdun = 0.6\n",
    "    #Tripples = 0.07 #using the mobility parameter graph\n",
    "\n",
    "if d50 == 1e-3:\n",
    "    Tarray = [0, 0.05, 0.07, 0.4, 1, 10]\n",
    "    \n",
    "tb = p*g*slope*flowdep #shear stress on bed usng mean flow depth of entire run\n",
    "tbcr = tb/((ps-p)*g*d50)  #critical bed shear stress for sediment movement\n",
    "partsize = np.power((g*((ps/p)-1))/(nu**2), (1/3))*d50 #for medium sand\n",
    "\n",
    "print(partsize, tbcr, flowdep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRZ8W81m4uVQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJO3HiEP4uVQ"
   },
   "source": [
    "<b>Define formula for calculating centroid </b>\n",
    "\n",
    "Calculated as mean x mean y. Requires x y data in :, 2 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:31.317624Z",
     "start_time": "2021-04-07T21:20:31.314027Z"
    },
    "executionInfo": {
     "elapsed": 30455,
     "status": "ok",
     "timestamp": 1616014272225,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "ByxXX27g4uVQ"
   },
   "outputs": [],
   "source": [
    "def centroidpython(data):\n",
    "    x, y = zip(*data)\n",
    "    l = len(x)\n",
    "    return sum(x) / l, sum(y) / l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4G9IqN294uVQ"
   },
   "source": [
    "<b> To find ideal number of bins for histogram using Sturges rule: n = 1 + 3.22 log (n) </b>\n",
    "\n",
    "This really is only for normal data, but whatever, otherwise, declare your own nbins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:32.213797Z",
     "start_time": "2021-04-07T21:20:32.210239Z"
    },
    "executionInfo": {
     "elapsed": 29091,
     "status": "ok",
     "timestamp": 1616014272225,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "66P3c8bR4uVQ"
   },
   "outputs": [],
   "source": [
    "def roundup(number, upto):\n",
    "    rounded = round(number/upto)*upto\n",
    "    return rounded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPkPj_vq4uVQ"
   },
   "source": [
    "## Plot of stratigraphy only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:34.565959Z",
     "start_time": "2021-04-07T21:20:34.561716Z"
    },
    "executionInfo": {
     "elapsed": 28443,
     "status": "ok",
     "timestamp": 1616014272225,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "7arfqtVH4uVQ"
   },
   "outputs": [],
   "source": [
    "# Make a user-defined colormap.\n",
    "cm1 = mycmap\n",
    "cnorm = mcol.Normalize(vmin=min(tim),vmax=max(tim))\n",
    "\n",
    "# Turn these into an object that can be used to map time values to colors and can be passed to plt.colorbar().\n",
    "cpick = cm.ScalarMappable(norm=cnorm,cmap=timesteps_r) \n",
    "cpick.set_array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lSnNuUQ4uVR"
   },
   "source": [
    "## Plot the froude and WSE profile of the run (if in streamwise direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:36.834033Z",
     "start_time": "2021-04-07T21:20:36.826865Z"
    },
    "executionInfo": {
     "elapsed": 27708,
     "status": "ok",
     "timestamp": 1616014272226,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "-vfVSVHD4uVR"
   },
   "outputs": [],
   "source": [
    "if datacond == 2: \n",
    "    fig, ax1 = plt.subplots(figsize = (19.2, 6.80))\n",
    "    water = ax1.plot(wse[:, 0], wse[:, -1], 'b', lw = '2', label = 'Water Surface Elevation')\n",
    "    bed = ax1. plot(elevation[:, 0], elevation[:, -1], 'k', lw = '2', label = 'Bed Elevation')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    fr = ax2.plot(froudenumber[:, 0], froudenumber[:, -1], 'r', lw = '1', label = 'Froude Number')\n",
    "    ax2.plot([0, elevation[-1, 0]], [1, 1], 'k--', lw = '1', alpha = 0.5)\n",
    "    \n",
    "    ax1.set_ylabel('Elevation, (m)', color = 'b')\n",
    "    ax1.tick_params(axis = 'y', labelcolor = 'b')\n",
    "    ax1.set_xlabel('Downstream Distance, (m)')\n",
    "    \n",
    "    ax2.tick_params(axis = 'y', labelcolor = 'r')\n",
    "    ax2.set_ylabel('Froude Number', color = 'r')\n",
    "    \n",
    "    lns = water + bed + fr\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    plt.legend(lns, labs, loc=0)\n",
    "    plt.title('Bed Conditions at end of run')\n",
    "    plt.savefig(savefilesto+modelrun+'bedcond' + '.png', format='png', dpi = 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:37.438724Z",
     "start_time": "2021-04-07T21:20:37.435278Z"
    },
    "executionInfo": {
     "elapsed": 27211,
     "status": "ok",
     "timestamp": 1616014272226,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "A4rw5J6g4uVR"
   },
   "outputs": [],
   "source": [
    "ages_ero = np.empty_like(xy_topo)\n",
    "posnew = np.arange(0, xy_topo.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miWZaWNH4uVR"
   },
   "source": [
    "## Declaring what the threshold for a hiatal surface is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:39.177610Z",
     "start_time": "2021-04-07T21:20:39.173886Z"
    },
    "executionInfo": {
     "elapsed": 26230,
     "status": "ok",
     "timestamp": 1616014272227,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "QhFVo-yL4uVR"
   },
   "outputs": [],
   "source": [
    "## SCENARIO 1: Hiatus = any elevation change that is less than x cm\n",
    "\n",
    "## SCENARIO 2: Hiatus = any elevation change that is less than x % of the average elevation change\n",
    "\n",
    "## SCENARIO 3: Hiatus = any elevation change that is less than the xth percentile of change at the timestep\n",
    "hiatal_scenario = 2\n",
    "\n",
    "threshold_thick = 0.005  #absolute value in metres \n",
    "\n",
    "nth = 25 #what percentile distribution to use to calculate hiatal surfaces for each timestep\n",
    "perc = 0.1 #fraction of percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:41.531046Z",
     "start_time": "2021-04-07T21:20:41.295007Z"
    },
    "executionInfo": {
     "elapsed": 25799,
     "status": "ok",
     "timestamp": 1616014272227,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "WiPeM6hA4uVS"
   },
   "outputs": [],
   "source": [
    "#print(len(posnew))\n",
    "for time in range (1, end_t):\n",
    "    #print(time)\n",
    "    ages_ero[time, :] = time \n",
    "    #stratinterpcub[time, :] = topointerp[time, :]\n",
    "    \n",
    "    lessthan = np.where(strat[time, :] < xy_topo[time-1, :])\n",
    "    ele_change = strat[time, :]-xy_topo[time-1, :] #find the bed elevation change across the domain\n",
    "                                                                   #it has to be before you reassign topography else \n",
    "                                                                   #you will get zeros where replaced\n",
    "    #print(ele_change)\n",
    "    #for space in range (0, ages_ero.shape[1]):\n",
    "        \n",
    "        #preexisting_strata_int = stratinterpcub[:time, :] #this is our search array, where we will erode\n",
    "        \n",
    "    #willerode_int = np.where(ele_change < 0)\n",
    "    #print(willerode_int)\n",
    "        #stratinterpcub[willerode_int, space] = stratinterpcub[time, space] #tune the stratigraphy for erosion\n",
    "        \n",
    "    ages_ero[time, lessthan] = ages_ero[time-1, lessthan]\n",
    "        \n",
    "    \n",
    "    if hiatal_scenario == 1:\n",
    "        threshold_thick = perc*(np.mean(ele_change)) #find the fraction of the average elevation change between the two latest timesteps\n",
    "    \n",
    "    if hiatal_scenario == 2:\n",
    "        threshold_thick = perc*np.percentile(ele_change, nth)\n",
    "    \n",
    "    if time != 0:\n",
    "        hiatus_idx = np.where(abs(ele_change) < abs(threshold_thick))\n",
    "    #print(time, hiatus_idx)\n",
    "    #print('change', ele_change)\n",
    "    #print(time, hiatalrange, hiatus_idx)\n",
    "    #print(time, np.mean(ele_change), hiatalrange)\n",
    "        ages_ero[time, hiatus_idx] = ages_ero[time-1, hiatus_idx]\n",
    "ages_ero[-1, :] = end_t\n",
    "        #print(ages_ero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K_uao1A4uVS"
   },
   "source": [
    "## Now we construct the conditions of deposition, hiatus or erosion\n",
    "\n",
    "This might be something to come back to becuase I currently do not have a working, transferrable definition of a hiatal surface, but for shits and giggles sake, since we are working with a field scale simulation and we expect m scale bars, lets say that a hiatal surface is a surface that aggrades less than 1mm ~ 0.1% of the elevation of the bar i.e. 0.99 x topo ≤ topo[t] ≤ 1.01 x topo[t-1]\n",
    "\n",
    "Let's define erosion where topo[t] < 0.99 x topo[t]\n",
    "\n",
    "Lets define deposition where topo[t] > 1.01 x topo[t-1]\n",
    "\n",
    "Then we will make an array that houses the condition of the surfaces only (stratcondition) defined by whether the subsequent surface is eroding, depositing or non-depositing sediment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06YwKxT24uVS"
   },
   "source": [
    "## Making the arrays that store erosion, hiatal, deposition and erosional and hiatal surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:20:43.198278Z",
     "start_time": "2021-04-07T21:20:43.182000Z"
    },
    "executionInfo": {
     "elapsed": 24224,
     "status": "ok",
     "timestamp": 1616014272390,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "lloIvTWf4uVS"
   },
   "outputs": [],
   "source": [
    "#these will all be filled with nans\n",
    "deposurf = erosurf.copy()\n",
    "hiatalsurf = erosurf.copy()\n",
    "erohiatalsurf = erosurf.copy()\n",
    "time_of_ero = erosurf.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyVeOcZq4uVS"
   },
   "source": [
    "## Create stratcondition (0, 1, 2 for depo, ero, hiatal) and surface arrays with nans and topo of ero/depo/hiatal surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T16:39:17.810246Z",
     "start_time": "2021-03-21T16:39:17.801463Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23114,
     "status": "ok",
     "timestamp": 1616014272390,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "tqImrz2l4uVS",
    "outputId": "20c45ac9-19c7-4e61-cf48-98e497fbe29d"
   },
   "source": [
    "test = np.where(stratcondition[:-1, :]==1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:21:25.573768Z",
     "start_time": "2021-04-07T21:21:11.538473Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23936,
     "status": "ok",
     "timestamp": 1616014273797,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "_nYu8ljm4uVT",
    "outputId": "e1c9dc2f-5f5d-4a0f-d9bd-a8612a1c3816"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    }
   ],
   "source": [
    "#stratcondition = np.zeros_like(xy_topo)\n",
    "#print(stratcondition.shape)\n",
    "\n",
    "thickness = np.zeros_like(xy_topo)\n",
    "#print(thickness.shape)\n",
    "\n",
    "#erosurf[stratcondition==1] = xy_topo[stratcondition==1]\n",
    "\n",
    "#hiatal_prop = 0.1 #multiple for proportion of thickness change defining a hiatal surface (decimal) \n",
    "                #i.e. a surface is hiatal if its elevation changes by less than 10% (of the percentile or average)\n",
    "\n",
    "#stratcondition[0, :] = 0\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize = (19.80, 10.8), tight_layout = True)#, sharey = True, sharex = True)#, sharex = True, sharey = True)\n",
    "\n",
    "for time in range (1, end_t):\n",
    "    \n",
    "    \n",
    "    #ax[0,0].plot(posnew, topointerp[time], color=cpick.to_rgba(time))\n",
    "    thickness[time, :] = strat[time, :]-strat[time-1, :]\n",
    "    thickness[time, :][np.isnan(thickness[time, :])] = 0\n",
    "    #print(thickness[time, :])\n",
    "    ax[0,1].plot(time, thickness[time, :].max(), marker = 'o', mew = 0, markerfacecolor = 'xkcd:cobalt blue')\n",
    "    ax[0,1].plot(time, thickness[time, :].min(), marker = 'o', mew = 0, markerfacecolor = 'xkcd:green yellow')\n",
    "    ax[0,1].plot(time, np.average(thickness[time, :]), marker = 'o', mew = 0, markerfacecolor = 'xkcd:coral')\n",
    "    #print(thickness[time, :].max(), thickness[time, :].min(), np.average(thickness[time, :]))\n",
    "    #print(thickness[thickness>0])\n",
    "    #ax[0, 1].set_xlim(0, end_t)\n",
    "    #ax[0, 1].set_ylim(-1, 1)\n",
    "    if hiatal_scenario == 1:\n",
    "            #using a percentage of the average thickness to constrain hiatal surfaces\n",
    "        threshold_thick = perc*(np.mean(thickness[time, :]))  #x% of the average sediment thickness at x time. may be negative.\n",
    "    elif hiatal_scenario == 2:\n",
    "            #using nth percentile of the thicknesses at each timestep\n",
    "        threshold_thick = perc*np.percentile(abs(thickness[time, :]), nth) #find the nth percentile of depositon at x time. positive number.\n",
    "    \n",
    "    ax[0,0].plot(time, threshold_thick, 'o', markerfacecolor = cpick.to_rgba(time), markeredgecolor = cpick.to_rgba(time), linewidth = 0.0001, markersize = 14, alpha = 0.5)\n",
    "    \n",
    "    \n",
    "    ax[0,0].set_ylim(-0.05, .05)\n",
    "    #print('10th', np.percentile(abs(thickness), nth), 'thresh', threshold_thick)\n",
    "    \n",
    "    hiatalwhere = np.where(abs(thickness[time, :]) < threshold_thick) #where elev change is less than 10% avg\n",
    "    depositingwhere = np.where(thickness[time, :] > threshold_thick)\n",
    "    erodingwhere = np.where(thickness[time, :] < -abs(threshold_thick))\n",
    "    #print(time, depositing where)\n",
    "    \n",
    "    \n",
    "    #print(time, 'hiatus', hiatalwhere)\n",
    "    #print('hdata', thickness[hiatalwhere])\n",
    "    #print(time, 'ero', erodingwhere)\n",
    "    #print('edata', thickness[thickness<0])\n",
    "    \n",
    "    #print(time, stratcondition[time, :])\n",
    "    \n",
    "    stratcondition[time, depositingwhere] = 0\n",
    "    stratcondition[time, erodingwhere] = 1\n",
    "    stratcondition[time, hiatalwhere] = 2\n",
    "    erodereset = np.where(stratcondition[time, :]==1)\n",
    "   \n",
    "    #print(erodingwhere, erodereset)\n",
    "    #print(stratcondition[time, :])\n",
    "    #print(erodereset)\n",
    "    #erosurf[time, erodingwhere] = xy_topo[time, erodingwhere]\n",
    "    erosurf[time, erodereset] = xy_topo[time, erodereset]\n",
    "    deposurf[time, depositingwhere] = xy_topo[time, depositingwhere]\n",
    "    hiatalsurf[time, hiatalwhere] = xy_topo[time, hiatalwhere]\n",
    "    erohiatalsurf[time, erodereset] = xy_topo[time, erodereset]\n",
    "    erohiatalsurf[time, hiatalwhere] = xy_topo[time, hiatalwhere]\n",
    "    #print('surf', erosurf[time, 220:])\n",
    "    \n",
    "    ax[1,0].plot(erosurf[time], color=cpick.to_rgba(time))\n",
    "    ax[1,1].plot(hiatalsurf[time], color=cpick.to_rgba(time))\n",
    "    ax[0, 2].plot(deposurf[time], color=cpick.to_rgba(time))\n",
    "    #ax[1, 2].plot(posnew, erohiatalsurf[time], color=cpick.to_rgba(time))\n",
    "    ax[1, 2].plot(erosurf[time], 'r')\n",
    "    ax[1, 2].plot(hiatalsurf[time], 'g')\n",
    "#erohiatalsurf[-1, :] = xy_topo[-1, :]\n",
    "\n",
    "#ax[1, 3].hist(percentile)\n",
    "#ax[0, 3].hist(abs(thickness).flatten())\n",
    "#ax[0, 3].axvline(x=np.percentile(abs(thickness), 1), c = 'r')   \n",
    "ax[0,0].title.set_text('Time Series of Threshold thickness vals')\n",
    "ax[0,1].title.set_text('time seties of bed elevation changes')\n",
    "ax[0,2].title.set_text('Depositional Surfaces Only')\n",
    "ax[1,0].title.set_text('Erosion Surfaces Only')\n",
    "ax[1,1].title.set_text('Hiatal Surfaces Only')\n",
    "ax[1,2].title.set_text('Erosional and Hiatal Surfaces')\n",
    "#ax[0, 3].title.set_text('Distribution of ALL thicknesses, red line = '+str(nth)+'th Percentile')\n",
    "#ax[1, 3].title.set_text('Distribution of ALL percentiles/timestep')\n",
    "\n",
    "ax[0, 1].legend()\n",
    "ax[0,0].set_facecolor('xkcd:grey') \n",
    "ax[0,1].set_facecolor('xkcd:grey') \n",
    "ax[0,2].set_facecolor('xkcd:grey') \n",
    "ax[1,0].set_facecolor('xkcd:grey') \n",
    "ax[1,1].set_facecolor('xkcd:grey') \n",
    "ax[1,2].set_facecolor('xkcd:grey') \n",
    "#print(thickness[3, :])\n",
    "#print(stratcondition)#[3, :])\n",
    "#np.where(stratcondition == 1)\n",
    "\n",
    "#plt.savefig(savefilesto+'surf_creation/'+modelrun+'testforhiat.png', format='png', dpi = 200)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T16:28:55.830874Z",
     "start_time": "2021-03-23T16:28:55.696656Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 23455,
     "status": "ok",
     "timestamp": 1616014273798,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "dPkV3wA24uVT",
    "outputId": "11df6d06-2ec9-417e-dbdb-c79542975b68"
   },
   "source": [
    "hist = plt.hist(ages_ero[stratcondition==2], **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:21:25.578379Z",
     "start_time": "2021-04-07T21:21:25.576230Z"
    },
    "executionInfo": {
     "elapsed": 22969,
     "status": "ok",
     "timestamp": 1616014273798,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "2CAbX1H84uVT"
   },
   "outputs": [],
   "source": [
    "#np.savetxt('/content/gdrive/My Drive/NAYS2DH/erosurfs_MT.csv', erohiatalsurf, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUWu5Fp64uVT"
   },
   "source": [
    "## histogram of thickness changes in entire stratigraphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T16:28:56.630718Z",
     "start_time": "2021-03-23T16:28:55.838132Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "executionInfo": {
     "elapsed": 22541,
     "status": "ok",
     "timestamp": 1616014274745,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "JoeAYn6C4uVT",
    "outputId": "f7010af3-e70d-4b8e-8266-7a1ce374476e"
   },
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (10, 4), tight_layout = True)\n",
    "hist = ax[0].hist(thickness.ravel(), bins = 50, ec = 'k', fc = 'xkcd:greyish', alpha = 0.5)\n",
    "ax[0].set_xlim(thickness.min(), thickness.max())\n",
    "thickavg = np.average(thickness, axis = 1)\n",
    "loc = np.average(thickavg)\n",
    "scale = np.std(thickavg)\n",
    "x = np.linspace(thickavg.min(), thickavg.max())\n",
    "y = stats.norm.pdf(x, loc, scale)\n",
    "\n",
    "ax[1].plot(x, y, label = 'PDF')\n",
    "ax[0].set_xlabel('Amount of elevation change at each xloc, m')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_title('Thickness change at every xpos/timestep \\n throughout the entire model run n = '+str(len(thickness.ravel())))\n",
    "#ax[0].set_xlim(-0.1, 0.1)\n",
    "\n",
    "avghist = ax[1].hist(thickavg, bins = 25, ec = 'k', fc = 'xkcd:greyish', alpha = 0.75, density = True, label = 'Actual data')\n",
    "ax[1].set_title('Avg thickness across entire XS per timestep')\n",
    "ax[1].set_xlabel('Thickness change (avg), across entire bed')\n",
    "ax[1].legend()\n",
    "plt.savefig(savefilesto+'stats/'+modelrun+'erodephist.png', dpi = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:21:25.586024Z",
     "start_time": "2021-04-07T21:21:25.581031Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21677,
     "status": "ok",
     "timestamp": 1616014274745,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "GpsXSFaQ4uVT",
    "outputId": "afbb776c-65a8-411b-c3c1-cef0e6d08048"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(thickness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hIP60LN4uVT"
   },
   "source": [
    "## so we have the ages of the stratigraphy\n",
    "technically an erosion surface will be all the same age? \n",
    "\n",
    "Sambrook smith et al, 2019 did a similar thing where they found the main topographic surfaces by collecting the topography that connects the erosional points\n",
    "\n",
    "I'm going to try to do that somehow\n",
    "\n",
    "ages_ero is the age of all the erosion surfaces\n",
    "erosurf houses all the erosional surfaces in their space time structure\n",
    "if we find age at erosurf and plot stratinterbcub at that time then we might have done it????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4Fi2E3L4uVU"
   },
   "source": [
    "## Plot Stratigraphy with erosurfaces and hiatals colourised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:21:28.796600Z",
     "start_time": "2021-04-07T21:21:28.663065Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 19336,
     "status": "ok",
     "timestamp": 1616014274746,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "gRYJeibK4uVU",
    "outputId": "9a686a1a-16aa-41ab-be5e-34fc448b8703"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## assuming bed shear stress output from nays is skin friction not form drag, \n",
    "# calculate mobility parameter and particle size parameter to predict facies\n",
    "#Tparam = (shearstresscub-tbcr)/tbcr #using the excess bed shear stress parameter\n",
    "Tparam = shear/(g*(ps-p)*d50)#using the mobility parameter\n",
    "hist = plt.hist(Tparam.ravel())\n",
    "#plt.hist(bedshear_crit.ravel(), fc='r')\n",
    "#plt.hist(bedshear_vel.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBvBWUob4uVV"
   },
   "source": [
    "## four subplots only (elev, depth, shear, velocity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T13:12:53.875985Z",
     "start_time": "2021-03-23T13:12:48.703782Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 26811,
     "status": "ok",
     "timestamp": 1616014283580,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "0qGMdv3Q4uVV",
    "outputId": "8b3e93a6-0fe1-4622-bb4f-6921fbce8cc8"
   },
   "source": [
    "start_time = 0 #would be start of model run\n",
    "end_time_int = num_timesteps-1 #would be end of model run\n",
    "\n",
    "tim_int = range(start_time,end_time) #range of time\n",
    "xdat_int = posnew #position\n",
    "ydat_int = [xy_topo[t] for t in tim_int] #stratigraphy/elevation @ position\n",
    "\n",
    "cmap_fac = ListedColormap(['xkcd:mud brown', 'xkcd:dirt', 'xkcd:sandy brown', 'xkcd:beige', 'xkcd:stone'])\n",
    "norm_fac = BoundaryNorm(Tarray, cmap_fac.N)\n",
    "\n",
    "\n",
    "    \n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=True, sharey=True, squeeze = True, tight_layout=True, figsize=(8, 8), dpi = 200)\n",
    "\n",
    "for y, t in zip(ydat_int,tim_int):\n",
    "    strat = ax1.plot(xdat_int,y,color=cpick.to_rgba(t), lw = 3, alpha = 1)\n",
    "\n",
    "fig.colorbar(cpick,label=\"Time\", ax=ax1)\n",
    "\n",
    "for strata in range (0, end_t):\n",
    "    \n",
    "    ax1.set_xlim((posnew.min(), posnew.max())) #braided test\n",
    "    #ax1.set_xlim((4000, 5000))\n",
    "    #ax1.set_ylim((-2.5, 2.5))\n",
    "    ax1.set_ylim((xy_topo.min(), xy_topo.max())) #braided test\n",
    "    \n",
    "    #plotting shear stress gradient\n",
    "\n",
    "    elevint = xy_topo[strata]\n",
    "    shearstressint = shear[strata]\n",
    "    depthint = scaleflow[strata]\n",
    "    #froudenumint = froudecub[strata]\n",
    "    #slopeint = gradcub[strata]\n",
    "    #froudenum[0] = is.nan\n",
    "    velint = flowvel[strata]\n",
    "    transportstage = Tparam[strata]\n",
    " \n",
    "    pointsint = np.array([posnew, elevint]).T.reshape(-1, 1, 2)\n",
    "    segmentsint = np.concatenate([pointsint[:-1], pointsint[1:]], axis=1)\n",
    "\n",
    "    #sheargradint = plt.Normalize(0, 1.5)\n",
    "    sheargradint = plt.Normalize(shear.min(), shear.max()) #bed shear based on skin friction for anti dunes = 4.410)\n",
    "    \n",
    "    \n",
    "    \n",
    "    gradlcint = LineCollection(segmentsint, cmap=plt.get_cmap(ss_facies_r), norm=sheargradint)\n",
    "    #gradlcint = LineCollection(segmentsint, cmap=cmap, norm=normbd)\n",
    "    gradlcint.set_array(shearstressint)\n",
    "    gradlcint.set_linewidth(3)\n",
    "    \n",
    "    gradlineint = ax3.add_collection(gradlcint)\n",
    "\n",
    "    ## Plotting with shear stress zones\n",
    "    \n",
    "    \n",
    "    \n",
    "    #boundlc = LineCollection(segmentsint, cmap=cmap, norm=normbd)\n",
    "    #boundlc.set_array(shearstressint)\n",
    "    #boundlc.set_linewidth(1.5)\n",
    "    \n",
    "    #boundline = ax2.add_collection(boundlc)\n",
    "    \n",
    "    #scale flow depth \n",
    "    depthgradint = plt.Normalize(depthint.min(), depthint.max())\n",
    "    \n",
    "    depthlcint = LineCollection(segmentsint, cmap=plt.get_cmap(depths), norm=depthgradint)\n",
    "    depthlcint.set_array(depthint)\n",
    "    depthlcint.set_linewidth(3)\n",
    "    \n",
    "    depthlineint = ax2.add_collection(depthlcint)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #froudegradint = plt.Normalize(froudenumint.min(), froudenumint.max())\n",
    "    \n",
    "    #froudelcint = LineCollection(segmentsint, cmap=plt.get_cmap('seismic'), norm=froudegradint)\n",
    "    #froudelcint.set_array(froudenumint)\n",
    "    #froudelcint.set_linewidth(2.5)\n",
    "    \n",
    "    #froudelineint = ax4.add_collection(froudelcint)\n",
    "\n",
    "    \n",
    "    #velogradint = plt.Normalize(velint.min(), velint.max()) ##gradient plot for flow velocity\n",
    "    \n",
    "    #velolcint = LineCollection(segmentsint, cmap = cmap_fac, norm = normbd)\n",
    "    #velolcint.set_array(velint)\n",
    "    #velolcint.set_linewidth(3)\n",
    "    \n",
    "    transport = plt.Normalize(transportstage.min(), transportstage.max()) ##gradient plot for flow velocity\n",
    "    \n",
    "    transportlc = LineCollection(segmentsint, cmap = cmap_fac, norm = norm_fac)\n",
    "    transportlc.set_array(transportstage)\n",
    "    transportlc.set_linewidth(2)\n",
    "    \n",
    "    #velolcint = LineCollection(segmentsint, cmap = plt.get_cmap(ss_facies_r), norm = velogradint)\n",
    "    #velolcint.set_array(velint)\n",
    "    #velolcint.set_linewidth(4)\n",
    "    \n",
    "    transportline = ax4.add_collection(transportlc)\n",
    "    #ax4.plot(erohiatalsurf[strata], 'k', lw = 3, alpha = 0.1)\n",
    "#for i in range (0, end_t):\n",
    "    #ax2.plot(posnew, stratinterpcub[i], 'k', lw = '1', alpha = 0.1)\n",
    "    #ax3.plot(posnew, stratinterpcub[i], 'k', lw = '1', alpha = 0.1)\n",
    "    #ax4.plot(posnew, stratinterpcub[i], 'k', lw = '1', alpha = 0.1)\n",
    "#plt.close(fig)\n",
    "ax1.set_facecolor('xkcd:grey')\n",
    "ax2.set_facecolor('xkcd:grey')\n",
    "ax3.set_facecolor('xkcd:grey')\n",
    "ax4.set_facecolor('xkcd:midnight blue')\n",
    "\n",
    "ax4.set_xlabel('Cross stream distance, m')\n",
    "\n",
    "ax1.set_ylabel('Bed Elevation, m')\n",
    "ax2.set_ylabel('Bed Elevation, m')\n",
    "ax3.set_ylabel('Bed Elevation, m')\n",
    "ax4.set_ylabel('Bed Elevation, m')\n",
    "\n",
    "#fig.colorbar(gradline, ax=ax1)    \n",
    "#fig.colorbar(gradlineint, label = \"Flow Velocity, m/s$^-1$\", ax=ax3)\n",
    "fig.colorbar(gradlineint, label = \"Shear Stress, N/m$^2$\", ax=ax3)\n",
    "#fig.colorbar(boundline, ax=ax2)\n",
    "fig.colorbar(depthlineint, ax=ax2, label = 'Flow Depth Fraction')\n",
    "fig.colorbar(transportline, ax=ax4, label = 'Mobility Parameter')\n",
    "#fig.colorbar(slopelineint, ax=ax4, label = 'Local Slope, ° degrees')\n",
    "ax1.title.set_text('Cross Stream Stratigraphy at '+str(xloc)+'m')\n",
    "ax3.title.set_text('Shear Stress, N/m$^2$')\n",
    "#ax3.title.set_text('Shields stress with bedform boundaries from Van Rijn (1990, 1993) bedform stability diagram')\n",
    "ax2.title.set_text('Local flow depth relative to bankfull flow depth')\n",
    "if d50 == 0.31e-3:\n",
    "    ax4.title.set_text('facies: No Mvt, Ripples, Dunes, Washed out dunes, UPB')\n",
    "elif d50 == 1e-3:\n",
    "    ax4.title.set_text('facies: No Mvt, LSPB, Dunes, Washed out dunes, UPB')\n",
    "#ax4.title.set_text('Local Slope')\n",
    "\n",
    "#plt.title(modelrun+' Triple Plot')\n",
    "plt.savefig(savefilesto+'tripleplot/'+modelrun+'.png', dpi = 100)\n",
    "\n",
    "#print(depth.min(), depth.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T16:41:11.061772Z",
     "start_time": "2021-03-23T16:41:10.873024Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-07T21:21:38.387Z"
    }
   },
   "outputs": [],
   "source": [
    "cmap_fac = ListedColormap(['xkcd:mud brown', 'xkcd:dirt', 'xkcd:sandy brown', 'xkcd:beige', 'xkcd:stone'])\n",
    "norm_fac = BoundaryNorm(Tarray, cmap_fac.N)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (16, 6), tight_layout = True)\n",
    "for t in range (0, end_t-1):\n",
    "    for xpos in posnew[:-1]:\n",
    "        fills = ax.fill_between([xpos, xpos+1], xy_topo[t, xpos], xy_topo[t+1, xpos+1], color = cmap_fac(Tparam[t+1, xpos]))#\n",
    "    ax.plot(erohiatalsurf[t], 'r', lw = 0.5, alpha = 0.5)\n",
    "ax.set_ylabel('Bed Elevation, m')\n",
    "ax.set_xlabel('Cross Stream Distance, m')\n",
    "if d50 == 0.31e-3:\n",
    "    ax.title.set_text('facies: No Mvt, Ripples, Dunes, Washed out dunes, UPB')\n",
    "elif d50 == 1e-3:\n",
    "    ax.title.set_text('facies: No Mvt, LSPB, Dunes, Washed out dunes, UPB')\n",
    "ax.set_facecolor('xkcd:midnight blue')\n",
    "fig.colorbar(cm.ScalarMappable(norm=norm_fac, cmap=cmap_fac), ax = ax, label = 'Mobility Parameter')\n",
    "plt.savefig(savefilesto+'tripleplot/facies_'+modelrun+'.png', dpi = 100)\n",
    "\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T13:12:55.326777Z",
     "start_time": "2021-03-23T13:12:53.877880Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "elapsed": 26722,
     "status": "ok",
     "timestamp": 1616014284439,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "8aJmpNdm4uVV",
    "outputId": "26811248-ca5c-43f7-e58f-ff1cad876f8e"
   },
   "source": [
    "fig, ax = plt.subplots(1, figsize = (16, 6), tight_layout = True)\n",
    "\n",
    "for i in range (0, end_t):\n",
    "    #print(i)\n",
    "    elevint = xy_topo[i]\n",
    "    pointsint = np.array([posnew, elevint]).T.reshape(-1, 1, 2)\n",
    "    segmentsint = np.concatenate([pointsint[:-1], pointsint[1:]], axis=1)\n",
    "    transportstage = Tparam[i]\n",
    " \n",
    "    transportlc = LineCollection(segmentsint, cmap = cmap_fac, norm = norm_fac)\n",
    "    transportlc.set_array(transportstage)\n",
    "    transportlc.set_linewidth(3)\n",
    "    \n",
    "    transportline = ax.add_collection(transportlc)\n",
    "    \n",
    "ax.set_xlim((posnew.min(), posnew.max())) #braided test\n",
    "ax.set_ylim((xy_topo.min(), xy_topo.max())) #braided test\n",
    "   \n",
    "ax.set_facecolor('xkcd:midnight blue')\n",
    "ax.set_xlabel('Cross stream distance, m')\n",
    "ax.set_ylabel('Bed Elevation, m')\n",
    "\n",
    "fig.colorbar(transportline, ax=ax, label = 'Mobility Parameter')\n",
    "if d50 == 0.31e-3:\n",
    "    ax.title.set_text('facies: No Mvt, Ripples, Dunes, Washed out dunes, UPB')\n",
    "elif d50 == 1e-3:\n",
    "    ax.title.set_text('facies: No Mvt, LSPB, Dunes, Washed out dunes, UPB')\n",
    "plt.savefig(savefilesto+'tripleplot/fac_'+modelrun+'.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1ZfMljA4uVV"
   },
   "source": [
    "## Heat Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T01:57:23.965Z"
    },
    "executionInfo": {
     "elapsed": 25306,
     "status": "ok",
     "timestamp": 1616014284440,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "A-dKb1sL4uVV"
   },
   "outputs": [],
   "source": [
    "## create difference matrix\n",
    "#SICslice = np.delete(xy_topo, 0, axis = 0) \n",
    "#TOPOslice = np.delete(strat, -1, axis = 0) \n",
    "#difference = SICslice-TOPOslice #differencing method rather than searching +ve diff = erosion, -ve =depo hia_range = np.percentile(difference, 1)\n",
    "difference = thickness\n",
    "diff_fraction = difference/(difference.max())\n",
    "#print(difference.shape)\n",
    "diff_llim= difference.min()\n",
    "diff_ulim = difference.max()\n",
    "#print(var_llim, var_ulim)\n",
    "\n",
    "\n",
    "vel_llim = flowvel.min()\n",
    "vel_ulim = flowvel.max()\n",
    "\n",
    "vels_frac = flowvel/vel_ulim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:02.735343Z",
     "start_time": "2021-03-23T17:37:02.702520Z"
    },
    "executionInfo": {
     "elapsed": 24665,
     "status": "ok",
     "timestamp": 1616014284440,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "jb8dmwZ54uVV"
   },
   "outputs": [],
   "source": [
    "class MidpointNormalize(mcol.Normalize):\n",
    "    def __init__(self, vmin=None, vmax=None, vcenter=None, clip=False):\n",
    "        self.vcenter = vcenter\n",
    "        mcol.Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        # I'm ignoring masked values and all kinds of edge cases to make a\n",
    "        # simple example...\n",
    "        x, y = [self.vmin, self.vcenter, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "#normalise = plt.Normalize(-1, 2)\n",
    "midnorm = MidpointNormalize(vmin=diff_llim, vcenter=0, vmax=diff_ulim)\n",
    "midnorm11 = MidpointNormalize(vmin=-1, vcenter=0, vmax=1)\n",
    "normalise = plt.Normalize(0, 1)\n",
    "#midnorm = MidpointNormalize(vmin=-1, vcenter=0, vmax=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:05.359097Z",
     "start_time": "2021-03-23T17:37:02.741030Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 24476,
     "status": "ok",
     "timestamp": 1616014284928,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "JDGSYnkD4uVW",
    "outputId": "7cdda63a-5db4-45ad-9a6a-79f320940d91"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize = (12, 8), tight_layout = True, sharex = True, sharey = True)\n",
    "\n",
    "\n",
    "absol = ax[0].pcolormesh(difference, cmap = 'seismic_r', norm = midnorm, shading = 'flat', snap = True)\n",
    "frac = ax[1].pcolormesh(diff_fraction, cmap = 'seismic_r', norm = midnorm11, shading = 'flat', snap = True)\n",
    "\n",
    "#absolv = ax[1,0].pcolormesh(velocub, cmap = 'YlGnBu', norm = velogradint, shading = 'flat', snap = True)\n",
    "#fracv = ax[1,1].pcolormesh(vels_frac, cmap = 'YlGnBu', norm = normalise, shading = 'flat', snap = True)\n",
    "\n",
    "#fig.colorbar()\n",
    "ttl = fig.suptitle('Wheeler Heat Maps for Ero/Dep/Hiat and Flow Velocity (m/s)')\n",
    "ttl.set_position([0.5, 1.01])\n",
    "#fig.tight_layout()\n",
    "#fig.subplots_adjust(top=10)\n",
    "\n",
    "fig.colorbar(absol, ax = ax[0], label = 'Subsequent Elevation Change')\n",
    "fig.colorbar(frac, ax = ax[1])\n",
    "#fig.colorbar(absolv, ax = ax[1,0], label = \"Flow Velocity, m/s\")\n",
    "#fig.colorbar(fracv, ax = ax[1,1])\n",
    "\n",
    "ax[0].set_ylabel('Timestep', fontsize = 12)\n",
    "ax[0].set_xlabel('Distance in Matrix, not in m', fontsize = 12)\n",
    "ax[1].set_xlabel('Distance in Matrix, not in m', fontsize = 12)\n",
    "print(diff_fraction.min(), diff_fraction.max())\n",
    "plt.savefig(savefilesto+'ero_dep_wheeler/'+modelrun+'.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HybvyFmA4uVW"
   },
   "source": [
    "Find the left and right edges of depositional packages, append depositional matrix to get co-ordintes of rectangles for wheeler diagram.\n",
    "\n",
    "In order for the logic statement to work to find the polygons, I created a border of nans around all ofthe arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:06.027847Z",
     "start_time": "2021-03-23T17:37:05.363383Z"
    },
    "executionInfo": {
     "elapsed": 22355,
     "status": "ok",
     "timestamp": 1616014284929,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "9qpeKsUe4uVW"
   },
   "outputs": [],
   "source": [
    "nan_border = np.empty([end_t, 1])\n",
    "nan_border.shape\n",
    "nan_border[:] = np.nan\n",
    "#topointerp_nb = np.concatenate((nan_border, strat, nan_border), axis = 1) #nb = nan border\n",
    "stratinterpcub_nb = np.concatenate((nan_border, xy_topo, nan_border), axis = 1)\n",
    "erosurf_nb = np.concatenate((nan_border, erosurf, nan_border), axis = 1)\n",
    "deposurf_nb = np.concatenate((nan_border, deposurf, nan_border), axis = 1)\n",
    "\n",
    "hiatalsurf_nb = np.concatenate((nan_border, hiatalsurf, nan_border), axis = 1)\n",
    "erohiatalsurf_nb = np.concatenate((nan_border, erohiatalsurf, nan_border), axis = 1)\n",
    "posnew = np.arange(0, xy_topo.shape[1], dtype = float)\n",
    "posnew_nb = np.insert(posnew, [0], [np.nan])\n",
    "posnew_nb = np.insert(posnew_nb, -1, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:06.125975Z",
     "start_time": "2021-03-23T17:37:06.030761Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21454,
     "status": "ok",
     "timestamp": 1616014284930,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "RZAbghNT4uVW",
    "outputId": "2ebfd749-1dc0-47b4-c1bd-8e3252476f33",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#was trying tosee if you can plot the topo at the same time of the erosion surffaces only, but the that ends up plotting all the topo\n",
    "time_erohiat = np.empty_like(ages_ero) #will store only the ages of the erosional and hiatal surfaces\n",
    "time_erohiat[:] = np.nan\n",
    "\n",
    "EHindex = np.where(stratcondition != 0) #find and remove locations of deposition\n",
    "\n",
    "time_erohiat[EHindex] = ages_ero[EHindex] #add ages to locations of erosion and hiatuses only\n",
    "\n",
    "boundary_time_notnan = time_erohiat[~np.isnan(time_erohiat)]\n",
    "\n",
    "boundary_time = np.unique(boundary_time_notnan)\n",
    "\n",
    "print(boundary_time.shape)\n",
    "boundary_time = np.sort(boundary_time)\n",
    "\n",
    "#for time in boundary_time:\n",
    "#    plt.plot(posnew, stratinterpcub[int(time)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:06.146232Z",
     "start_time": "2021-03-23T17:37:06.128612Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20647,
     "status": "ok",
     "timestamp": 1616014284930,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "mvNhdKbP4uVW",
    "outputId": "a38b4552-f33a-458b-bfe4-e67d133ded23"
   },
   "outputs": [],
   "source": [
    "print(runtime, end_t)\n",
    "depcontrol = deposurf_nb.copy()\n",
    "depcontrol[~np.isnan(depcontrol)] = 1\n",
    "erocontrol = erosurf_nb.copy()\n",
    "erocontrol[~np.isnan(erocontrol)] = 1\n",
    "#erocontrol[~np.isnan()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6T9o0MNF4uVW"
   },
   "source": [
    "## Wheeler Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:16.970198Z",
     "start_time": "2021-03-23T17:37:06.148203Z"
    },
    "executionInfo": {
     "elapsed": 19927,
     "status": "ok",
     "timestamp": 1616014285876,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "UpFIbSho4uVW"
   },
   "outputs": [],
   "source": [
    "#print(len(posnew_nb))\n",
    "l_edges = np.empty([1,]) #left edge distance measure of the wheeler fill\n",
    "l_index = np.empty([1,]) #index value of left edge\n",
    "r_edges = np.empty([1,]) #right edge of the wheeler fill\n",
    "r_index = np.empty([1,]) #index value of right edge\n",
    "dep_age = np.empty([1,]) #age of each deposit for the wheeler diagram\n",
    "for i in range (0,end_t):\n",
    "    #if ~np.isnan(deposurf[i, 0]):\n",
    "    #   rectangles[i, 0] = ages[i, 0] #if the left edge of the stratigraphy is a depositional surface\n",
    "\n",
    "    for xpos in range (0, len(posnew_nb)-1):\n",
    "        l_edge = np.all((np.isnan(deposurf_nb[i, xpos]) and ~np.isnan(deposurf_nb[i, xpos+1])))\n",
    "        r_edge = np.all((~np.isnan(deposurf_nb[i, xpos]) and np.isnan(deposurf_nb[i, xpos+1])))\n",
    "        \n",
    "        if l_edge == True:\n",
    "            l_edges = np.append(l_edges, [posnew_nb[xpos+1]], axis = 0)\n",
    "            l_index = np.append(l_index, [xpos+1], axis = 0)\n",
    "            #print(posnew_nb[xpos+1], 'potato')\n",
    "        if r_edge == True:\n",
    "            #print(xpos, 'tomato')\n",
    "            r_edges = np.append(r_edges, [posnew_nb[xpos-1]], axis = 0)\n",
    "            r_index = np.append(r_index, [xpos+1], axis = 0)\n",
    "            dep_age = np.append(dep_age, [i], axis = 0)\n",
    "        \n",
    "#print('L', l_edges)\n",
    "#print('R',r_edges)\n",
    "#print(dep_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.204809Z",
     "start_time": "2021-03-23T17:37:16.971837Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "executionInfo": {
     "elapsed": 24147,
     "status": "ok",
     "timestamp": 1616014290880,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "a5CfrxO-4uVX",
    "outputId": "80b0a111-1ced-4fc3-8499-a897bce8d525"
   },
   "outputs": [],
   "source": [
    "##Now we concatenate the L and R and age arrays to form a database to plot the rectangles\n",
    "l_edges = np.reshape(l_edges, [len(l_edges), 1])\n",
    "r_edges = np.reshape(r_edges, [len(r_edges), 1])\n",
    "dep_age = np.reshape(dep_age, [len(dep_age), 1])\n",
    "\n",
    "rectangles = np.empty([len(l_edges), 5])\n",
    "rectangles[:, 0] = dep_age[:, 0]\n",
    "rectangles[:, 1] = l_edges[:, 0]\n",
    "rectangles[:, 2] = r_edges[:, 0]\n",
    "rectangles[:, 3] = rectangles[:, 2]-rectangles[:, 1]\n",
    "rectangles[:, 4] = 1\n",
    "#print(rectangles)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15,5), gridspec_kw={'width_ratios': [1, 2]}, tight_layout =True)\n",
    "for i in range (1, len(rectangles)):\n",
    "    #rect = plt.Rectangle((rectangles[i,1], rectangles[i,0]), rectangles[i,3], rectangles[i,4], color=cpick.to_rgba(rectangles[i, 0]), ec = 'k', linewidth = 0.5)\n",
    "    ax1.add_patch(plt.Rectangle((rectangles[i,1], rectangles[i,0]), rectangles[i,3], rectangles[i,4], color=cpick.to_rgba(rectangles[i, 0]), ec = 'k', linewidth = 0.5))\n",
    "    \n",
    "for i in range(0, end_t):\n",
    "    ax1.plot(posnew_nb, i*erocontrol[i, :], 'r--', linewidth = 0.5)\n",
    "    ax1.plot(posnew, i*(hiatalsurf[i, :]/hiatalsurf[i, :]), 'g--', linewidth = 0.5)\n",
    "    #ax1.plot(posnew_nb, i*depcontrol[i, :], 'b.', linewidth = 0.5)\n",
    "\n",
    "for i in range (0, end_t-1):\n",
    "    ax2.fill_between(posnew, xy_topo[i+1], xy_topo[i], color=cpick.to_rgba(i), alpha = .3)\n",
    "    #polys = PolyCollection.get_clip_path(self)\n",
    "\n",
    "#Plot topo in black to find main surfaces    \n",
    "for i in range (0, end_t):\n",
    "    #ax1.plot(posnew, stratinterpcub[i], 'k', lw = '1', alpha = 0.1)\n",
    "    ax2.plot(posnew, erosurf[i], 'r', lw='1.5', alpha = 1)\n",
    "    ax2.plot(posnew, hiatalsurf[i], 'c', lw='1.5', alpha = 1)\n",
    "    ax2.plot(posnew, deposurf[i], 'w', lw='0.5', alpha = 0.7)\n",
    "\n",
    "ax2.plot(posnew, erosurf[end_t-1], 'r', lw='1.5', alpha = 1)\n",
    "ax2.plot(posnew, deposurf[end_t-1], 'b', lw='0.5', alpha = 0.7)\n",
    "ax2.plot(posnew, hiatalsurf[end_t-1], 'y', lw='1.5', alpha = 1)\n",
    "ax2.set_facecolor('k')\n",
    "plt.xlim(0, posnew.max())\n",
    "#plt.ylim(0, num_timesteps)\n",
    "fig.colorbar(cpick,label=\"Time\", ax=ax2)\n",
    "plt.savefig(savefilesto+'wheeler/'+modelrun+'.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.213378Z",
     "start_time": "2021-03-23T17:37:02.500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1616017566818,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "EcCMgmAD4uVX",
    "outputId": "53354ae9-86cb-4a24-b97d-c7fff41f23df"
   },
   "outputs": [],
   "source": [
    "widths = xposition.max(axis = 1)\n",
    "times = np.arange(0, end_t)\n",
    "rate = np.empty([end_t-1])\n",
    "\n",
    "floodzone = mpl.patches.Rectangle((fldstart, widths.min()), fldlength, widths.max()-widths.min(), color = 'b', alpha = 0.1, label = 'Flooding')\n",
    "\n",
    "\n",
    "for i in range(0, end_t-1):\n",
    "    rate[i] = (widths[i]-widths[i+1])/(times[i]-times[i+1])\n",
    "    #print(rate[i])\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "ax.plot(times, widths, c = 'r', ls = '--', marker='.', mfc = 'r', mec = 'k', mew = 0, label = 'channel widening')\n",
    "ax.set_ylabel('Channel Width, m')\n",
    "ax.set_xlabel('Timestep')\n",
    "ax.add_patch(floodzone)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(times, thickness.max(axis = 1)/times, c = 'c', ls = '--', marker='.', mfc = 'c', mec = 'k', mew = 0.3, label = 'Max thickness')\n",
    "ax2.plot(times, thickness.min(axis = 1)/times, c = 'k', ls = '--', marker='.', mfc = 'k', mec = 'k', mew = 0, label = 'Min thickness')\n",
    "ax2.plot(times, np.mean(thickness, axis = 1)/times, c = 'g', ls = '--', marker='.', mfc = 'g', mec = 'k', mew = 0, label = 'Mean thickness')\n",
    "ax2.set_ylabel('Bed thickness, m')\n",
    "plt.title('The question is: Will these trends hold for variable discharge')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.savefig(savefilesto+'widths/'+modelrun+'.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T23:50:09.695057Z",
     "start_time": "2021-03-21T23:50:09.556814Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.215279Z",
     "start_time": "2021-03-23T17:37:02.508Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1616017593718,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "l1yP7VPDJ2x1",
    "outputId": "a68e4a4e-9a66-4c87-985f-30c471326b47"
   },
   "outputs": [],
   "source": [
    "plt.plot(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.217456Z",
     "start_time": "2021-03-23T17:37:02.513Z"
    },
    "executionInfo": {
     "elapsed": 22494,
     "status": "ok",
     "timestamp": 1616014291102,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "WB5AiJLw4uVX"
   },
   "outputs": [],
   "source": [
    "SIC_startcrop = xy_topo.copy()\n",
    "SIC_startcrop = np.concatenate((nan_border, SIC_startcrop, nan_border), axis = 1)\n",
    "\n",
    "SIC_startcrop = np.delete(SIC_startcrop, 0, axis = 0)\n",
    "SIC_startcrop.shape\n",
    "\n",
    "SIC_endcrop = xy_topo.copy()\n",
    "SIC_endcrop = np.concatenate((nan_border, SIC_endcrop, nan_border), axis = 1)\n",
    "\n",
    "SIC_endcrop = np.delete(SIC_endcrop, -1, axis=0)\n",
    "SIC_endcrop.shape\n",
    "\n",
    "delta = SIC_startcrop-SIC_endcrop\n",
    "\n",
    "delta[np.where(delta[:]==0)] = np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOj_KouR4uVX"
   },
   "source": [
    "## Create the left and right vertices of each polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.219023Z",
     "start_time": "2021-03-23T17:37:02.519Z"
    },
    "executionInfo": {
     "elapsed": 21924,
     "status": "ok",
     "timestamp": 1616014292041,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "LMZPq_uE4uVX"
   },
   "outputs": [],
   "source": [
    "\n",
    "#print(len(posnew_nb))\n",
    "#l_edges = np.empty([1,]) #left edge distance measure of the wheeler fill\n",
    "l_idx = np.empty([1,]) #index value of left edge\n",
    "#r_edges = np.empty([1,]) #right edge of the wheeler fill\n",
    "r_idx = np.empty([1,]) #index value of right edge\n",
    "surf_age = np.empty([1,]) #age of each deposit for the wheeler diagram\n",
    "for i in range (0, len(delta)):\n",
    "    #if ~np.isnan(deposurf[i, 0]):\n",
    "    #   rectangles[i, 0] = ages[i, 0] #if the left edge of the stratigraphy is a depositional surface\n",
    "\n",
    "    for xpos in range (0, len(posnew_nb)-1):\n",
    "        l_edge = np.all((np.isnan(delta[i, xpos]) and ~np.isnan(delta[i, xpos+1])))\n",
    "        r_edge = np.all((~np.isnan(delta[i, xpos]) and np.isnan(delta[i, xpos+1])))\n",
    "        #print(xpos, 'L', l_edge)\n",
    "        #print(xpos, 'R', r_edge)\n",
    "        if l_edge == True:\n",
    "            #l_edges = np.append(l_edges, [posnew_nb[xpos+1]], axis = 0)\n",
    "            l_idx = np.append(l_idx, [xpos], axis = 0)\n",
    "            #print(posnew_nb[xpos+1], 'potato')\n",
    "            surf_age = np.append(surf_age, [i], axis = 0)\n",
    "        if r_edge == True:\n",
    "            #print(xpos, 'tomato')\n",
    "            #r_edges = np.append(r_edges, [posnew_nb[xpos-1]], axis = 0)\n",
    "            r_idx = np.append(r_idx, [xpos], axis = 0)\n",
    "            \n",
    "#surf_age = np.append(surf_age, [i+1], axis = 0)\n",
    "#print('L', l_edges)\n",
    "#print('R',r_edges)\n",
    "#print(dep_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIsIGe2A4uVX"
   },
   "source": [
    "Store all those vertices in an array that houses the time of each polygon and the left and right edges of the poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.220737Z",
     "start_time": "2021-03-23T17:37:02.526Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20298,
     "status": "ok",
     "timestamp": 1616014292043,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "nREpL9ZZ4uVX",
    "outputId": "6c1202a1-646b-4246-d2c3-d3c2fb7d06c2"
   },
   "outputs": [],
   "source": [
    "l_idx = np.reshape(l_idx, [len(l_idx), 1])\n",
    "#l_index = np.delete(l_index, 1)\n",
    "\n",
    "l_idx = l_idx.astype(int)\n",
    "r_idx = np.reshape(r_idx, [len(r_idx), 1])\n",
    "#r_index = np.delete(r_index, 1)\n",
    "r_idx = r_idx.astype(int)\n",
    "\n",
    "#print(l_idx[:, 0], r_idx[:, 0])\n",
    "surf_age = np.reshape(surf_age, [len(surf_age), 1])\n",
    "\n",
    "print(l_idx.shape, r_idx.shape, surf_age.shape)\n",
    "\n",
    "vertices_b = surf_age\n",
    "vertices_b = np.append(vertices_b, l_idx, axis = 1)\n",
    "vertices_b = np.append(vertices_b, r_idx, axis = 1)\n",
    "\n",
    "#print(vertices)#, vertices.shape)\n",
    "\n",
    "\n",
    "cent_array = np.empty([len(vertices_b), 8])\n",
    "cent_array[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.223123Z",
     "start_time": "2021-03-23T17:37:02.530Z"
    },
    "executionInfo": {
     "elapsed": 19930,
     "status": "ok",
     "timestamp": 1616014292896,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "2uLNHbs64uVY"
   },
   "outputs": [],
   "source": [
    "## Plot the filled stratigraphy, create a polygon for each, find the centroid, store the centroid and its age\n",
    "fig, ax1 = plt.subplots(1, 1, tight_layout=True, squeeze=True, figsize = (10,6))\n",
    "poly_data = {}\n",
    "for i in range (1, len(vertices_b)):\n",
    "    time = int(vertices_b[i, 0])\n",
    "    left = int(vertices_b[i, 1])\n",
    "    right = int(vertices_b[i, 2])\n",
    "\n",
    "    poly = ax1.fill_between(posnew[left:right], xy_topo[time, left:right], xy_topo[time+1, left:right])\n",
    "    pverts = poly.get_paths()[0].vertices\n",
    "    \n",
    "    polygon = Polygon(pverts) #create a shapely polygon\n",
    "    #print(polygon)\n",
    "    poly_data[i] = polygon\n",
    "    area = polygon.area\n",
    "    bounds = polygon.bounds\n",
    "    #print(type(bounds[0]))\n",
    "    cent_array[i, 4] = bounds[0]\n",
    "    cent_array[i, 5] = bounds[1]\n",
    "    cent_array[i, 6] = bounds[2]\n",
    "    cent_array[i, 7] = bounds[3]\n",
    "    cent_array[i, 3] = area\n",
    "    ctroid = polygon.centroid\n",
    "    cent_array[i, 0] = time\n",
    "    cent_array[i, 1] = ctroid.x\n",
    "    cent_array[i, 2] = ctroid.y\n",
    "\n",
    "    \n",
    "    ax1.plot(cent_array[i, 1], cent_array[i, 2], 'k*')\n",
    "#ax1.set_xlim((200, 300))\n",
    "ax1.set_xlim(posnew.min(), posnew.max())\n",
    "ax1.set_ylim(xy_topo.min(), xy_topo.max())\n",
    "#print('12', poly_data[12])\n",
    "plt.close(fig)\n",
    "#for i in range (0, num_timesteps-1):\n",
    "#   plt.plot(posnew, stratinterpcub[i, :], 'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoTq1XdN4uVY"
   },
   "source": [
    "## Coring the stratigraphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.225552Z",
     "start_time": "2021-03-23T17:37:02.537Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18663,
     "status": "ok",
     "timestamp": 1616014292897,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "yJqe4Ocb4uVY",
    "outputId": "f4512a93-48e6-4330-e125-46a4cc7fe179"
   },
   "outputs": [],
   "source": [
    "print(len(posnew), posnew.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.226964Z",
     "start_time": "2021-03-23T17:37:02.542Z"
    },
    "executionInfo": {
     "elapsed": 17831,
     "status": "ok",
     "timestamp": 1616014292898,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "rL5UJrYS4uVY"
   },
   "outputs": [],
   "source": [
    "coresthick = {} #dictionary to hold set thickness\n",
    "coresbounds = {} #dictionary to hold bound surfaces\n",
    "num_sets = np.empty([len(posnew), 2])\n",
    "num_sets[:, 0] = posnew\n",
    "threshold = 0.1\n",
    "for i in range(0, len(posnew)):\n",
    "    keystr = 'x'+str(i)\n",
    "    \n",
    "    elevatcore = np.delete(erohiatalsurf[:, i], np.where(np.isnan(erohiatalsurf[:, i])))\n",
    "    bounds = np.unique(elevatcore)\n",
    "    #print(i, bounds)\n",
    "    \n",
    "    coresbounds[keystr] = bounds #finds the bounding elevations of sets\n",
    "    \n",
    "    diffs = np.empty([0])\n",
    "    \n",
    "    for j, k in zip(bounds[:-1], bounds[1:]):\n",
    "        #print(i, j, k)\n",
    "        diff = k-j #more positive -- less positive = positive\n",
    "        #print(diff)\n",
    "        if diff > threshold:\n",
    "            diffs = np.append(diffs, diff)\n",
    "\n",
    "\n",
    "    coresthick[keystr] = diffs\n",
    "    num_sets[i, 1] = len(diffs)\n",
    "    \n",
    "    #print(cores[keystr+'_thick'])\n",
    "#print(cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.228176Z",
     "start_time": "2021-03-23T17:37:02.551Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "executionInfo": {
     "elapsed": 18843,
     "status": "ok",
     "timestamp": 1616014294611,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "4Z7aKckM4uVY",
    "outputId": "95f1340c-a1cd-4f9b-fa2d-91b83aa7807f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (14, 4), tight_layout=True)\n",
    "ax[0].plot(num_sets[:, 0], num_sets[:, 1], 'k')\n",
    "for i in range (0, end_t):\n",
    "    ax[1].plot(posnew, xy_topo[i], 'k', lw = 0.1, alpha = 0.4)\n",
    "    ax[1].plot(posnew, erosurf[i], 'r', lw = 1.5)\n",
    "    ax[1].plot(posnew, hiatalsurf[i], 'g', lw = 1.5)\n",
    "ax[1].set_ylabel('Bed Elevation, m')\n",
    "ax[0].set_ylabel('Number of Sets')\n",
    "ax[1].set_xlabel(\"Cross Stream Position\")\n",
    "ax[0].set_xlabel(\"Cross Stream Position\")\n",
    "ax[1].legend()\n",
    "\n",
    "ax[0].set_title('Time series of number of sets at iloc = '+str(iloc))\n",
    "ax[1].set_title('Stratigraphy at '+str(iloc)+'. Ero = red, Hiatal = green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.229704Z",
     "start_time": "2021-03-23T17:37:02.558Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "executionInfo": {
     "elapsed": 19815,
     "status": "ok",
     "timestamp": 1616014296723,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "_tlZGz2w4uVY",
    "outputId": "3ffd1792-08ce-476a-af43-23eee309934b"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize = (14, 4), tight_layout = True)\n",
    "ax[0].plot(num_sets[:, 0], num_sets[:, 1], 'k')\n",
    "ax[0].set_title('Number of sets along the section')\n",
    "\n",
    "for loc, core in zip(posnew, coresthick):\n",
    "    avg = np.average(coresthick[core])\n",
    "    #print(coresthick[core], avg)\n",
    "    if len(coresthick[core]) >0:\n",
    "        ax[1].plot(loc, np.average(coresthick[core]), 'o', markerfacecolor = 'b', markeredgecolor = 'k')\n",
    "        ax[2].plot(loc, coresthick[core].max(), 'o', markerfacecolor = 'g', markeredgecolor = 'k')\n",
    "        \n",
    "ax[2].set_ylim(0, 2)\n",
    "ax[1].set_ylim(0, 2)\n",
    "\n",
    "ax[2].set_title('Maximum set thickness at each cor loc')\n",
    "ax[1].set_title('Average set thickness at each cor loc')\n",
    "\n",
    "fig.suptitle(str(iloc))\n",
    "\n",
    "#plt.savefig(savefilesto+'/corestats/'+modelrun+'.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVniuKXY4uVY"
   },
   "source": [
    "Trying to recreate plots from Nicholas et al., 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.231686Z",
     "start_time": "2021-03-23T17:37:02.567Z"
    },
    "executionInfo": {
     "elapsed": 17197,
     "status": "ok",
     "timestamp": 1616014296723,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "DHA_R7KL4uVZ"
   },
   "outputs": [],
   "source": [
    "#Image(filename='/Users/safiya/Desktop/Nicholas2016_fig2.png', width = 400, height = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.233027Z",
     "start_time": "2021-03-23T17:37:02.574Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16351,
     "status": "ok",
     "timestamp": 1616014296724,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "3P044mSr4uVZ",
    "outputId": "2824cd2d-8344-4fb5-cd4c-a3daee94f797"
   },
   "outputs": [],
   "source": [
    "print(posnew.min(), posnew.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.234727Z",
     "start_time": "2021-03-23T17:37:02.581Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 17451,
     "status": "ok",
     "timestamp": 1616014298569,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "RsnhRwN94uVZ",
    "outputId": "ab7c97da-542f-4856-d052-cdb51a0c364f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize = (16, 4))\n",
    "histo = ax[0].hist(thickness.ravel(), bins = 25, density = True, ec = 'k', fc = 'xkcd:greyish')\n",
    "for loc, core in zip(posnew, coresthick):\n",
    "    avg = np.average(coresthick[core])\n",
    "    #print(coresthick[core], avg)\n",
    "    loc = int(loc)\n",
    "    if len(coresthick[core]) >0:\n",
    "        \n",
    "        ax[1].plot(np.average(trueflow[:, loc]), np.average(coresthick[core]), 'o', markerfacecolor = 'b', markeredgecolor = 'k')\n",
    "        ax[2].plot(trueflow[:, loc].max(), coresthick[core].max(), 'o', markerfacecolor = 'g', markeredgecolor = 'k') \n",
    "        ax[-1].plot(np.std(thickness[:, loc]), np.average(coresthick[core]), '+', markeredgecolor = 'k')\n",
    "        \n",
    "        #ax[1].plot(np.average(actual_flowdepthcub[loc, :]), np.average(coresthick[core]), 'o', markerfacecolor = 'b', markeredgecolor = 'k')\n",
    "        #ax[2].plot(actual_flowdepthcub[loc, :].max(), coresthick[core].max(), 'o', markerfacecolor = 'g', markeredgecolor = 'k') \n",
    "        #ax[-1].plot(np.std(thickness[loc, :]), np.average(coresthick[core]), '+', markeredgecolor = 'k')\n",
    "ax[1].set_ylabel('Mean set thickness')\n",
    "ax[2].set_ylabel('Max set thickness')\n",
    "\n",
    "ax[1].set_xlabel('Mean Flow Depth')\n",
    "ax[2].set_xlabel('Max Flow Depth')\n",
    "\n",
    "ax[1].set_ylabel('Mean set/depth across stream')\n",
    "ax[2].set_ylabel('Max set/depth across stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zue7dBlL4uVZ"
   },
   "source": [
    "Centroids are being plotted in very tiny polygons so this is a bit of code to delete the centroids of the polygons with the 10th percentile smallest area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.236640Z",
     "start_time": "2021-03-23T17:37:02.588Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15716,
     "status": "ok",
     "timestamp": 1616014298569,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "FKQhg2Wd4uVZ",
    "outputId": "7f74ebef-87e1-4637-9a99-36e5e0870cc9"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 4))\n",
    "#too_small_area = (cent_array[np.where(abs(cent_array[:, 3])<0.000001)])\n",
    "#print(cent_array[:, 3])\n",
    "ax1.hist(cent_array[:, 3])\n",
    "ptile10 = np.nanpercentile(cent_array[:, 3], 10)\n",
    "\n",
    "print(ptile10)\n",
    "ax1.axvline(ptile10, c = 'r')\n",
    "\n",
    "cent_wnan = cent_array.copy()\n",
    "deletewhere = np.where(cent_array[:, 3] < ptile10)\n",
    "cent_wnan[deletewhere] = np.nan\n",
    "\n",
    "cent_nonan = cent_wnan[~np.isnan(cent_wnan).any(axis = 1)] #delete all rows in cent_wnan matrix with nan values\n",
    "\n",
    "ax2.hist(cent_nonan[:, 0])\n",
    "ax1.set_title('Distribution of polygon areas woth 10thp', fontsize = 12)\n",
    "ax2.set_title('distribution of ages of polys with area > 10thp', fontsize = 12)\n",
    "textstr1 = str(len(cent_array))\n",
    "textstr2 = len(cent_nonan)\n",
    "ax1.set_xlabel('polygon area')\n",
    "ax2.set_xlabel('deposit age (10s of min)')\n",
    "ax1.text(2, 200, 'n='+str(len(cent_array)))\n",
    "ax2.text(25, 40, 'n='+str(len(cent_nonan)))\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.238264Z",
     "start_time": "2021-03-23T17:37:02.596Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "executionInfo": {
     "elapsed": 19573,
     "status": "ok",
     "timestamp": 1616014303070,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "mYfnfebP4uVZ",
    "outputId": "c88b77d9-a562-4836-db34-e07dc89c827c"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, tight_layout = True, figsize = (15, 5), dpi = 200)\n",
    "for i in range (0, end_t-1):\n",
    "    #ax.plot(posnew, stratinterpcub[i], cpick.to_rgba(i), lw = 0.5, alpha = 0.4)\n",
    "    ax.fill_between(posnew, xy_topo[i+1], xy_topo[i], color=cpick.to_rgba(i), alpha= 0.5, linewidth = 0.0)\n",
    "    #ax.plot(posnew, erohiatalsurf[i], 'r', lw = 1, alpha = 0.5)\n",
    "\n",
    "    #ax.plot(cent_array[i, 1], cent_array[i, 2], marker = '*', markerfacecolor = 'k', markeredgecolor = 'k', mew = .5,   ms = 4, alpha = 1)\n",
    "for i in range (0, end_t):#,5):\n",
    "    #ax.plot(posnew, stratinterpcub[i], color = 'xkcd:midnight blue',  ls = '--', lw = .8, alpha = .3)\n",
    "    #ax.plot(cent_wnan[i, 1], cent_array[i, 2], marker = '.', markerfacecolor = cpick.to_rgba(cent_array[i, 0]), markeredgecolor = 'k', mew = .1,   ms = 5, alpha = 1)\n",
    "    ax.plot(posnew, erohiatalsurf[i], 'k', lw = 1, alpha = .2)\n",
    "for i in range (1, len(vertices_b)):\n",
    "    ax.plot(cent_wnan[i, 1], cent_wnan[i, 2], marker = 'o', markerfacecolor = 'k', markeredgecolor = 'k', mew = .5,   ms = 5, alpha = 1)\n",
    "for i in range (1, len(cent_nonan)):\n",
    "    ax.plot(cent_nonan[i, 1], cent_nonan[i, 2], marker = 'o', color = cpick.to_rgba(cent_nonan[i, 0]), markeredgecolor = 'k', mew = .5,   ms = 5, alpha = 1)\n",
    "print(erohiatalsurf.shape)\n",
    "print(posnew.shape)\n",
    "ax.set_facecolor('xkcd:grey')\n",
    "fig.colorbar(cpick,label=\"Time, min\", ax=ax)\n",
    "#ax.set_xlim((200, 300))\n",
    "ax.set_xlim(posnew.min(), posnew.max())\n",
    "ax.set_ylim(xy_topo.min(), xy_topo.max())\n",
    "\n",
    "ax.set_xlabel('Cross Stream Distance, m')\n",
    "ax.set_ylabel('Bed Elevation, m')\n",
    "#cpick.to_rgba(cent_wnan[i, 0]), marker color by time\n",
    "#ax.set_ylim(stratinterpcub.min(), stratinterpcub.max())\n",
    "\n",
    "plt.savefig(savefilesto+'herofig/'+modelrun+'.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyP95Se54uVa"
   },
   "source": [
    "age, cent x (1), cent y (2), cent area (3), pol minx (4), pol miny (5), pol maxx (6), pol maxy (7), pol width (8), pol heigt (9), distance (10), reltime (11), delx (12), dely (13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.239900Z",
     "start_time": "2021-03-23T17:37:02.602Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18284,
     "status": "ok",
     "timestamp": 1616014303071,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "OMG_oPR44uVa",
    "outputId": "02589047-6f33-4825-ae36-cbb3c880550e"
   },
   "outputs": [],
   "source": [
    "print(cent_nonan[:5, 4])\n",
    "print(cent_nonan[:5, 6])\n",
    "poly_widths = cent_nonan[:, 6]-cent_nonan[:, 4]\n",
    "poly_widths = np.reshape(poly_widths, [len(poly_widths), 1])\n",
    "\n",
    "poly_heights = cent_nonan[:, 7]-cent_nonan[:, 5]\n",
    "poly_heights = np.reshape(poly_heights, [len(poly_heights), 1])\n",
    "#print(poly_widths.shape)\n",
    "\n",
    "cent_nonan = np.concatenate((cent_nonan, poly_widths, poly_heights), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.241322Z",
     "start_time": "2021-03-23T17:37:02.610Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17619,
     "status": "ok",
     "timestamp": 1616014303071,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "DOElZXe84uVa",
    "outputId": "3fd7a4e1-0c0f-4fac-c4f9-0b442a265149"
   },
   "outputs": [],
   "source": [
    "range_x = position.max()-position.min()\n",
    "range_y = xy_topo.max()-xy_topo.min()\n",
    "print(range_x, range_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.243053Z",
     "start_time": "2021-03-23T17:37:02.614Z"
    },
    "executionInfo": {
     "elapsed": 16948,
     "status": "ok",
     "timestamp": 1616014303072,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "qNWfohLb4uVa"
   },
   "outputs": [],
   "source": [
    "dists = np.zeros([len(cent_nonan[:, 3]), 1])\n",
    "reltime = np.zeros([len(cent_nonan[:, 3]), 1]) #will store the elapsed time between two consecutive centroids\n",
    "delx = np.zeros([len(cent_nonan[:, 3]), 1])\n",
    "dely = np.zeros([len(cent_nonan[:, 3]), 1])\n",
    "\n",
    "for i in range (0, len(cent_nonan)-1):\n",
    "    dists[i, 0] = ((cent_nonan[i+1, 1]-cent_nonan[i, 1])**2+(cent_nonan[i+1, 2]-cent_nonan[i, 2])**2)**0.5\n",
    "    reltime[i,0] = cent_nonan[i+1, 0]-cent_nonan[i, 0]\n",
    "    delx[i, 0] = (cent_nonan[i+1, 1]-cent_nonan[i, 1])/range_x\n",
    "    dely[i, 0] = (cent_nonan[i+1, 2]-cent_nonan[i, 2])/range_y\n",
    "cent_nonan = np.concatenate((cent_nonan, dists, reltime, delx, dely), axis = 1)\n",
    "#print(cent_nonan[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.244526Z",
     "start_time": "2021-03-23T17:37:02.619Z"
    },
    "executionInfo": {
     "elapsed": 16318,
     "status": "ok",
     "timestamp": 1616014303073,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "V1ttV5zl4uVa"
   },
   "outputs": [],
   "source": [
    "#print(cent_nonan[-1, :])\n",
    "#np.save('/content/gdrive/My Drive/Python/Stratigraphy/Data/nparrays/'+arrayfolder+'/c'+str(iloc), cent_nonan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.246783Z",
     "start_time": "2021-03-23T17:37:02.625Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 702
    },
    "executionInfo": {
     "elapsed": 17703,
     "status": "ok",
     "timestamp": 1616014305233,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "mNMyGedg4uVa",
    "outputId": "762cbb5d-c0e2-4d53-fc43-c6a266631825"
   },
   "outputs": [],
   "source": [
    "nbins = 1+3.122*math.log(len(cent_nonan), 10)\n",
    "nbins = roundup(nbins, 10)\n",
    "\n",
    "fig, ax = plt.subplots(2,4, figsize = (20,10))\n",
    "\n",
    "## ICD\n",
    "ax[0,0].hist(cent_nonan[:, 10], **kwargs)\n",
    "ax[0,0].set_title('Distribution of distance between centroids', fontsize = 12)\n",
    "ax[0,0].set_xlabel('Distance, m',  fontsize = 12)\n",
    "ax[0,0].set_ylim(0, 400)\n",
    "## del x\n",
    "ax[0,1].hist(cent_nonan[:, 12], **kwargs)\n",
    "ax[0,1].set_title('del x',  fontsize = 12)\n",
    "ax[0,1].set_xlabel('Fraction of cross stream distance',  fontsize = 12)\n",
    "ax[0,1].set_ylim(0, 400)\n",
    "## del y\n",
    "ax[0,2].hist(cent_nonan[:, 13],  **kwargs)\n",
    "ax[0,2].set_title('del y',  fontsize = 12)\n",
    "ax[0,2].set_xlabel('Fraction of total elevation',  fontsize = 12)\n",
    "ax[0,2].set_ylim(0, 500)\n",
    "## distance in terms of del x and del y\n",
    "deldist = (delx**2+dely**2)**0.5\n",
    "ax[0, 3].hist(deldist,  **kwargs)\n",
    "ax[0, 3].set_title('Proportional ICD using delx dely',  fontsize = 12)\n",
    "ax[0, 3].set_xlabel('non-dimensionalised ICD',  fontsize = 12)\n",
    "ax[0, 3].set_ylim(0, 400)\n",
    "## polygon widths\n",
    "ax[1,0].hist(cent_nonan[:, 8],  **kwargs)\n",
    "ax[1,0].set_title('Distribution of polygon widths', fontsize = 12)\n",
    "ax[1,0].set_xlabel('widths, m',  fontsize = 12)\n",
    "ax[1,0].set_ylim(0, 1000)\n",
    "#polygon heights\n",
    "ax[1,1].hist(cent_nonan[:, 9], **kwargs)\n",
    "ax[1,1].set_title('Distribution of polygon heights', fontsize = 12)\n",
    "ax[1,1].set_xlabel('heights, m',  fontsize = 12)\n",
    "ax[1,1].set_ylim(0, 1000)\n",
    "#polygon w:H ratio\n",
    "ax[1,2].hist(cent_nonan[:, 8]/cent_nonan[:, 9], **kwargs)\n",
    "ax[1,2].set_title('Distribution of W:H Ratios of polys', fontsize = 12)\n",
    "ax[1,2].set_xlabel('W:H Ratios of polys',  fontsize = 12)\n",
    "ax[1, 2].set_ylim(0, 1500)\n",
    "## polygon areas\n",
    "ax[1, 3].hist(cent_nonan[:, 3], **kwargs)\n",
    "ax[1, 3].set_title('Distribution of polygon areas',  fontsize = 12)\n",
    "ax[1, 3].set_xlabel('area, $m^2$',  fontsize = 12)\n",
    "ax[1, 3].set_ylim(0, 1500)\n",
    "\n",
    "ptiles = np.array([10, 25, 50, 75, 95])\n",
    "ptile_store = np.empty([8, len(ptiles)])\n",
    "\n",
    "indexlist = [cent_nonan[:, 10], cent_nonan[:, 12], cent_nonan[:, 13], deldist, cent_nonan[:, 8], cent_nonan[:, 9], cent_nonan[:, 8]/cent_nonan[:, 9],cent_nonan[:, 3]]\n",
    "colours = ['r', 'b', 'g', 'c', 'm', 'y', 'k']\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(0, len(ax)):\n",
    "\n",
    "    ptile_store[i, :] = np.percentile(indexlist[i], ptiles)\n",
    "    for j in range (0, len(ptiles)):\n",
    "        ax[i].axvline(ptile_store[i, j], c = colours[j], ls = '--', label = str(ptiles[j])+'th = '+ str(round(ptile_store[i, j], 2))) \n",
    "        #ax[i].axvline(ptile_store[i, j], c = 'b', ls = '--', label = '25th = '+ str(round(icd_ptiles[1])))\n",
    "        #ax[i].axvline(ptile_store[i, j], c = 'g', ls = '--', label = '50th = '+ str(round(icd_ptiles[2])))\n",
    "    ax[i].legend()\n",
    "#icd_ptiles = np.percentile(cent_nonan[:, 10], [10, 25, 50])\n",
    "\n",
    "#print(icd_ptiles)\n",
    "#p10 = ax[0, 0].axvline(icd_ptiles[0], c = 'r', ls = '--', label = '10th = '+ str(round(icd_ptiles[0]))) \n",
    "#p25 = ax[0, 0].axvline(icd_ptiles[1], c = 'b', ls = '--', label = '25th = '+ str(round(icd_ptiles[1])))\n",
    "#p50 = ax[0, 0].axvline(icd_ptiles[2], c = 'g', ls = '--', label = '50th = '+ str(round(icd_ptiles[2])))\n",
    "\n",
    "\n",
    "icd_norm = cent_nonan[:, 10]/ptile_store[0, 0]\n",
    "\n",
    "#ax[1, 0].hist(icd_norm, bins = nbins)\n",
    "#ax[1,0].set_title('Normalised ICD',  fontsize = 12)\n",
    "#ax[0,0].text(20, 50, 'n='+str(len(cent_nonan)))\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle('I = '+str(iloc)+' n='+str(len(cent_nonan)))\n",
    "plt.savefig(savefilesto+'stats/'+modelrun+'.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.248440Z",
     "start_time": "2021-03-23T17:37:02.630Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "elapsed": 19637,
     "status": "ok",
     "timestamp": 1616014309112,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "P2uxFOOW4uVa",
    "outputId": "643b860d-64dd-40ab-e577-f6f23a7146e7"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize = (15, 6), tight_layout = True, sharex = True)\n",
    "\n",
    "for i in range (0, len(cent_nonan)):\n",
    "    ax[0].plot(cent_nonan[i, 1], cent_nonan[i, 0], marker = 'o', lw = '0', ms = '10', color = cpick.to_rgba(cent_nonan[i, 0]), mew = 0)\n",
    "for i in range (0, end_t):\n",
    "    ax[1].plot(posnew, erohiatalsurf[i], 'k', lw = 0.8)\n",
    "    ax[1].plot(posnew, xy_topo[i], 'k', lw = 0.1)\n",
    "    \n",
    "for i in range (0, len(posnew)):\n",
    "    ax[1].axvline(posnew[i], c = 'k', ls = 'dotted', lw = 0.3)\n",
    "    ax[0].axvline(posnew[i], c = 'k', ls = 'dotted', lw = 0.3)\n",
    "    \n",
    "ax[0].set_xlim(0, posnew.max())\n",
    "ax[0].set_ylabel('Age of centroid')\n",
    "ax[1].set_xlabel('Cross stream grid lcoation')\n",
    "ax[1].set_title('Stratigraphy form main bounding surfaces')\n",
    "ax[1].set_ylabel('Bed Elevation')\n",
    "\n",
    "ax[0].set_title('Spatial distribution of ages of centroids in the stratigraphy')\n",
    "\n",
    "ax[0].set_facecolor('xkcd:silver')\n",
    "ax[1].set_facecolor('xkcd:silver')\n",
    "plt.savefig(savefilesto+'stats/'+modelrun+'act_space.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.250632Z",
     "start_time": "2021-03-23T17:37:02.636Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "executionInfo": {
     "elapsed": 16492,
     "status": "ok",
     "timestamp": 1616014310227,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "_q9MywJm4uVa",
    "outputId": "a7a80db0-1044-4ed8-a25e-ce5401c938f0"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize = (10, 8))\n",
    "for i in range(0, end_t):\n",
    "    ax[0].plot(posnew, xy_topo[i], 'b', alpha = 0.5, lw = 2)\n",
    "    ax[0].plot(posnew, deposurf[i], 'r', alpha = 0.5, lw = 2)\n",
    "    \n",
    "    ax[1].plot(posnew, erohiatalsurf[i], 'k', alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.252053Z",
     "start_time": "2021-03-23T17:37:02.642Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "executionInfo": {
     "elapsed": 15696,
     "status": "ok",
     "timestamp": 1616014310452,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "pHI7fhLX4uVb",
    "outputId": "529d3ae9-3afe-44ea-9b3e-fd1a2320c786"
   },
   "outputs": [],
   "source": [
    "cent_xage = cent_nonan[:, :2].copy() #create an array with only the centroid age and the rounded x location\n",
    "cent_xage[:, 1] = np.round(cent_xage[:, 1], 0)\n",
    "\n",
    "gappiness = np.empty([int(position.max()), 2]) #matrix to store the gappiness measure of the data\n",
    "                                       #0-position (rounded) #1 = number of centroids at xpos\n",
    "    \n",
    "for i in range (0, int(position.max()), 1):\n",
    "    gappiness[i, 0] = i\n",
    "    gappiness[i, 1] = 100*len(np.where(cent_xage[:, 1]==i)[0])/end_t\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize = (5, 5))\n",
    "distofages = ax.hist(gappiness[:, 1],  **kwargs)\n",
    "ax.set_title('Activity Metric--Distribution of number of centroids per core location')\n",
    "ax.set_xlabel('% time represented (# of centroids/number of timesteps)')\n",
    "plt.savefig(savefilesto+'stats/'+modelrun+'activity.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNyoSDsk4uVb"
   },
   "source": [
    "## A overly wordy note on pervasive surfaces of erosion and non-depositon..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytEo3iLZ4uVb"
   },
   "source": [
    "22/01/2021\n",
    "\n",
    "(tl;dr) Skip to the 5th paragraph for the code logic, paragraphs 1-5 are sort of an amuse-bouche:\n",
    "\n",
    "Okay, I spent the last coupe of weeks tinkering with different methods of picking out the mot pervasive/prominent/erosiva/longest non-depositional surfaces in the stratigraphy and it feels immensely complicated. I've tried multiple methods that are equally shitty, but some have their own advantages. \n",
    "\n",
    "In an ideal case, I'd be able to lot out entire, through-going and complete surfaces at each timestep that repesent the major bounding surfaces in the stratigraphy after the end of the entire model run. I have, to this date, never seen this done in a paper. The closest to this was the extracion of surfaces of erosion in Wietse Van de Lageweg's dissertation (Chapter 5, also published in 2014ish, its a sedimentology paper i think). Being able to pull out these surfaces is important becuse they most likely represent the best (quality and quantity) resolution of the internal evolution of the channel belt that we may be able to capture in the feld, so naturally, we would want to compare the model stratigraphy at this similar scale. \n",
    "\n",
    "The problem I'm having is that I only have piecemal locations of erosion and non-deposition that do not make complete surfaces on the plots, ergo I cannot create polygons (read: bar packages) to get various statistics from the model stratigraphy. No statistics == No thesis == Stuck in State College forever (only a little bit melodramatic here, if you are reading this and you are not Safiya, I apologise, unless you find this rant amusing. I hope you do. Sorry for the digression, anyway back to extracting the most prominent bar packges from the model stratigraphy...). In an ideal case I would be able to (and sorry this is essentially a figure so I am thinking like a graphic artist/GIS person here) plot all the piecemal erosion and hiatal surfaces on a plot that is technically 3D (x-y-time) as I plotted it using a for loop then flatten the plot onto only the x-y dimension, then I would have essentially a polyline (I think, idk what a polyline is tbh, but essentially I'd have a complex shape made up of multiple polygons..maybe I'll have a muiltipolygon idk) which I could transfer to another software package (maybe Petrel, Arc, Photoshop, ImageJ or something) and extract the dimesions and attributeds of the polygons, get my statistics and graduate (or publish, who know's what future Saf will do). There are two problems with this approach:\n",
    "\n",
    "1. I do not know how to do that, and I worry that the exercise in figuring it out will literally take all of the power my two brain cells possess and take literally forever because I ain't no coder and have yet to find a comp scientist that can help with this problem in a *helpful* way. \n",
    "2. This mightn't even be beneficial because then we will have the same problem of assessing the universiality of image based statistical approaches to building statistics like this (re Ellen and Liz stick model avulsion problems with relating model space to field space) and all of my years of furious stack exchange searches will be a (comparative) waste\n",
    "    \n",
    "With this in mind, I am trying to adopt a vector-based, pseudo-statistical approach to figure put exactly which surfaces will statistically get preserved in outcrop (omfg I could literally write a paper on this but again I digress...). I've tried a couple ways to get this going in the cells that follow, using the array that stores the ages of all surfaces of the stratigraphy (ages_ero). This array accounts for resetting of the age of a deposit when it is eroded and also for when there is non-deposition (hiatus):\n",
    "\n",
    "<u>Approach 1: </u> Non-normalised, high-frequency surfaces\n",
    "In this method I plot the frequency of the ages of the aurfaces in the straitgraphy assuming that more pervasive and persistence surfaces will have higher frequencies. We want to then know what the distribution of frequencies looks like. Essentially we want the freuqncies that are the outliers, the things that are more frequent than anything else, these are, theoretially correspodent to erosion surfaces that rework the most stratigraphy or hiatal surfaces that receive no deposition for the longest time. When building the aes_ero matrix, these are the surfaces that get reset the most therefore should have an inordinately high frequency. I then create set of quantiles that we want (that in this case specify te degree of eworking/non-depositing at the surface), for this case I use the 75th, 80th, 85th and 90th. I plot the ages (read: timesteps) that correspond to those quantiles, and compare the resultant stratigraphy to that which is output when I plot the erohiatalsurfs only. This method is ok, however it doesn't cover some of the earlier surfaces since either (1) the 'area' that is available to be reworked at a time is much less than that available at a later time, say comparing erosion at timestep 25 having '25 x posnew' number of locations for erosion vs timestep 200 with '200 x posnew' locations to erode (assuming even deposition everyhere between those time maybe. I'm hoping you understand how this might be problematic. But somehow it kinda works, it seems like consistently, surfaces above the 85th ish percentile best for the erohistalsurf surfaces, which is interesting. \n",
    "\n",
    "<u>Approach 2: </u> Normalised, high-frequency surfaces\n",
    "This is the same method as Approach 1 except I normalise the Frequency of ages in the ages_ero matrix to the area available for erosion at each timestep. Theoretically this would give us a more representative distribution of the most erosive surfaces but so far it just skewes the histogram to the left (aggressively) and almost all of the surfaces become insignificant (excl the initial surface and the final one)\n",
    "\n",
    "<u>Approach 3: </u>Most continuous erosion surfaces\n",
    "Here I found the count of the number of points of erosion/non-deposition at each timestep with the hypothesis that the surfaces that were the most pervasive and persistent were the ones that resampled the most stratigrahy therefore the count of non-nan elements in the array should be highest in the important surfaces (again using quantiles). There are a couple problems with this but the most significant is that erosion surfaces are not always the same age  so when you sum the non-nan elements it's not entirely an accurate representation and it isqute unlikely that a surface across the entire cross section is an erosion surface, meaning that we get alo the bits of the surface that are depositonal and that does not help in further defining the packages. \n",
    "\n",
    "These all suk, but I think Approach 1 is the best so far, at 85th percentile. I have to find the right way to normalise for Approach 2 because theoretically it should be the most ideal method of the 3. \n",
    "\n",
    "## Update 25/01\n",
    "I think the 50th quantile (median) using te corrected version of approach 2 (v2, new UP 25/01) is the best at pulling out the erosion surfaces most accurately with the least 'extra' surfaces that make the resolution finer than what we seee in the field. \n",
    "\n",
    "## LOL JOKES 03/02\n",
    "The previous statement was only true for like one cross section. The EH count approach (approach 3) seems to be effective-est for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbrZLMV84uVb"
   },
   "source": [
    "## EH Count method (rooted in approach 3)\n",
    "So far the method of finding the surfaces that reworked > x frac of surf is the best\n",
    "3 Feb 2021\n",
    "\n",
    "Here we find the number of points of erosion and hiatus on each surface (Erocount) then find the fraction of the surface that is reworked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.253499Z",
     "start_time": "2021-03-23T17:37:02.654Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 13059,
     "status": "ok",
     "timestamp": 1616014310453,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "izmy8LU44uVb",
    "outputId": "205042f8-5915-4c44-f42e-55881f793a9d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "erocount = np.empty([end_t, 3]) #holding the count of non-nan elements to identiy major erosion/hiatal surfs\n",
    "\n",
    "for row in range (0, end_t):\n",
    "    length = int(np.floor(xposition[row, -1]-xposition[row, 0])) #length of the section at time, t\n",
    "    erocount[row, 0] = row #timestep value\n",
    "    endpos = int(xposition[row].max())\n",
    "    #print(endpos)\n",
    "    erocount[row, 1] = np.count_nonzero(~np.isnan(erohiatalsurf[row, :endpos])) #count number of EH surfs\n",
    "    erocount[row, 2] = erocount[row, 1]/length #fraction of surf EH reworked\n",
    "\n",
    "    #print(erocount.shape)\n",
    "\n",
    "plt.hist(erocount[:, 2], bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynlm3gMC4uVb"
   },
   "source": [
    "## Another Friday, another ramble about percentiles:\n",
    "\n",
    "So, right now I'm using (today is Feb 5th 2021 btw) a 'propoerion of bed surface reworked' type approach to find the main erosional and hiatal surfaces in the stratigraphy. The fundamental tenet here is that the most 'boundy' bounding surfaces will be he surfaces that have had the longest times of non-deposition or the most erosion, ie. I'm assuming that the stratigraphy is made up of deposition along the most active reworking surfaces. \n",
    "\n",
    "Do the method I employ to do this is to find the sum of locations across the section, at each timestep, that undergo hiatal or erosion events and represent that as the fraction of the bed that is reworked. Then, I find, from the distribution of these fractions, the nth percentile, my threshold for what is major vs minor. To check to find which percentile is the 'best' percentile, I have a nifty little loop that basically tests the stratigraphy/main surfaces output at each percentile against the true erohiatalsurf array for similarity. \n",
    "\n",
    "My rule for similarity is as follows: *the percentiles that produce the best stratigraphy will be the percentiles where (1) as many unique elevation points are incorporated into the bounding surface array as possible and (2) as little true erosion data are deleted*. This 2nd rule requires then, that the number of elements in the bounding surface array cannot be less than the number of elements in the erohiatal array, which is why, as you'll see, the percentile value is so low-I'm leaving in a lot of extra stratigraphy to make sure that this statistical stratigraphy I develop is as faithful to the actual erohiatal data as possible. \n",
    "\n",
    "As you'll see, we end up with a lot of extra surfaces that are likely unresolveable in the field, much like the hiatal situation. I think it's fair to say that for a 100m wide outcrop, we will not be able to resolve bars/mar kinematics that are less than ~2cm thickness, so I removed any surface that aggrades by less than that, kind of like how those folks remove surfaces for dunes...maybe ths is something worth checking out. I think the Van Rijn literature cites that the scaling/removal relationship for sub-bar scale topography is 0.3 x flowdepth dune scale)...what I'm getting at is that for my ~0.8m flow depths, remooving 2cm of topography is a non-issue.  \n",
    "\n",
    "As you will see, this method produces a very nice estimation of the EH stratigraphy, with much of the small scale topography removed. I think, for my efforts and the value of this piece, this method will suffice and I am moving on to statistics headaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11186,
     "status": "ok",
     "timestamp": 1616014310454,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "Dou7mCFy4uVc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.255339Z",
     "start_time": "2021-03-23T17:37:02.664Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10557,
     "status": "ok",
     "timestamp": 1616014310640,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "b2qEJqUU4uVc",
    "outputId": "c22f75bc-8375-4aab-979f-66bd6be9925f"
   },
   "outputs": [],
   "source": [
    "threshperc = np.arange(50, 100) #range of percentiles to test the fir over\n",
    "coverage = np.empty([1, len(posnew)])\n",
    "\n",
    "for i in threshperc:\n",
    "    thresh_frac = np.percentile(erocount[:, 2], i) #fraction of surface we want reworked to call it a major surface\n",
    "    big_times = erocount[np.where(erocount[:, 2] >= thresh_frac), 0]\n",
    "    big_times = big_times.ravel()\n",
    "    #print(len(big_times))\n",
    "    bounding_surfaces = np.empty([1, len(posnew)+1]) #will store coordinates of main bounding surfaces plus the time of the surface\n",
    "    bounding_surfaces[:] = np.nan\n",
    "\n",
    "\n",
    "    for t in big_times:\n",
    "        t = int(t)\n",
    "    #ax[0].plot(posnew, stratinterpcub[i, :], c='w', lw=1.5)#, color = cpick.to_rgba(i), lw = 1.5)\n",
    "    \n",
    "        boundscratch = np.append(t, xy_topo[t, :])\n",
    "        bounding_surfaces = np.append(bounding_surfaces, np.reshape(boundscratch, [1, len(posnew)+1]), axis=0)\n",
    "\n",
    "\n",
    "    dummymatrix = np.empty_like(xy_topo)\n",
    "\n",
    "    dummymatrix[:] = np.nan\n",
    "    \n",
    "# dont nee dummy matrix\n",
    "    num_elements = np.count_nonzero(~np.isnan(bounding_surfaces))-len(bounding_surfaces)+1\n",
    "    #num_elements = np.count_nonzero(~np.isnan(erohiatalsurf))\n",
    "    #print(num_elements, len(bounding_surfaces))\n",
    "    for column in range (1, len(posnew)): #search corewise across the section\n",
    "        uniquebounds = np.unique(bounding_surfaces[:, column][np.where(~np.isnan(bounding_surfaces[:, column]))]) #search bounding surfaces\n",
    "        unique_eh_bounds = np.unique(erohiatalsurf[:, column-1][np.where(~np.isnan(erohiatalsurf[:, column-1]))]) #search erohiatal surfaces\n",
    "        #uniquebounds = np.delete(uniquebounds, np.nan)\n",
    "        #unique_eh_bounds = np.delete(unique_eh_bounds, np.nan)\n",
    "        #print(uniquebounds, unique_eh_bounds)\n",
    "        #print(np.intersect1d(uniquebounds, unique_eh_bounds))\n",
    "        coverage[0, column] = len(np.intersect1d(uniquebounds, unique_eh_bounds))\n",
    "    \n",
    "    totalcoverage = np.sum(coverage) #total overlap of the two arrays\n",
    "    total_coverage_frac = totalcoverage/num_elements\n",
    "\n",
    "    \n",
    "    if num_elements < np.count_nonzero(~np.isnan(erohiatalsurf)):\n",
    "        break\n",
    "print('Best percentile = ', i, '. Coverage = ', total_coverage_frac)\n",
    "bestperc = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.256920Z",
     "start_time": "2021-03-23T17:37:02.669Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9594,
     "status": "ok",
     "timestamp": 1616014310640,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "_lBZpSk64uVc",
    "outputId": "0fd28709-9775-4d10-d860-3b6300b0f341"
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(~np.isnan(erohiatalsurf[1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.258352Z",
     "start_time": "2021-03-23T17:37:02.682Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 12749,
     "status": "ok",
     "timestamp": 1616014314628,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "o7QeHEaM4uVc",
    "outputId": "e4a9ccfa-d5f1-4bf5-bb3a-52fb2bc672cd"
   },
   "outputs": [],
   "source": [
    "## look at what removing some different threshold thicknesses will do to the data\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "bd_surfs = np.empty([1, len(posnew)+1])\n",
    "#print(fnl_surfs)\n",
    "\n",
    "for i in range(2, bounding_surfaces.shape[0]):\n",
    "    x = np.mean(bounding_surfaces[i, :]-bounding_surfaces[i-1, :])\n",
    "    #print(x)\n",
    "    \n",
    "    plt.plot(i, x, 'ro')\n",
    "plt.axhline(np.percentile(thickness, 70))    \n",
    "fig, ax = plt.subplots(4, 1, figsize = (18, 18), tight_layout = True, sharex = True)\n",
    "for i in range (0, 3):\n",
    "    for j in range (0, end_t):\n",
    "        ax[i].plot(posnew, erohiatalsurf[j, :], 'k',  lw = 3, alpha = 0.7)\n",
    "\n",
    "mean_thickness_threshold = np.array([0.01, 0.02, 0.03])\n",
    "for i in range(1, bounding_surfaces.shape[0]): ## bounding_surfaces row 0 is a row of nans, start from 2\n",
    "    x = np.mean(bounding_surfaces[i, :]-bounding_surfaces[i-1, :]) #check to see hpw much sediment added at that time\n",
    "\n",
    "    if x > 0.00:\n",
    "        ax[0].plot(posnew, bounding_surfaces[i, 1:])\n",
    "        \n",
    "    if x > mean_thickness_threshold[1]:\n",
    "        ax[1].plot(posnew, bounding_surfaces[i, 1:])\n",
    "        ax[1].set_title('Threshold = '+str(mean_thickness_threshold[1]))\n",
    "    if x > mean_thickness_threshold.max():\n",
    "        ax[2].plot(posnew, bounding_surfaces[i, 1:])\n",
    "        ax[2].set_title('Threshold = '+str(mean_thickness_threshold.max()))\n",
    "        bd_surfs = np.append(bd_surfs, np.reshape(bounding_surfaces[i, :], [1, len(posnew)+1]), axis = 0)\n",
    "        \n",
    "    #if x < 0.03:\n",
    "        #print(i)\n",
    "        \n",
    "\n",
    "#print(fnl_surfs[0, :])\n",
    "fnl_surfs = np.delete(bd_surfs, 0, axis = 0)\n",
    "\n",
    "#print(fnl_surfs[0, :])\n",
    "print(bounding_surfaces.shape, bd_surfs.shape, fnl_surfs.shape)\n",
    "print('number of surfaces deleted = ', (bounding_surfaces.shape[0]-1)-fnl_surfs.shape[0])\n",
    "\n",
    "for i in range (0, len(fnl_surfs)):\n",
    "    ax[3].plot(posnew, fnl_surfs[i, 1:])\n",
    "ax[3].set_title('Final surfaces matrix')\n",
    "ax[-1].set_xlim(0, posnew.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.260584Z",
     "start_time": "2021-03-23T17:37:02.686Z"
    },
    "executionInfo": {
     "elapsed": 11510,
     "status": "ok",
     "timestamp": 1616014314629,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "pQZV9IjB4uVc"
   },
   "outputs": [],
   "source": [
    "#np.save('/content/gdrive/My Drive/Python/Stratigraphy/Data/nparrays/boundingsurfs/hourly'+modelrun, fnl_surfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJhddDyu4uVc"
   },
   "source": [
    "## Coring the *new* stratigraphy\n",
    "\n",
    "I am too lazy to really change the names of things that much so Imma prefix all the copied variables with 'bs'...for 'bounding surfaces'....not bullshit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.262133Z",
     "start_time": "2021-03-23T17:37:02.693Z"
    },
    "executionInfo": {
     "elapsed": 9459,
     "status": "ok",
     "timestamp": 1616014314630,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "SiGxms9X4uVd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bs_coresthick = {} #dictionary to hold set thickness\n",
    "bs_coresbounds = {} #dictionary to hold bound surfaces\n",
    "bs_num_sets = np.empty([len(posnew), 2])\n",
    "bs_num_sets[:, 0] = posnew\n",
    "bs_threshold = 0.1\n",
    "for i in range(0, len(posnew)):\n",
    "    keystr = 'x'+str(i)\n",
    "    \n",
    "    elevatcore = fnl_surfs[:, i+1]\n",
    "    bounds = np.unique(elevatcore)\n",
    "    \n",
    "    bs_coresbounds[keystr] = bounds #finds the bounding elevations of sets\n",
    "    \n",
    "    diffs = np.empty([0]) #temp array to store thickness of sets > a threshold thickness\n",
    "    \n",
    "    for j, k in zip(bounds[:-1], bounds[1:]):\n",
    "        #print(i, j, k)\n",
    "        diff = k-j #more positive -- less positive = positive\n",
    "        if diff > bs_threshold:\n",
    "            diffs = np.append(diffs, diff)\n",
    "    bs_coresthick[keystr] = diffs\n",
    "    bs_num_sets[i, 1] = len(diffs)\n",
    "    \n",
    "    #print(cores[keystr+'_thick'])\n",
    "#print(cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.263572Z",
     "start_time": "2021-03-23T17:37:02.701Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "executionInfo": {
     "elapsed": 10279,
     "status": "ok",
     "timestamp": 1616014316257,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "VYmLwvD34uVd",
    "outputId": "ea631e61-a8f4-4e87-b3af-e70d4b3919a6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (14, 4), tight_layout=True)\n",
    "ax[0].plot(bs_num_sets[:, 0], bs_num_sets[:, 1], 'k', label = 'Main Surfaces extract')\n",
    "ax[0].plot(num_sets[:, 0], num_sets[:, 1], 'k--', alpha = 0.7, label = 'True EH surfaces')\n",
    "for i in range (0, end_t):\n",
    "    #ax[1].plot(posnew, stratinterpcub[i], 'k', lw = 0.1, alpha = 0.4)\n",
    "    #ax[1].plot(posnew, erosurf[i], 'r', lw = 2)\n",
    "    #ax[1].plot(posnew, hiatalsurf[i], 'g', lw = 2)\n",
    "    ax[1].plot(posnew, erohiatalsurf[i], 'r', lw = 2)\n",
    "for i in range (0, len(fnl_surfs)):\n",
    "    ax[1].plot(posnew, fnl_surfs[i, 1:], 'k', lw = 0.5)\n",
    "\n",
    "for i in range(0, len(posnew)):\n",
    "    ax[1].axvline(posnew[i], c = 'k', ls = 'dotted', lw = 0.3)\n",
    "    ax[0].axvline(posnew[i], c = 'k', ls = 'dotted', lw = 0.3)\n",
    "ax[1].set_ylabel('Bed Elevation, m')\n",
    "ax[0].set_ylabel('Number of Sets')\n",
    "ax[1].set_xlabel(\"Cross Stream Position\")\n",
    "ax[0].set_xlabel(\"Cross Stream Position\")\n",
    "ax[0].legend()\n",
    "ax[0].set_ylim(0, 15)\n",
    "\n",
    "ax[0].set_title('Time series of number of sets at iloc = '+str(iloc))\n",
    "ax[1].set_title('Stratigraphy at '+str(iloc)+'. Ero = red, Hiatal = green')\n",
    "plt.savefig(savefilesto+'corestats/compare'+modelrun+'.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.264940Z",
     "start_time": "2021-03-23T17:37:02.711Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "executionInfo": {
     "elapsed": 11096,
     "status": "ok",
     "timestamp": 1616014317721,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "_kE8CBvA4uVd",
    "outputId": "62e094c2-ff2c-4d30-de96-a98aebee842e"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize = (14, 4), tight_layout = True)\n",
    "ax[0].plot(bs_num_sets[:, 0], bs_num_sets[:, 1], 'k')\n",
    "ax[0].set_title('Number of sets along the section')\n",
    "\n",
    "for loc, core in zip(posnew, bs_coresthick):\n",
    "    avg = np.average(bs_coresthick[core])\n",
    "    #print(coresthick[core], avg)\n",
    "    if len(bs_coresthick[core]) >0:\n",
    "        ax[1].plot(loc, np.average(bs_coresthick[core]), 'o', markerfacecolor = 'b', markeredgecolor = 'k')\n",
    "        ax[2].plot(loc, bs_coresthick[core].max(), 'o', markerfacecolor = 'g', markeredgecolor = 'k')\n",
    "        \n",
    "ax[2].set_ylim(0, 2)\n",
    "ax[1].set_ylim(0, 2)\n",
    "\n",
    "ax[2].set_title('Maximum set thickness at each cor loc')\n",
    "ax[1].set_title('Average set thickness at each cor loc')\n",
    "\n",
    "fig.suptitle(str(iloc))\n",
    "\n",
    "#plt.savefig(savefilesto+'/corestats/'+modelrun+'.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.266662Z",
     "start_time": "2021-03-23T17:37:02.718Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "executionInfo": {
     "elapsed": 10875,
     "status": "ok",
     "timestamp": 1616014318161,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "7RV7xUTVgYTg",
    "outputId": "73988c6b-28cc-422b-c3a3-5cfe6bdb3986"
   },
   "outputs": [],
   "source": [
    "allthick_der = np.array([])\n",
    "allthick_eh= np.array([])\n",
    "plt.figure(figsize = (10, 6))\n",
    "#print(allthicknesses_derived.shape)\n",
    "for key1, key2 in zip(bs_coresthick, coresthick):\n",
    "\n",
    "  #print(bs_coresthick[key])\n",
    "  allthick_der = np.append(allthick_der, bs_coresthick[key1])\n",
    "  allthick_eh = np.append(allthick_eh, coresthick[key2])\n",
    "  \n",
    "hist2 = plt.hist(allthick_der, **kwargs, density =True,label = 'Main Surfaces extract')\n",
    "  \n",
    "hist = plt.hist(allthick_eh, histtype='step', density=True, edgecolor = 'k', linewidth = 2.5, bins= 20, label='True EH thicknesses')\n",
    "  \n",
    "plt.legend()\n",
    "plt.ylabel('Normalised Frequency')\n",
    "plt.xlabel('Vertical Set Thickness, m')\n",
    "plt.title('Distribution of core thicknesses of sets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiN6RZ5p4uVd"
   },
   "source": [
    "def find_percentiles (age_array, ptilestart, ptilestop, ptileint):\n",
    "    \n",
    "    \n",
    "    #create the frequency array\n",
    "    \n",
    "    hist, bins = np.histogram(age_array.ravel(), bins = end_t) #find frequency of all ages in stratigraphy\n",
    "    hist = np.reshape(hist, [num_timesteps, 1])\n",
    "    age_dist = np.reshape(np.arange(0, num_timesteps), [num_timesteps, 1]) \n",
    "    frequencies = np.append(age_dist, hist, axis = 1) #frequency of each timestep in the stratigraphy\n",
    "    \n",
    "    #declare the percentile range you want to test fit to\n",
    "    ptiles_to_test = np.arange(ptilestart/100, ptilestop/100, ptileint/100)\n",
    "    #print('get ptiles', ptiles_to_test.shape)\n",
    "    ptile_vals = np.quantile(frequencies[:, 1], ptiles_to_test)\n",
    "    #print('ptilevals', ptile_vals.shape)\n",
    "    \n",
    "    best_intersect = 0.0000 #pace holder for best value of intersection across the percentiles\n",
    "    best_ptile = 0 #placeholder for best percentile value\n",
    "    \n",
    "    for val in ptile_vals:\n",
    "        #print(val)\n",
    "        ## make new array to store main surfaces\n",
    "        main_surfs_age = np.empty_like(erohiatalsurf) \n",
    "    \n",
    "        ero_idx = np.where(frequencies[:, 1] > val) #find which ages exceed the percentile value youre testing\n",
    "        #print('ero_idx', ero_idx)\n",
    "        target_ages = frequencies[:, 0][ero_idx] #fidnthe ages that correspond tp the value\n",
    "        #print('ages', target_ages)\n",
    "        for age in target_ages:\n",
    "            main_surfs_age[age, :] = stratinterpcub[age, :]\n",
    "            \n",
    "        find_intersections = np.where(main_surfs_age == erohiatalsurf)\n",
    "        #print(find_intersections)\n",
    "        num_intersect = len(find_intersections[0])/(len(erohiatalsurf.ravel()))\n",
    "        #print('number of intersections', num_intersect)\n",
    "        if num_intersect > best_intersect:\n",
    "            best_intersect = num_intersect\n",
    "            best_ptile = ptiles_to_test[np.where(ptile_vals ==val)]\n",
    "    #print(best_intersect, best_ptile)\n",
    "    return('Best percentile: ', best_ptile, 'Intersection value:', best_intersect)#, 'Percentile vals:', ptile_vals)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PI_U0RT4uVd"
   },
   "source": [
    "find_percentiles(ages_ero, 75, 100, 1)\n",
    "\n",
    "#main_surfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.267936Z",
     "start_time": "2021-03-23T17:37:02.724Z"
    },
    "executionInfo": {
     "elapsed": 7636,
     "status": "ok",
     "timestamp": 1616014318161,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "zPgEH0si4uVd"
   },
   "outputs": [],
   "source": [
    "hiatal_loc = hiatalsurf.copy()\n",
    "hiatal_loc[np.where(~np.isnan(hiatal_loc))] = 1\n",
    "fig = plt.figure(figsize = (10, 10))\n",
    "plt.pcolormesh(ages_ero, cmap = mycmap)\n",
    "plt.colorbar()\n",
    "#for i in range (0, end_t):\n",
    "#    plt.plot(posnew, hiatal_loc[i, :]*i, 'r--')\n",
    "#    plt.plot(posnew_nb, i*erocontrol[i, :], 'w--')\n",
    "plt.plot(posnew, hiatal_loc[0, :]*0, 'r--', label = 'hiatal time')\n",
    "plt.plot(posnew_nb, erocontrol[0, :]*0, 'w--', label = 'erosional time')\n",
    "plt.legend(loc = 'upper right')\n",
    "#plt.gca().invert_yaxis()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_iUBj6E4uVd"
   },
   "source": [
    "## Grouping centroids by their spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5775,
     "status": "ok",
     "timestamp": 1616014318162,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "s8ujPs2x4uVe",
    "outputId": "114ebd42-9a31-429f-8454-cdbf04d5cb0a"
   },
   "source": [
    "## trying to group the centroids by their distance moved\n",
    "\n",
    "icd_threshold = np.quantile(cent_nonan[:, 10], 0.5)\n",
    "print(icd_threshold)\n",
    "groups = {}\n",
    "#\n",
    "initint = str(0) #you want to start the labelling here\n",
    "groups['group-'+initint] = np.array([0])\n",
    "print(groups['group-'+initint])\n",
    "for pos in range(1, len(cent_nonan[:, 0])): #change this to zip the entire array\n",
    "    #print(age, pos)\n",
    "    \n",
    "    if cent_nonan[pos, 10] <=  icd_threshold:\n",
    "        groups['group-'+initint] = np.append(groups['group-'+initint], pos)\n",
    "    else:\n",
    "        initint = str(int(initint) + 1) #update string to create new key\n",
    "        groups['group-'+initint] = np.array([pos])\n",
    "        \n",
    "#print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.269264Z",
     "start_time": "2021-03-23T17:37:02.731Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## trying to group the centroids by their distance moved\n",
    "\n",
    "icd_threshold = np.quantile(cent_nonan[:, 10], 0.5)\n",
    "#icd_threshold = 50\n",
    "print(icd_threshold)\n",
    "groups = {}\n",
    "#\n",
    "initint = str(0) #you want to start the labelling here\n",
    "groups['group-'+initint] = np.array([0])\n",
    "print(groups['group-'+initint])\n",
    "for pos in range(1, len(cent_nonan[:, 0])): #change this to zip the entire array\n",
    "    #print(age, pos)\n",
    "## group centroids on age difference and distance moved\n",
    "    if (cent_nonan[pos, 10] <=  icd_threshold) & (np.isclose(cent_nonan[pos, 0], cent_nonan[pos-1, 0], atol = 5)):\n",
    "        groups['group-'+initint] = np.append(groups['group-'+initint], pos)\n",
    "    else:\n",
    "        initint = str(int(initint) + 1) #update string to create new key\n",
    "        groups['group-'+initint] = np.array([pos])\n",
    "    \n",
    "#print(initint, len(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.270660Z",
     "start_time": "2021-03-23T17:37:02.739Z"
    }
   },
   "outputs": [],
   "source": [
    "checkcents = groups.get('group-196')\n",
    "\n",
    "top = int(cent_nonan[checkcents[0], 0])\n",
    "bottom = int(cent_nonan[checkcents[-1], 0])\n",
    "print(top, bottom)\n",
    "plt.plot(cent_nonan[checkcents, 1], cent_nonan[checkcents, 2], 'o')\n",
    "for i in range (top, bottom):\n",
    "    plt.plot(xy_topo[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.272442Z",
     "start_time": "2021-03-23T17:37:02.745Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "executionInfo": {
     "elapsed": 1663,
     "status": "ok",
     "timestamp": 1616018142711,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "mFYypUyWRAvD",
    "outputId": "97acdfc9-8aa0-439b-9f72-ae580695a320"
   },
   "outputs": [],
   "source": [
    "##plot length of centroid groups with time compared to widening rate\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize = (12, 15), tight_layout = True)\n",
    "#ax.plot(times, widths, c = 'r', ls = '--', marker='.', mfc = 'r', mec = 'k', mew = 0, label = 'channel widening')\n",
    "floodzone = mpl.patches.Rectangle((fldstart, rate.min()), fldlength, rate.max()-rate.min(), color = 'b', alpha = 0.1, label = 'Flooding')\n",
    "\n",
    "\n",
    "\n",
    "ax1.plot(rate, c = 'r', ls = '--', marker='.', mfc = 'r', mec = 'k', mew = 0, label = 'rate')\n",
    "ax1.set_ylabel('Channel Widening Rate, m/timestep')\n",
    "ax2.plot(rate, c = 'r', ls = '--', marker='.', mfc = 'r', mec = 'k', mew = 0, label = 'rate')\n",
    "ax2.set_ylabel('Channel Widening Rate, m/timestep')\n",
    "ax3.plot(rate, c = 'r', ls = '--', marker='.', mfc = 'r', mec = 'k', mew = 0, label = 'rate')\n",
    "ax3.set_ylabel('Channel Widening Rate, m/timestep')\n",
    "ax4.plot(rate, c = 'r', ls = '--', marker='.', mfc = 'r', mec = 'k', mew = 0, label = 'rate')\n",
    "ax4.set_ylabel('Channel Widening Rate, m/timestep')\n",
    "\n",
    "ax4.set_xlabel('Timestep')\n",
    "ax1.xaxis.grid(True)\n",
    "ax5 = ax1.twinx()\n",
    "ax6 = ax2.twinx()\n",
    "ax7 = ax3.twinx()\n",
    "ax8 = ax4.twinx()\n",
    "#ax3 = ax.twinx()\n",
    "for key in groups:\n",
    "    #print(key)\n",
    "    t = cent_nonan[groups[key][0], 0] #time of group\n",
    "    #tmax = cent_nonan[groups[key][-1], 0] #time of group\n",
    "    #grp_num = len(groups[key]) #how many packages in the group\n",
    "    length = cent_nonan[groups[key][-1], 0]-cent_nonan[groups[key][0], 1] #length of time epresented in package, duration, timesteps\n",
    "    width = cent_nonan[groups[key], 1].max()-cent_nonan[groups[key], 1].min() #width, m of package\n",
    "    height = cent_nonan[groups[key], 2].max()-cent_nonan[groups[key], 2].min() #height, m of package\n",
    "    \n",
    "    ax5.plot(t, width, 'x', mew = 2.5, color = cpick.to_rgba(cent_nonan[int(t), 0]))\n",
    "    ax6.plot(t, height, '^', mew = 2.5, color = cpick.to_rgba(cent_nonan[int(t), 0]))\n",
    "    ax7.plot(t, (width/range_x)/(height/range_y), 'o', mew = 2.5, color = cpick.to_rgba(cent_nonan[int(t), 0]))\n",
    "    #ax3.plot(t, height, '^', mew = 2.5, color = cpick.to_rgba(cent_nonan[int(t), 0]))\n",
    "    #ax.set_xlabel('Package width, m')\n",
    "\n",
    "deltaelev = xy_topo.max(axis = 1)-xy_topo.min(axis = 1)\n",
    "ax8.plot(times, deltaelev, '+', label = 'min-max')\n",
    "ax8.plot(times, np.average(xy_topo, axis = 1), '.', label = 'mean')\n",
    "ax8.legend()\n",
    "\n",
    "\n",
    "ax1.add_patch(floodzone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.273804Z",
     "start_time": "2021-03-23T17:37:02.749Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "executionInfo": {
     "elapsed": 94581,
     "status": "ok",
     "timestamp": 1615986937449,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "1Nd_GIuO4uVe",
    "outputId": "85535a1b-fec5-496c-bbd9-f6a0d17fce73"
   },
   "outputs": [],
   "source": [
    "#print(groups)\n",
    "fig = plt.figure(figsize = (20, 2))\n",
    "plt.plot(cent_nonan[:, 10], 'k.')\n",
    "plt.axhline(icd_threshold, lw = 1.2, c =  'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.275206Z",
     "start_time": "2021-03-23T17:37:02.754Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94571,
     "status": "ok",
     "timestamp": 1615986937450,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "7p1jpO5f4uVe",
    "outputId": "5809f591-f69d-4c63-f976-e6d7e114d698"
   },
   "outputs": [],
   "source": [
    "discrete_cmap = plt.cm.prism  # define the colormap\n",
    "# extract all colors from the .jet map\n",
    "#cmaplist = [discrete_cmap(i) for i in range(0, 100)]\n",
    "cmaplist = [discrete_cmap(i) for i in range(discrete_cmap.N)]\n",
    "print(discrete_cmap.N)\n",
    "# force the first color entry to be grey\n",
    "#cmaplist[0] = (.5, .5, .5, 1.0)\n",
    "\n",
    "# create the new map\n",
    "discrete_cmap = mpl.colors.LinearSegmentedColormap.from_list('discrete', cmaplist, discrete_cmap.N)\n",
    "\n",
    "# define the bins and normalize\n",
    "bounds = np.linspace(0, len(groups), len(groups))\n",
    "norm_disc = mpl.colors.BoundaryNorm(bounds, discrete_cmap.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T17:37:56.276799Z",
     "start_time": "2021-03-23T17:37:02.758Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "executionInfo": {
     "elapsed": 96663,
     "status": "ok",
     "timestamp": 1615986939555,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "hqF4EAy64uVe",
    "outputId": "71a6494d-b43e-41ea-91e6-4e17bc37a204"
   },
   "outputs": [],
   "source": [
    "# Make a user-defined colormap for the smaller time range of the centroid groups.\n",
    "cnorm_cents = mcol.Normalize(vmin=0,vmax=len(groups))\n",
    "print(len(groups))\n",
    "\n",
    "# Turn these into an object that can be used to map time values to colors and can be passed to plt.colorbar().\n",
    "cpick_cent = cm.ScalarMappable(norm=norm_disc,cmap=discrete_cmap) \n",
    "cpick_cent.set_array([])     \n",
    "\n",
    "\n",
    "colors = cm.prism(np.linspace(0, 1, len(groups)))\n",
    "\n",
    "fig, ax = plt.subplots( figsize = (12, 4), tight_layout = True, squeeze = True)\n",
    "for key in groups:\n",
    "    #print(key)\n",
    "    #print(list(groups).index(key))\n",
    "    \n",
    "    for i in groups[key]:\n",
    "        #print(i)\n",
    "        ax.plot(cent_nonan[i, 1], cent_nonan[i, 2], marker = 'o', color = cpick_cent.to_rgba(list(groups).index(key)))\n",
    "    #print(list(groups).index(key))\n",
    "fig.colorbar(cpick_cent, ax = ax)\n",
    "for i in range (0, len(fnl_surfs)):\n",
    "    ax.plot(posnew, fnl_surfs[i, 1:], 'k', lw = 0.5, alpha = 0.1)\n",
    "for i in range (0, end_t):\n",
    "    ax.plot(posnew, erohiatalsurf[i], 'k')\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('xkcd:greyish')\n",
    "plt.ylim(xy_topo.min(), xy_topo.max())\n",
    "plt.title('Centroids grouped by ICD. Cutoff = '+str(icd_threshold)+'m')\n",
    "#plt.savefig(savefilesto+'centroidmap/'+modelrun+'.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l6t8Qzv4uVe"
   },
   "source": [
    "## Code to make gif animations of the strat infil, clunky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajA0OySt4uVe"
   },
   "source": [
    "fig, ax = plt.subplots(2, 1, sharex = True, sharey = True, figsize = (10, 8), dpi = 200)\n",
    "ax[0].set_xlim(position.min(), position.max())\n",
    "ax[1].set_ylim(stratinterpcub.min(), stratinterpcub.max())\n",
    "\n",
    "camera = Camera(fig)\n",
    "#runtime = runtime+1-(runtime%10)\n",
    "for age in range (end_t-1):\n",
    "    fill = ax[0].fill_between(posnew, stratinterpcub[age+1], stratinterpcub[age], color=cpick.to_rgba(age), alpha = 1)\n",
    "    ax[0].plot(posnew, stratinterpcub[age], 'k', alpha = 0.5)\n",
    "    ax[0].plot(posnew, stratinterpcub[age+1], 'k', alpha = 0.5)\n",
    "    for time in range (0, age):\n",
    "        ax[1].fill_between(posnew, stratinterpcub[time+1], stratinterpcub[time], color=cpick.to_rgba(time), alpha = 1)\n",
    "        ax[1].plot(posnew, erohiatalsurf[time+1], 'r', alpha = 0.7)\n",
    "        ax[1].plot(posnew, deposurf[time+1], 'b', alpha = 0.2, lw = 0.5)\n",
    "\n",
    "    \n",
    "    camera.snap()\n",
    "    #ax.clear()\n",
    "animation = camera.animate()  \n",
    "animation.save(savefilesto+'/Gifs/'+modelrun+'bedfilling.gif', writer = 'imagemagick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DR4Jkmmv4uVf"
   },
   "source": [
    "packages = np.empty([1, 1, 1])\n",
    "fig, ax1 = plt.subplots(1, 1, tight_layout=True, squeeze=True)#, figsize=(19.80, 10.8))\n",
    "\n",
    "polyCollection = ax1.fill_between(posnew, stratinterpcub[0], stratinterpcub[1], color=cpick.to_rgba(0), alpha = 1)\n",
    "points = polyCollection.get_paths()[0].vertices #get vertices from the fill polygons\n",
    "poly = Polygon(points) #convert vertices to polygon\n",
    "#print(points)\n",
    "#mpoly = Polygon(points)\n",
    "\n",
    "#polygon_store = {'polygon 0':{'age': 0, 'verts':poly, 'patch': PolygonPatch(poly)}}\n",
    "polygon_store = {'polygon 0': poly}#, 'patch 0':patch}\n",
    "\n",
    "for age in range (0, end_t-1):\n",
    "    polyCollection = ax1.fill_between(posnew, stratinterpcub[age+1], stratinterpcub[age], color=cpick.to_rgba(age), alpha = 1)\n",
    "    points = polyCollection.get_paths()[0].vertices\n",
    "  \n",
    "    poly = Polygon(points) #make vertices a polygon\n",
    "    polystring = \"polygon \"+str(age)\n",
    "    patchstring = \"patch \"+str(age)\n",
    "    polygon_store[polystring] = poly #create key in dictionary 'polygon_store' which holds the vertices of the polygon at 'age'\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots(1, figsize = (5,5))\n",
    "\n",
    "for i in range (0,10):\n",
    "    #fig, ax = plt.subplots(1, figsize = (10, 10))\n",
    "    namestring = \"polygon \"+str(i)\n",
    "    patch = PolygonPatch(polygon_store[namestring])\n",
    "    ax.add_patch(patch)\n",
    "\n",
    "ax.set_xlim(0, 18)\n",
    "ax.set_ylim(stratinterpcub.min(), stratinterpcub.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFdAXP0K4uVf"
   },
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 4), tight_layout = True, dpi = 400)\n",
    "#data = ax.contourf(xdomain, ydomain, elev, levels = 500, cmap = 'gist_earth')\n",
    "def animate(i):\n",
    "    ax.plot(posnew, deposurf[i], 'xkcd:midnight blue', ls = '--', lw = 0.8, alpha = 0.3)\n",
    "    ax.fill_between(posnew, stratinterpcub[i+1], stratinterpcub[i], color=cpick.to_rgba(i), edgecolor = 'xkcd:midnight blue', lw = 1, ls = '--', alpha = 1)\n",
    "    ax.plot(posnew, erohiatalsurf[i], 'xkcd:midnight blue', lw = 1.5, alpha = 1)\n",
    "    \n",
    "    cent_loc = np.where(cent_array[:, 0]==i)\n",
    "    ax.plot(cent_array[cent_loc, 1], cent_array[cent_loc, 2], marker = '.', markerfacecolor = 'k', markeredgecolor = 'k', mew = .1,   ms = 4, alpha = .5)\n",
    "    ax.set_facecolor('xkcd:grey')\n",
    "\n",
    "    ax.set_ylim(stratinterpcub.min(), stratinterpcub.max())\n",
    "    #ax.set_xlim(1000, 2000)\n",
    "    ax.set_title('Time = '+ str(i*10)+ ' min') \n",
    "    ax.set_xlabel('Streamwise Distance, m')\n",
    "    ax.set_ylabel('Bed Elevation, m')\n",
    "\n",
    "\n",
    "interval = 0.02#in seconds     \n",
    "ani = animation.FuncAnimation(fig, animate, num_timesteps-1, interval=interval*1e+3, blit=False)\n",
    "#FFwriter = animation.FFMpegWriter()\n",
    "\n",
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=5, metadata=dict(artist='Me'), bitrate=1800)\n",
    "\n",
    "ani.save(savefilesto+'Videos/'+modelrun+'.mp4', writer = writer)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRo7sonu4uVf"
   },
   "source": [
    "## Make the bed set thickness time series plots.\n",
    "xloc = np.random.choice(int(position.max()), 4, replace = False)\n",
    "print(xloc)\n",
    "fig, ax = plt.subplots(1, len(xloc), figsize = (16, 4), tight_layout = True)\n",
    "\n",
    "for xloc, i in zip(xloc, np.arange(0, len(xloc))): \n",
    "    ax[i].plot(age_dist, topointerp[:, xloc], 'k')\n",
    "\n",
    "    sets = np.sort(topointerp[:, xloc])\n",
    "\n",
    "    ax[i].axhline(sets[0], c = 'r', ls = '--', lw = 0.5)\n",
    "    #ax[i].axhline(sets[-1], c = 'r', ls = '--', lw = 0.5)\n",
    "    #ax[i].axhline(sets[2], c = 'r', ls = '--', lw = 0.5)\n",
    "    #ax[i].axhline(sets[3], c = 'r', ls = '--', lw = 0.5)\n",
    "    ax[i].set_title(str(iloc)+' elevation time series @'+str(xloc))\n",
    "    \n",
    "plt.savefig('/content/gdrive/My Drive/Python/Stratigraphy/Plots/elevtimeseries/'+modelrun+'.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2E9CZqY4uVf"
   },
   "source": [
    "## APPROACH 2-v1 (legacy, placed as markdown because I think the counting algorithm is wrong)\n",
    "I do not think this normalising thing is working because we get a gigantically skewed histogram of normalised ages that make it difficult to pull out the main surfaces but I think with the improvements in the hiatal pick-out-ing we get a more even spread of age data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YqdxtAa4uVf"
   },
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (12, 6), tight_layout = True)\n",
    "allages = ages_ero.ravel()\n",
    "age_hist, bins = np.histogram(allages, bins = end_t) #gets frequency of alhow l ages\n",
    "age_hist = np.reshape(age_hist, [num_timesteps, 1])\n",
    "age_dist = np.reshape(np.arange(0, end_t), [end_t, 1]) #a column vector of all of the number of timesteos\n",
    "\n",
    "#normalising time of erosion to area available for erosion with Liz 14/01\n",
    "\n",
    "erodible_area = 200*age_dist # finding the area available to erode\n",
    "area_norm = np.divide(age_hist, erodible_area) #normalising area resampled to area available to be resampled \n",
    "area_norm[0] = 0\n",
    "#age_norm = np.delete(age_norm, 0, axis = 0)\n",
    "#plt.hist(age_norm)\n",
    "normalising = np.concatenate((age_hist, erodible_area, area_norm), axis = 1) #column vector of frequencies of ages \n",
    "                                                                        #(i.e. area reworked)[0], area avail to rework [1]\n",
    "                                                                        # and the normalised area/reworking time [2]\n",
    "#print(normalising)\n",
    "\n",
    "#ax[0].plot(age_dist, area_norm) #plot the series showing the area resampled normalised to the area available for resampling\n",
    "#ax[0].plot(age_dist[1:], age_hist[1:])\n",
    "#ax[0].set_ylim(0, area_norm.max())\n",
    "norm_area_hist, bins = np.histogram(area_norm, bins = num_timesteps) #gets frequency of ages, normalised to the area available for reworking\n",
    "norm_area_hist = np.reshape(norm_area_hist, [num_timesteps, 1])\n",
    "\n",
    "#frequencies = np.append(age_dist, hist, axis = 1)\n",
    "frequencies = np.append(age_dist, norm_area_hist, axis = 1) # becuase we want the frequencies of the normalised age/areas from which we can find the most frequent\n",
    "\n",
    "print(np.percentile(frequencies, 10))\n",
    "\n",
    "\n",
    "print(frequencies.shape)\n",
    "\n",
    "ax[0].hist(area_norm, bins = num_timesteps)\n",
    "ax[1].hist(norm_area_hist, bins = 50) #frequency of frequencies\n",
    "ax[0].set_title('Vol Resampled, normalised to area')\n",
    "ax[1].set_title('Freq Distribution of amount of Vol Resampled')\n",
    "\n",
    "want_quants = np.array([0.25, 0.50, 0.75, 0.90])\n",
    "quants = np.quantile(frequencies[:, 1], want_quants)\n",
    "mode_arr = stats.mode(frequencies[:, 1])\n",
    "print('Mode: ', mode_arr)\n",
    "print(str(100*want_quants[0])+'th: ', quants[0], str(100*want_quants[1])+'th: ', quants[1], str(100*want_quants[2])+'th: ', quants[2], str(100*want_quants[3])+'th: ', quants[3])\n",
    "\n",
    "for i in mode_arr:\n",
    "    mode = ax[1].axvline(i, lw = 1.2, ls = '-.', c = 'k', label = 'Mode')\n",
    "#mean = ax[axloc].axvline(mean_temp, lw = 1.2, ls = '--', c = 'xkcd:slate', label = 'Mean (Arith)')\n",
    "#median = ax[axloc].axvline(median_temp, lw = 1.2, ls = '--', c = 'xkcd:grey', label = 'Median')\n",
    "quant50 = ax[1].axvline(quants[0], lw = 1.2, ls = '--', c = 'xkcd:grey', label = '50th ' + str(np.floor(quants[0])))\n",
    "quant75 = ax[1].axvline(quants[1], lw = 1.2, ls = '--', c = 'xkcd:kelly green', label = '75th ' +str(np.floor(quants[1])))\n",
    "quant85 = ax[1].axvline(quants[2], lw = 1.2, ls = '--', c = 'xkcd:red', label = '85th ' +str(np.floor(quants[2])))\n",
    "quant95 = ax[1].axvline(quants[3], lw = 1.2, ls = '--', c = 'xkcd:blue', label = '95th '+str(np.floor(quants[3])))\n",
    "    \n",
    "ax[1].legend(bbox_to_anchor=(0.5, 0.55), handles = [mode, quant50, quant75, quant85, quant95])\n",
    "#print('freq', hist)\n",
    "\n",
    "fig, ax = plt.subplots(len(quants)+2, figsize = (10, 18), tight_layout = True, sharex = True, sharey = True)\n",
    "for i in range(0, len(quants)+2):\n",
    "    for j in range (0, end_t-1): \n",
    "        ax[i].plot(posnew, stratinterpcub[j], c='k', lw = '0.1', alpha = 1)\n",
    "ax[0].set_title('All Surfaces')\n",
    "\n",
    "\n",
    "##### if we want to plot with \n",
    "\n",
    "## 50th percentile, i.e. median\n",
    "ero_surf_idx_50 = np.where(frequencies[:, 1]>quants[0])\n",
    "target_ages_50 = frequencies[:, 0][ero_surf_idx_50]\n",
    "\n",
    "for i in target_ages_50:\n",
    "    ax[1].plot(posnew, stratinterpcub[i], lw = 1, c = 'r', label = str(100*want_quants[0])+'th percentile surfaces')\n",
    "    ax[5].plot(posnew, stratinterpcub[i], lw = 2, c = 'k', alpha = 0.7, label = str(10*want_quants[0]))\n",
    "    \n",
    "ax[1].set_title(str(100*want_quants[0])+'th Quantile vol resampled')\n",
    "\n",
    "## 75th percentile i.e. 3rd quartile\n",
    "ero_surf_idx_75 = np.where(frequencies[:, 1]>quants[1]) #find frequencies (idx) where vol resampling > 3rd quartile\n",
    "target_ages_75 = frequencies[:, 0][ero_surf_idx_75] #make an array of the ages that > q3 vol resampling\n",
    "\n",
    "\n",
    "## make new array to store main surfaces\n",
    "main_surfs = np.empty([1, len(posnew)])\n",
    "\n",
    "#plot the target ages\n",
    "for i in target_ages_75:\n",
    "    ax[2].plot(posnew, stratinterpcub[i], lw = 1, c = 'r', label = str(100*want_quants[1])+'th percentile surfaces')\n",
    "    ax[5].plot(posnew, stratinterpcub[i], lw = 2, c = 'c', alpha = 0.7, label = str(10*want_quants[1]))\n",
    "    strat_temp = np.reshape(stratinterpcub[i], [1, len(stratinterpcub[i])])\n",
    "    main_surfs = np.concatenate((main_surfs, strat_temp))\n",
    "ax[2].set_title(str(100*want_quants[1])+'th Quantile of Vol Resampled')\n",
    "\n",
    "## 85th percentile\n",
    "\n",
    "ero_surf_idx_85 = np.where(frequencies[:, 1]>quants[2])\n",
    "target_ages_85 = frequencies[:, 0][ero_surf_idx_85]\n",
    "\n",
    "for i in target_ages_85:\n",
    "    ax[3].plot(posnew, stratinterpcub[i], lw = 1, c = 'r', label = str(100*want_quants[2])+'th percentile surfaces')\n",
    "    ax[5].plot(posnew, stratinterpcub[i], lw = 2, c = 'm', alpha = 0.7, label = str(10*want_quants[2]))\n",
    "    strat_temp = np.reshape(stratinterpcub[i], [1, len(stratinterpcub[i])])\n",
    "    main_surfs = np.concatenate((main_surfs, strat_temp))\n",
    "ax[3].set_title(str(100*want_quants[2])+'th Quantile of Vol Resampled')\n",
    "\n",
    "## 95th percentile\n",
    "\n",
    "ero_surf_idx_95 = np.where(frequencies[:, 1]>quants[3]) #find frequencies (idx) where vol resampling > 3rd quartile\n",
    "target_ages_95 = frequencies[:, 0][ero_surf_idx_95] #make an array of the ages that > q3 vol resampling\n",
    "\n",
    "#plot the target ages\n",
    "for i in target_ages_95:\n",
    "    ax[4].plot(posnew, stratinterpcub[i], lw = 1, c = 'r', label = str(100*want_quants[3])+'th percentile surfaces')\n",
    "    ax[5].plot(posnew, stratinterpcub[i], lw = 2, c = 'y', alpha = 0.7, label = str(10*want_quants[3]))\n",
    "ax[4].set_title(str(100*want_quants[3])+'th Quantile of Vol. Resampled')\n",
    "\n",
    "for i in range (0, end_t-1):\n",
    "    ax[5].plot(posnew, erohiatalsurf[i], lw = 2, c = 'xkcd:slate', label = 'ero surfs', alpha = 0.3)\n",
    "\n",
    "ax[5].set_title('Real surfaces of erosion (g), ptile surfs (r 75, b 95)')\n",
    "ax[5].set_xlabel('Cross stream distance, m')\n",
    "fig.suptitle('With Normalising')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iklmZoRs4uVf"
   },
   "source": [
    "## APPROACH 2-v2 (New, UD 25/01/2021) Also markdown bc not ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stV6hVPT4uVf"
   },
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (6, 3), tight_layout = True)\n",
    "allages = ages_ero.ravel()\n",
    "age_dist = np.reshape(np.arange(0, end_t), [end_t, 1]) #a column vector of all of the number of timesteos\n",
    "\n",
    "age_count = np.empty_like(age_dist).astype(float)\n",
    "age_normtoarea = age_count.copy().astype(float)\n",
    "for age in range (1, end_t):\n",
    "   \n",
    "    area = age*posnew.max()\n",
    "    available_area = ages_ero[:age+1, :]\n",
    "    age_count[age]=np.count_nonzero(available_area == age)\n",
    "    age_normtoarea[age] = age_count[age]/area\n",
    "       \n",
    "age_normtoarea[0] = age_normtoarea[1] \n",
    "\n",
    "norm_area_hist, bins = np.histogram(age_normtoarea, bins = num_timesteps) #gets frequency of ages, normalised to the area available for reworking\n",
    "norm_area_hist = np.reshape(norm_area_hist, [num_timesteps, 1])\n",
    "#print(norm_area_hist)\n",
    "\n",
    "#frequencies = np.append(age_dist, hist, axis = 1)\n",
    "frequencies = np.append(age_dist, norm_area_hist, axis = 1) # becuase we want the frequencies of the normalised age/areas from which we can find the most frequent\n",
    "\n",
    "# plot histograms\n",
    "ax[0].hist(age_normtoarea, bins = num_timesteps, histtype = 'step')\n",
    "ax[1].hist(norm_area_hist, bins = 50) #frequency of frequencies\n",
    "ax[0].set_title('Vol Resampled, normalised to area')\n",
    "ax[1].set_title('Freq Distribution of amount of Vol Resampled')\n",
    "\n",
    "# declare which quantiles you want to work with\n",
    "want_quants = np.array([0.50, 0.75, 0.90, 0.95])\n",
    "quants = np.quantile(frequencies[:, 1], want_quants)\n",
    "mode_arr = stats.mode(frequencies[:, 1])\n",
    "\n",
    "# print these stats\n",
    "print('Mode: ', mode_arr)\n",
    "print(str(100*want_quants[0])+'th: ', quants[0], str(100*want_quants[1])+'th: ', quants[1], str(100*want_quants[2])+'th: ', quants[2], str(100*want_quants[3])+'th: ', quants[3])\n",
    "\n",
    "# if you want lines on the frequency histogram plot\n",
    "for i in mode_arr:\n",
    "    mode = ax[1].axvline(i, lw = 1.2, ls = '-.', c = 'k', label = 'Mode')\n",
    "#mean = ax[axloc].axvline(mean_temp, lw = 1.2, ls = '--', c = 'xkcd:slate', label = 'Mean (Arith)')\n",
    "#median = ax[axloc].axvline(median_temp, lw = 1.2, ls = '--', c = 'xkcd:grey', label = 'Median')\n",
    "quant50 = ax[1].axvline(quants[0], lw = 1.2, ls = '--', c = 'xkcd:grey', label = '50th ' + str(np.floor(quants[0])))\n",
    "quant75 = ax[1].axvline(quants[1], lw = 1.2, ls = '--', c = 'xkcd:kelly green', label = '75th ' +str(np.floor(quants[1])))\n",
    "quant85 = ax[1].axvline(quants[2], lw = 1.2, ls = '--', c = 'xkcd:red', label = '85th ' +str(np.floor(quants[2])))\n",
    "quant95 = ax[1].axvline(quants[3], lw = 1.2, ls = '--', c = 'xkcd:blue', label = '95th '+str(np.floor(quants[3])))\n",
    "    \n",
    "ax[1].legend(bbox_to_anchor=(0.5, 0.55), handles = [mode, quant50, quant75, quant85, quant95])\n",
    "#print('freq', hist)\n",
    "\n",
    "\n",
    "## make new array to store main surfaces\n",
    "main_surfs = np.empty([1, len(posnew)])\n",
    "\n",
    "fig, ax = plt.subplots(len(quants)+2, figsize = (10, 18), tight_layout = True, sharex = True, sharey = True)\n",
    "for i in range(0, len(quants)+2):\n",
    "    for j in range (0, end_t-1): \n",
    "        ax[i].plot(posnew, stratinterpcub[j], c='k', lw = '0.1', alpha = 1)\n",
    "ax[0].set_title('All Surfaces')\n",
    "\n",
    "##### if we want to plot with \n",
    "\n",
    "## 50th percentile, i.e. median\n",
    "ero_surf_idx_50 = np.where(frequencies[:, 1]>quants[0])\n",
    "target_ages_50 = frequencies[:, 0][ero_surf_idx_50]\n",
    "\n",
    "for i in target_ages_50:\n",
    "    ax[1].plot(posnew, stratinterpcub[i], lw = 3, c = 'r', ls = ':', label = str(100*want_quants[0])+'th percentile surfaces')\n",
    "    ax[5].plot(posnew, stratinterpcub[i], lw = 2, c = 'k', alpha = 0.7, label = str(10*want_quants[0]))\n",
    "    strat_temp = np.reshape(stratinterpcub[i], [1, len(stratinterpcub[i])])\n",
    "    main_surfs = np.concatenate((main_surfs, strat_temp))\n",
    "ax[1].set_title(str(100*want_quants[0])+'th Quantile vol resampled')\n",
    "\n",
    "## 75th percentile i.e. 3rd quartile\n",
    "ero_surf_idx_75 = np.where(frequencies[:, 1]>quants[1]) #find frequencies (idx) where vol resampling > 3rd quartile\n",
    "target_ages_75 = frequencies[:, 0][ero_surf_idx_75] #make an array of the ages that > q3 vol resampling\n",
    "\n",
    "#plot the target ages\n",
    "for i in target_ages_75:\n",
    "    ax[2].plot(posnew, stratinterpcub[i], lw = 3, c = 'r', ls = ':', label = str(100*want_quants[1])+'th percentile surfaces')\n",
    "    ax[5].plot(posnew, stratinterpcub[i], lw = 2, c = 'c', alpha = 0.7, label = str(10*want_quants[1]))\n",
    "    \n",
    "ax[2].set_title(str(100*want_quants[1])+'th Quantile of Vol Resampled')\n",
    "\n",
    "## 85th percentile\n",
    "\n",
    "ero_surf_idx_85 = np.where(frequencies[:, 1]>quants[2])\n",
    "target_ages_85 = frequencies[:, 0][ero_surf_idx_85]\n",
    "\n",
    "for i in target_ages_85:\n",
    "    ax[3].plot(posnew, stratinterpcub[i], lw = 1, c = 'r', label = str(100*want_quants[2])+'th percentile surfaces')\n",
    "    ax[5].plot(posnew, stratinterpcub[i], lw = 2, c = 'm', alpha = 0.7, label = str(10*want_quants[2]))\n",
    "    strat_temp = np.reshape(stratinterpcub[i], [1, len(stratinterpcub[i])])\n",
    "    main_surfs = np.concatenate((main_surfs, strat_temp))\n",
    "ax[3].set_title(str(100*want_quants[2])+'th Quantile of Vol Resampled')\n",
    "\n",
    "## 95th percentile\n",
    "\n",
    "ero_surf_idx_95 = np.where(frequencies[:, 1]>quants[3]) #find frequencies (idx) where vol resampling > 3rd quartile\n",
    "target_ages_95 = frequencies[:, 0][ero_surf_idx_95] #make an array of the ages that > q3 vol resampling\n",
    "\n",
    "#plot the target ages\n",
    "for i in target_ages_95:\n",
    "    ax[4].plot(posnew, stratinterpcub[i], lw = 1, c = 'r', label = str(100*want_quants[3])+'th percentile surfaces')\n",
    "    ax[5].plot(posnew, stratinterpcub[i], lw = 2, c = 'y', alpha = 0.7, label = str(10*want_quants[3]))\n",
    "ax[4].set_title(str(100*want_quants[3])+'th Quantile of Vol. Resampled')\n",
    "\n",
    "for i in range (0, end_t-1):\n",
    "    ax[1].plot(posnew, erohiatalsurf[i], lw = 2, c = 'xkcd:slate', label = 'ero surfs', alpha = 0.5)\n",
    "\n",
    "ax[5].set_title('Real surfaces of erosion (g), ptile surfs (r 75, b 95)')\n",
    "ax[5].set_xlabel('Cross stream distance, m')\n",
    "fig.suptitle('With Normalising')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXclP8Xr4uVg"
   },
   "source": [
    "## Approach 1: Version 1 also not good for surfaces where there is a lot of later reowkring. MK 03/02/21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxmsYcKE4uVg"
   },
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (12, 6), tight_layout = True)\n",
    "allages = ages_ero.ravel()\n",
    "hist, bins = np.histogram(allages, bins = num_timesteps) #gets frequency of alhow l ages\n",
    "hist = np.reshape(hist, [num_timesteps, 1])\n",
    "age_dist = np.reshape(np.arange(0, num_timesteps), [num_timesteps, 1]) #frequency of all ages and ages\n",
    "frequencies = np.append(age_dist, hist, axis = 1)\n",
    "#print(frequencies.shape)\n",
    "ptiles = np.percentile(allages, [50, 75, 90])\n",
    "\n",
    "\n",
    "# I want to plot the most frequent timesteps. I'm finding how much time (frequency) does each time rework the stratigraphy \n",
    "#(i.e. how deep is each erosion surface\n",
    "# if a surface is really erosive, there will be a higher count of that time in the ages_ero array which means that\n",
    "# it will be have a high frequency (lets call this resampling volume). So we know the ages frequency, but we need to know which ages are the ones to \n",
    "# look for, i.e. above a threshold volume of resampling, therefore we should have a histogram that shows the distribution\n",
    "# of volume resampling in the stratigraphy. We want the really high volume resampling amounts, which should be anything in\n",
    "# the tail of this histogram. The ages, that fall in that distribution (Q3 and above or some percentile value) are the\n",
    "# main erosion ages we want to sample. i.e. plot stratigraphy at that freuency of volume resampling. I think this makes sense?????\n",
    "\n",
    "ax[0].plot(age_dist, hist)\n",
    "#ax[0].hist(allages, bins = num_timesteps)\n",
    "ax[1].hist(hist, bins = 50) #frequency of frequencies\n",
    "ax[0].set_title('Volume resampling, for AGES')\n",
    "ax[1].set_title('Freq Distribution of amount of Vol Resampled')\n",
    "\n",
    "want_quants = np.array([0.95, 0.50, 0.75, 0.90])\n",
    "quants = np.quantile(frequencies[:, 1], want_quants)\n",
    "mode_arr = stats.mode(frequencies[:, 1])\n",
    "print('Mode: ', mode_arr)\n",
    "print('50th: ', quants[0], ' 75th: ', quants[1], ' 90th ', quants[2])\n",
    "\n",
    "for i in mode_arr:\n",
    "    mode = ax[1].axvline(i, lw = 1.2, ls = '-.', c = 'k', label = 'Mode')\n",
    "#mean = ax[axloc].axvline(mean_temp, lw = 1.2, ls = '--', c = 'xkcd:slate', label = 'Mean (Arith)')\n",
    "#median = ax[axloc].axvline(median_temp, lw = 1.2, ls = '--', c = 'xkcd:grey', label = 'Median')\n",
    "quant50 = ax[1].axvline(quants[0], lw = 1.2, ls = '--', c = 'xkcd:grey', label = '50th ' + str(np.floor(quants[0])))\n",
    "quant75 = ax[1].axvline(quants[1], lw = 1.2, ls = '--', c = 'xkcd:kelly green', label = '75th ' +str(np.floor(quants[1])))\n",
    "quant85 = ax[1].axvline(quants[2], lw = 1.2, ls = '--', c = 'xkcd:red', label = '85th ' +str(np.floor(quants[2])))\n",
    "quant95 = ax[1].axvline(quants[3], lw = 1.2, ls = '--', c = 'xkcd:blue', label = '95th '+str(np.floor(quants[3])))\n",
    "    \n",
    "ax[1].legend(bbox_to_anchor=(0.5, 0.55), handles = [mode, quant50, quant75, quant85, quant95])\n",
    "#print('freq', hist)\n",
    "\n",
    "fig, ax = plt.subplots(len(quants)+2, figsize = (10, 18), tight_layout = True, sharex = True, sharey = True)\n",
    "for i in range(0, len(quants)+2):\n",
    "    for j in range (0, end_t-1): \n",
    "        ax[i].fill_between(posnew, stratinterpcub[j+1], stratinterpcub[j], color=cpick.to_rgba(j), alpha= 0.5, linewidth = 0.0)\n",
    "        ax[i].plot(posnew, erohiatalsurf[j], c='k', lw = '3', ls=':', alpha = 1)\n",
    "ax[0].set_title('All Surfaces')\n",
    "##### if we want to plot with \n",
    "\n",
    "## 50th percentile, i.e. median\n",
    "ero_surf_idx_50 = np.where(frequencies[:, 1]>quants[0])\n",
    "target_ages_50 = frequencies[:, 0][ero_surf_idx_50]\n",
    "\n",
    "for i in target_ages_50:\n",
    "    ax[1].plot(posnew, stratinterpcub[i], lw = 1, c = 'r', ls = '--', label = str(100*want_quants[0])+'th percentile surfaces')\n",
    "    ax[5].plot(posnew, stratinterpcub[i], lw = 2, c = 'k', alpha = 0.7, label = str(10*want_quants[0]))\n",
    "    \n",
    "ax[1].set_title(str(100*want_quants[0])+'th Quantile vol resampled')\n",
    "\n",
    "## 75th percentile i.e. 3rd quartile\n",
    "ero_surf_idx_75 = np.where(frequencies[:, 1]>quants[1]) #find frequencies (idx) where vol resampling > 3rd quartile\n",
    "target_ages_75 = frequencies[:, 0][ero_surf_idx_75] #make an array of the ages that > q3 vol resampling\n",
    "\n",
    "\n",
    "## make new array to store main surfaces\n",
    "main_surfs = np.empty([1, len(posnew)])\n",
    "\n",
    "#plot the target ages\n",
    "for i in target_ages_75:\n",
    "    ax[2].plot(posnew, stratinterpcub[i], lw = 1, c = 'r', ls = '--', label = str(100*want_quants[1])+'th percentile surfaces')\n",
    "    ax[5].plot(posnew, stratinterpcub[i], lw = 2, c = 'c', alpha = 0.7, label = str(10*want_quants[1]))\n",
    "    #strat_temp = np.reshape(stratinterpcub[i], [1, len(stratinterpcub[i])])\n",
    "    #main_surfs = np.concatenate((main_surfs, strat_temp))\n",
    "ax[2].set_title(str(100*want_quants[1])+'th Quantile of Vol Resampled')\n",
    "\n",
    "## 85th percentile\n",
    "\n",
    "ero_surf_idx_85 = np.where(frequencies[:, 1]>quants[2])\n",
    "target_ages_85 = frequencies[:, 0][ero_surf_idx_85]\n",
    "\n",
    "for i in target_ages_85:\n",
    "    ax[3].plot(posnew, stratinterpcub[i], lw = 1, c = 'r', ls = '--', label = str(100*want_quants[2])+'th percentile surfaces')\n",
    "    ax[5].plot(posnew, stratinterpcub[i], lw = 2, c = 'm', alpha = 0.7, label = str(10*want_quants[2]))\n",
    "    strat_temp = np.reshape(stratinterpcub[i], [1, len(stratinterpcub[i])])\n",
    "    main_surfs = np.concatenate((main_surfs, strat_temp))\n",
    "ax[3].set_title(str(100*want_quants[2])+'th Quantile of Vol Resampled')\n",
    "\n",
    "## 95th percentile\n",
    "\n",
    "ero_surf_idx_95 = np.where(frequencies[:, 1]>100)#quants[3]) #find frequencies (idx) where vol resampling > 3rd quartile\n",
    "target_ages_95 = frequencies[:, 0][ero_surf_idx_95] #make an array of the ages that > q3 vol resampling\n",
    "\n",
    "#plot the target ages\n",
    "for i in target_ages_95:\n",
    "    ax[4].plot(posnew, stratinterpcub[i], lw = 1, c = 'r', ls = '--', label = str(100*want_quants[3])+'th percentile surfaces')\n",
    "    ax[5].plot(posnew, stratinterpcub[i], lw = 2, c = 'y', alpha = 0.7, label = str(10*want_quants[3]))\n",
    "ax[4].set_title(str(100*want_quants[3])+'th Quantile of Vol. Resampled')\n",
    "\n",
    "for i in range (0, end_t-1):\n",
    "    ax[5].plot(posnew, erohiatalsurf[i], lw = 2, c = 'k', label = 'ero surfs', alpha =1)\n",
    "\n",
    "ax[5].set_title('Real surfaces of erosion (g), ptile surfs (r 75, b 95)')\n",
    "ax[5].set_xlabel('Cross stream distance, m')\n",
    "#ax[5].set_facecolor('k')\n",
    "#p90 = ax[3].plot(posnew, stratinterpcub[-1], lw = 2, c = 'b', alpha = 0.3, label = '90th')\n",
    "#p75 = ax[3].plot(posnew, stratinterpcub[-1], lw = 2, c = 'r', alpha = 0.3, label = '75th')\n",
    "#act = ax[3].plot(posnew, stratinterpcub[-1], lw = 2, c = 'g', alpha = 0.3, label = 'erosurf')\n",
    "\n",
    "#ax[3].legend((p90, p75, act), ('90th', '75th', 'actual surf'))\n",
    "\n",
    "main_surfs = np.delete(main_surfs, 0, axis = 0) #delete first row of main surfs because it was empty\n",
    "#plt.savefig('ages_nonnorm.png')\n",
    "#plt.close(fig)\n",
    "fig.suptitle('Without Normalising')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 96653,
     "status": "ok",
     "timestamp": 1615986939556,
     "user": {
      "displayName": "Safiya Alpheus",
      "photoUrl": "",
      "userId": "18049693271511077124"
     },
     "user_tz": 240
    },
    "id": "X_uDievM6AeS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6K_uao1A4uVS",
    "7hIP60LN4uVT",
    "bNyoSDsk4uVb",
    "ytEo3iLZ4uVb",
    "2l6t8Qzv4uVe",
    "ajA0OySt4uVe",
    "DR4Jkmmv4uVf",
    "XFdAXP0K4uVf",
    "qRo7sonu4uVf",
    "V2E9CZqY4uVf",
    "iklmZoRs4uVf",
    "LXclP8Xr4uVg"
   ],
   "name": "StratBuilder-CleanMar21.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "816px",
    "left": "1548px",
    "right": "20px",
    "top": "114px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
